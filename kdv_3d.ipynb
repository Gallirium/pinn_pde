{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "110dfcf0-4dd3-437d-890f-5018c7acceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pde\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "from numpy import pi\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bca308ba-727c-41d8-b660-2d2ebe3ca75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveEquationPYPDE(variable, eq, ic=\"2/cosh(x)**2\", xrange=None, yrange=None, points=150, trange=0.4):\n",
    "    #initialize xrange\n",
    "    if xrange is None:\n",
    "        xrange = [-pi, pi]\n",
    "    \n",
    "    #eq is passed in as a string with x as the dependent variable\n",
    "    eq = pde.PDE({variable: eq})\n",
    "    \n",
    "    if yrange is not None:\n",
    "        grid = pde.grids.CartesianGrid([xrange, yrange], [points], periodic=[True, True])\n",
    "    else:\n",
    "        grid = pde.grids.CartesianGrid([xrange], [points],  periodic=True)\n",
    "    \n",
    "    field = pde.ScalarField.from_expression(grid, ic)\n",
    "\n",
    "    storage = pde.MemoryStorage()\n",
    "    trackers = ['progress', storage.tracker(interval=0.001)]\n",
    "    #bc = [\"periodic\", {'derivative': 'periodic'}]\n",
    "    #field /= np.sqrt(field.to_scalar('norm_squared').integral.real)\n",
    "\n",
    "    solver = pde.ExplicitSolver(eq, backend='numba' ,scheme='runge-kutta', adaptive=True, tolerance=1e-9)\n",
    "    controller = pde.Controller(solver, t_range=trange, tracker=trackers)\n",
    "    solution = controller.run(field, dt=1e-4)\n",
    "    \n",
    "    return field, storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf9ce56d-c1e1-4960-b47a-2ab8ee79eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 0.4/0.4 [00:16<00:00, 40.61s/it]\n"
     ]
    }
   ],
   "source": [
    "s1, st1 = solveEquationPYPDE('phi', f\"- 6 * phi * d_dx(phi) - laplace(d_dx(phi))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ade6c86-cae2-41a1-9251-e4d2db2a8423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pde.tools.plotting.PlotReference at 0x795ccd544080>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pde.plot_kymograph(st1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cabd3bd-7e9c-4b28-8007-e34f2f7a6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = s1.get_line_data()['data_x'].reshape(-1,1) # X x 1\n",
    "t = np.array(st1.times).reshape(-1,1) # T x 1\n",
    "x_grid, t_grid = np.meshgrid(x, t)\n",
    "\n",
    "\n",
    "phi = np.array(st1.data)\n",
    "#u = np.real(phi) #.flatten().reshape(-1,1)[::10].reshape(79,30)   #.flatten()[:,None]\n",
    "#v = np.imag(phi) #.flatten().reshape(-1,1)[::10].reshape(79,30)  #.flatten()[:,None]\n",
    "\n",
    "#plt.pcolormesh(x_grid, t_grid, np.sqrt(u**2 + v**2))\n",
    "\n",
    "##Generate the two indices\n",
    "idx1 = np.random.choice(x_grid.shape[0], 105, replace=False)\n",
    "idx1 = np.sort(idx1)\n",
    "idx2 = np.random.choice(x_grid.shape[1], 100, replace=False)\n",
    "idx2 = np.sort(idx2)\n",
    "\n",
    "def gen_train(grid):\n",
    "    grid_new = grid[idx1]\n",
    "    grid_new = grid_new[:,idx2]\n",
    "    return grid_new\n",
    "    \n",
    "x_train, t_train, phi_train = gen_train(x_grid), gen_train(t_grid), gen_train(phi)\n",
    "\n",
    "## attempt to plot\n",
    "##plt.pcolormesh(x_train, t_train, phi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96f863ec-fffb-400d-ab5b-d94fd3f43223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 1) (10500, 1) (10500, 1)\n"
     ]
    }
   ],
   "source": [
    "## reshape to feed into model\n",
    "x_train = x_train.flatten()[:,None]\n",
    "t_train = t_train.flatten()[:,None]\n",
    "\n",
    "phi_train = phi_train.flatten()[:,None]\n",
    "print(x_train.shape, t_train.shape, phi_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0404a067-f3b3-4729-a28d-f0912cb60a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## boilerplate stuff for GPU...\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class PINN():\n",
    "    def __init__(self, x, t, phi):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32, requires_grad=True, device=device)\n",
    "        self.t = torch.tensor(t, dtype=torch.float32, requires_grad=True, device=device)\n",
    "        \n",
    "        self.phi = torch.tensor(phi, dtype=torch.float32, device=device)\n",
    "        \n",
    "        self.null = torch.zeros((self.x.shape[0],1), device=device)\n",
    "\n",
    "        self.net_init()\n",
    "        #self.optim = torch.optim.LBFGS(self.net.parameters(),\n",
    "        #                               lr=0.01,\n",
    "        #                               max_iter=50000,\n",
    "        #                               max_eval=50000,\n",
    "        #                               history_size=50,\n",
    "        #                               tolerance_grad=1e-8,\n",
    "        #                               tolerance_change=0.5 * np.finfo(float).eps,\n",
    "        #                               line_search_fn='strong_wolfe'\n",
    "        #                              )\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.net.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.ls = 0\n",
    "        self.step = 0\n",
    "        print(self.x.shape, self.t.shape, self.phi.shape)\n",
    "    \n",
    "    def net_init(self, dDim=256, act=torch.nn.Tanh):\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, dDim),act(),\n",
    "            torch.nn.Linear(dDim, dDim),act(),\n",
    "            torch.nn.Linear(dDim, dDim),act(),\n",
    "            torch.nn.Linear(dDim, dDim),act(),\n",
    "            torch.nn.Linear(dDim, dDim),act(),\n",
    "            torch.nn.Linear(dDim, dDim),act(),\n",
    "            torch.nn.Linear(dDim, 1)\n",
    "        )\n",
    "        self.net.to(device)\n",
    "    \n",
    "    def func(self, x, t):\n",
    "        phi = self.net(torch.hstack((x, t)))\n",
    "\n",
    "        phi_t = torch.autograd.grad(phi, t, torch.ones_like(phi), create_graph=True)[0]\n",
    "        phi_x = torch.autograd.grad(phi, x, torch.ones_like(phi), create_graph=True)[0]\n",
    "        phi_xx = torch.autograd.grad(phi_x, x, torch.ones_like(phi_x), create_graph=True)[0]\n",
    "        phi_xxx = torch.autograd.grad(phi_xx, x, torch.ones_like(phi_xx), create_graph=True)[0]\n",
    "\n",
    "        f_phi = phi_t + 6 * phi * phi_x + phi_xxx\n",
    "\n",
    "        return phi, f_phi\n",
    "        \n",
    "    def closure(self):\n",
    "        self.optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        phi_pred, f_phi_pred = self.func(self.x, self.t)\n",
    "        \n",
    "        phi_loss = self.mse(phi_pred, self.phi)\n",
    "        f_phi_loss = self.mse(f_phi_pred, self.null)\n",
    "        print(f\"Data loss: {phi_loss.item()}, Function loss: {f_phi_loss.item()}\")\n",
    "        self.ls = phi_loss + f_phi_loss\n",
    "        self.ls.backward()\n",
    "\n",
    "        ## Show some graphs and stuff if step % 500 == 0\n",
    "        \n",
    "        print(f\"Step: {self.step}, Loss: {self.ls}\")\n",
    "        self.step += 1\n",
    "        return self.ls\n",
    "\n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "        for i in range(5000):\n",
    "            self.optim.step(self.closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e89e7506-162e-4b83-986e-038ca2b2c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallirium/dostNN/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10500, 1]) torch.Size([10500, 1]) torch.Size([10500, 1])\n"
     ]
    }
   ],
   "source": [
    "kdv = PINN(x_train, t_train, phi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cee970c-8a3b-4736-b788-864df0f47447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loss: 0.7677629590034485, Function loss: 0.00010391690739197657\n",
      "Step: 0, Loss: 0.767866849899292\n",
      "Data loss: 0.7226406931877136, Function loss: 9.284700354328379e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallirium/dostNN/lib/python3.11/site-packages/torch/autograd/__init__.py:251: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [10500, 1], strides() = [1, 0]\n",
      "param.sizes() = [10500, 1], strides() = [1, 0] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:219.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: 0.7227335572242737\n",
      "Data loss: 0.6797754764556885, Function loss: 0.00018965166236739606\n",
      "Step: 2, Loss: 0.6799651384353638\n",
      "Data loss: 0.6386188268661499, Function loss: 0.0004593803605530411\n",
      "Step: 3, Loss: 0.6390781998634338\n",
      "Data loss: 0.5987187623977661, Function loss: 0.0010472032008692622\n",
      "Step: 4, Loss: 0.599765956401825\n",
      "Data loss: 0.559819221496582, Function loss: 0.0022025792859494686\n",
      "Step: 5, Loss: 0.562021791934967\n",
      "Data loss: 0.5218454003334045, Function loss: 0.004305982496589422\n",
      "Step: 6, Loss: 0.5261513590812683\n",
      "Data loss: 0.48493069410324097, Function loss: 0.007892830297350883\n",
      "Step: 7, Loss: 0.4928235113620758\n",
      "Data loss: 0.44941702485084534, Function loss: 0.013627279549837112\n",
      "Step: 8, Loss: 0.46304431557655334\n",
      "Data loss: 0.4158943295478821, Function loss: 0.022184183821082115\n",
      "Step: 9, Loss: 0.43807852268218994\n",
      "Data loss: 0.3851945102214813, Function loss: 0.0338548980653286\n",
      "Step: 10, Loss: 0.4190494120121002\n",
      "Data loss: 0.3583681583404541, Function loss: 0.047939449548721313\n",
      "Step: 11, Loss: 0.4063076078891754\n",
      "Data loss: 0.33646196126937866, Function loss: 0.062214046716690063\n",
      "Step: 12, Loss: 0.3986760079860687\n",
      "Data loss: 0.3201841115951538, Function loss: 0.07337069511413574\n",
      "Step: 13, Loss: 0.39355480670928955\n",
      "Data loss: 0.3097040057182312, Function loss: 0.07863777875900269\n",
      "Step: 14, Loss: 0.3883417844772339\n",
      "Data loss: 0.30455589294433594, Function loss: 0.07757867127656937\n",
      "Step: 15, Loss: 0.3821345567703247\n",
      "Data loss: 0.30398431420326233, Function loss: 0.07187928259372711\n",
      "Step: 16, Loss: 0.37586361169815063\n",
      "Data loss: 0.3068605661392212, Function loss: 0.06416245549917221\n",
      "Step: 17, Loss: 0.371023029088974\n",
      "Data loss: 0.3117557168006897, Function loss: 0.05648019537329674\n",
      "Step: 18, Loss: 0.36823591589927673\n",
      "Data loss: 0.3170115649700165, Function loss: 0.0497979037463665\n",
      "Step: 19, Loss: 0.3668094575405121\n",
      "Data loss: 0.3212451934814453, Function loss: 0.04414213076233864\n",
      "Step: 20, Loss: 0.36538732051849365\n",
      "Data loss: 0.3236289620399475, Function loss: 0.03914867341518402\n",
      "Step: 21, Loss: 0.3627776503562927\n",
      "Data loss: 0.32399484515190125, Function loss: 0.034567590802907944\n",
      "Step: 22, Loss: 0.3585624396800995\n",
      "Data loss: 0.32257017493247986, Function loss: 0.030377190560102463\n",
      "Step: 23, Loss: 0.3529473543167114\n",
      "Data loss: 0.31985363364219666, Function loss: 0.026816388592123985\n",
      "Step: 24, Loss: 0.3466700315475464\n",
      "Data loss: 0.3164379298686981, Function loss: 0.024213751778006554\n",
      "Step: 25, Loss: 0.3406516909599304\n",
      "Data loss: 0.31292399764060974, Function loss: 0.022973883897066116\n",
      "Step: 26, Loss: 0.33589789271354675\n",
      "Data loss: 0.30983009934425354, Function loss: 0.023396389558911324\n",
      "Step: 27, Loss: 0.3332265019416809\n",
      "Data loss: 0.3074762523174286, Function loss: 0.025449544191360474\n",
      "Step: 28, Loss: 0.33292579650878906\n",
      "Data loss: 0.3058777153491974, Function loss: 0.028433727100491524\n",
      "Step: 29, Loss: 0.33431145548820496\n",
      "Data loss: 0.3047676980495453, Function loss: 0.031029080972075462\n",
      "Step: 30, Loss: 0.3357967734336853\n",
      "Data loss: 0.3037739098072052, Function loss: 0.03192305937409401\n",
      "Step: 31, Loss: 0.3356969654560089\n",
      "Data loss: 0.30269837379455566, Function loss: 0.03075742907822132\n",
      "Step: 32, Loss: 0.33345580101013184\n",
      "Data loss: 0.3015567660331726, Function loss: 0.028263960033655167\n",
      "Step: 33, Loss: 0.3298207223415375\n",
      "Data loss: 0.3004719913005829, Function loss: 0.02576647885143757\n",
      "Step: 34, Loss: 0.3262384831905365\n",
      "Data loss: 0.2994842529296875, Function loss: 0.024349240586161613\n",
      "Step: 35, Loss: 0.32383349537849426\n",
      "Data loss: 0.2984873950481415, Function loss: 0.0244440920650959\n",
      "Step: 36, Loss: 0.3229314982891083\n",
      "Data loss: 0.2972230315208435, Function loss: 0.025820259004831314\n",
      "Step: 37, Loss: 0.3230432868003845\n",
      "Data loss: 0.29546159505844116, Function loss: 0.02781473845243454\n",
      "Step: 38, Loss: 0.3232763409614563\n",
      "Data loss: 0.2931552529335022, Function loss: 0.02976626344025135\n",
      "Step: 39, Loss: 0.3229215145111084\n",
      "Data loss: 0.29050761461257935, Function loss: 0.031309083104133606\n",
      "Step: 40, Loss: 0.32181668281555176\n",
      "Data loss: 0.2879309058189392, Function loss: 0.03239777684211731\n",
      "Step: 41, Loss: 0.3203286826610565\n",
      "Data loss: 0.28589802980422974, Function loss: 0.0331355556845665\n",
      "Step: 42, Loss: 0.31903359293937683\n",
      "Data loss: 0.2847757637500763, Function loss: 0.03341205045580864\n",
      "Step: 43, Loss: 0.31818780303001404\n",
      "Data loss: 0.28468096256256104, Function loss: 0.032839495688676834\n",
      "Step: 44, Loss: 0.31752046942710876\n",
      "Data loss: 0.2855150103569031, Function loss: 0.03107503242790699\n",
      "Step: 45, Loss: 0.3165900409221649\n",
      "Data loss: 0.2870895266532898, Function loss: 0.028177373111248016\n",
      "Step: 46, Loss: 0.3152669072151184\n",
      "Data loss: 0.28912127017974854, Function loss: 0.024797214195132256\n",
      "Step: 47, Loss: 0.31391847133636475\n",
      "Data loss: 0.2913026213645935, Function loss: 0.02166680619120598\n",
      "Step: 48, Loss: 0.3129694163799286\n",
      "Data loss: 0.29327094554901123, Function loss: 0.019240733236074448\n",
      "Step: 49, Loss: 0.312511682510376\n",
      "Data loss: 0.29458391666412354, Function loss: 0.01764760911464691\n",
      "Step: 50, Loss: 0.31223154067993164\n",
      "Data loss: 0.29499322175979614, Function loss: 0.016721239313483238\n",
      "Step: 51, Loss: 0.3117144703865051\n",
      "Data loss: 0.2944237291812897, Function loss: 0.016336381435394287\n",
      "Step: 52, Loss: 0.31076011061668396\n",
      "Data loss: 0.29306524991989136, Function loss: 0.01645687408745289\n",
      "Step: 53, Loss: 0.3095221221446991\n",
      "Data loss: 0.291311115026474, Function loss: 0.017072709277272224\n",
      "Step: 54, Loss: 0.3083838224411011\n",
      "Data loss: 0.2895577549934387, Function loss: 0.018073348328471184\n",
      "Step: 55, Loss: 0.30763110518455505\n",
      "Data loss: 0.2881483733654022, Function loss: 0.01907138153910637\n",
      "Step: 56, Loss: 0.3072197437286377\n",
      "Data loss: 0.2872788608074188, Function loss: 0.019553961232304573\n",
      "Step: 57, Loss: 0.30683282017707825\n",
      "Data loss: 0.28696873784065247, Function loss: 0.01924719475209713\n",
      "Step: 58, Loss: 0.30621594190597534\n",
      "Data loss: 0.28716081380844116, Function loss: 0.018237708136439323\n",
      "Step: 59, Loss: 0.30539852380752563\n",
      "Data loss: 0.2876824140548706, Function loss: 0.016919678077101707\n",
      "Step: 60, Loss: 0.30460208654403687\n",
      "Data loss: 0.28826653957366943, Function loss: 0.015670904889702797\n",
      "Step: 61, Loss: 0.3039374351501465\n",
      "Data loss: 0.2886403799057007, Function loss: 0.014658336527645588\n",
      "Step: 62, Loss: 0.3032987117767334\n",
      "Data loss: 0.28863415122032166, Function loss: 0.013947919011116028\n",
      "Step: 63, Loss: 0.3025820851325989\n",
      "Data loss: 0.2882069945335388, Function loss: 0.01362325344234705\n",
      "Step: 64, Loss: 0.3018302619457245\n",
      "Data loss: 0.28747063875198364, Function loss: 0.013699468225240707\n",
      "Step: 65, Loss: 0.30117011070251465\n",
      "Data loss: 0.2865985929965973, Function loss: 0.014018765650689602\n",
      "Step: 66, Loss: 0.30061736702919006\n",
      "Data loss: 0.2856924831867218, Function loss: 0.014312848448753357\n",
      "Step: 67, Loss: 0.30000531673431396\n",
      "Data loss: 0.28483816981315613, Function loss: 0.01438149530440569\n",
      "Step: 68, Loss: 0.29921966791152954\n",
      "Data loss: 0.28404101729393005, Function loss: 0.01432999037206173\n",
      "Step: 69, Loss: 0.29837101697921753\n",
      "Data loss: 0.28327131271362305, Function loss: 0.014381305314600468\n",
      "Step: 70, Loss: 0.29765263199806213\n",
      "Data loss: 0.28241878747940063, Function loss: 0.01463671401143074\n",
      "Step: 71, Loss: 0.29705551266670227\n",
      "Data loss: 0.2813849151134491, Function loss: 0.015018374659121037\n",
      "Step: 72, Loss: 0.29640328884124756\n",
      "Data loss: 0.2802201807498932, Function loss: 0.01540429424494505\n",
      "Step: 73, Loss: 0.2956244647502899\n",
      "Data loss: 0.27907663583755493, Function loss: 0.015747439116239548\n",
      "Step: 74, Loss: 0.2948240637779236\n",
      "Data loss: 0.27815619111061096, Function loss: 0.01591370813548565\n",
      "Step: 75, Loss: 0.29406988620758057\n",
      "Data loss: 0.2775318920612335, Function loss: 0.015748944133520126\n",
      "Step: 76, Loss: 0.29328083992004395\n",
      "Data loss: 0.2771311402320862, Function loss: 0.015300719998776913\n",
      "Step: 77, Loss: 0.29243186116218567\n",
      "Data loss: 0.2767902612686157, Function loss: 0.014821489341557026\n",
      "Step: 78, Loss: 0.29161176085472107\n",
      "Data loss: 0.2762400507926941, Function loss: 0.014590291306376457\n",
      "Step: 79, Loss: 0.2908303439617157\n",
      "Data loss: 0.27528905868530273, Function loss: 0.014671017415821552\n",
      "Step: 80, Loss: 0.2899600863456726\n",
      "Data loss: 0.2739253044128418, Function loss: 0.015073243528604507\n",
      "Step: 81, Loss: 0.288998544216156\n",
      "Data loss: 0.27238011360168457, Function loss: 0.015683883801102638\n",
      "Step: 82, Loss: 0.28806400299072266\n",
      "Data loss: 0.2709207534790039, Function loss: 0.016244875267148018\n",
      "Step: 83, Loss: 0.28716564178466797\n",
      "Data loss: 0.26974961161613464, Function loss: 0.01645781472325325\n",
      "Step: 84, Loss: 0.2862074375152588\n",
      "Data loss: 0.2688746750354767, Function loss: 0.01632448099553585\n",
      "Step: 85, Loss: 0.2851991653442383\n",
      "Data loss: 0.2681174576282501, Function loss: 0.016071561723947525\n",
      "Step: 86, Loss: 0.28418901562690735\n",
      "Data loss: 0.26723599433898926, Function loss: 0.015902867540717125\n",
      "Step: 87, Loss: 0.28313887119293213\n",
      "Data loss: 0.26611241698265076, Function loss: 0.01592589169740677\n",
      "Step: 88, Loss: 0.28203830122947693\n",
      "Data loss: 0.2647773325443268, Function loss: 0.016169827431440353\n",
      "Step: 89, Loss: 0.28094714879989624\n",
      "Data loss: 0.2633861005306244, Function loss: 0.016429712995886803\n",
      "Step: 90, Loss: 0.27981582283973694\n",
      "Data loss: 0.2619733214378357, Function loss: 0.016632359474897385\n",
      "Step: 91, Loss: 0.2786056697368622\n",
      "Data loss: 0.26047104597091675, Function loss: 0.01691286265850067\n",
      "Step: 92, Loss: 0.2773839235305786\n",
      "Data loss: 0.2587827444076538, Function loss: 0.017358820885419846\n",
      "Step: 93, Loss: 0.27614155411720276\n",
      "Data loss: 0.25689902901649475, Function loss: 0.01793007180094719\n",
      "Step: 94, Loss: 0.27482908964157104\n",
      "Data loss: 0.2550772726535797, Function loss: 0.018402479588985443\n",
      "Step: 95, Loss: 0.27347975969314575\n",
      "Data loss: 0.2535070478916168, Function loss: 0.018572939559817314\n",
      "Step: 96, Loss: 0.2720799744129181\n",
      "Data loss: 0.25214293599128723, Function loss: 0.01848449371755123\n",
      "Step: 97, Loss: 0.2706274390220642\n",
      "Data loss: 0.2506078779697418, Function loss: 0.01854013279080391\n",
      "Step: 98, Loss: 0.2691480219364166\n",
      "Data loss: 0.24857117235660553, Function loss: 0.019016975536942482\n",
      "Step: 99, Loss: 0.26758813858032227\n",
      "Data loss: 0.24617135524749756, Function loss: 0.019801370799541473\n",
      "Step: 100, Loss: 0.26597273349761963\n",
      "Data loss: 0.24382324516773224, Function loss: 0.020489417016506195\n",
      "Step: 101, Loss: 0.26431265473365784\n",
      "Data loss: 0.24183005094528198, Function loss: 0.02075287327170372\n",
      "Step: 102, Loss: 0.262582927942276\n",
      "Data loss: 0.23990921676158905, Function loss: 0.020888544619083405\n",
      "Step: 103, Loss: 0.26079776883125305\n",
      "Data loss: 0.23769326508045197, Function loss: 0.021236687898635864\n",
      "Step: 104, Loss: 0.258929967880249\n",
      "Data loss: 0.2353028655052185, Function loss: 0.02171473391354084\n",
      "Step: 105, Loss: 0.2570176124572754\n",
      "Data loss: 0.23287302255630493, Function loss: 0.02214459702372551\n",
      "Step: 106, Loss: 0.25501760840415955\n",
      "Data loss: 0.23021875321865082, Function loss: 0.02273971028625965\n",
      "Step: 107, Loss: 0.2529584765434265\n",
      "Data loss: 0.22721751034259796, Function loss: 0.023613564670085907\n",
      "Step: 108, Loss: 0.25083106756210327\n",
      "Data loss: 0.22425608336925507, Function loss: 0.024373115971684456\n",
      "Step: 109, Loss: 0.24862919747829437\n",
      "Data loss: 0.22169330716133118, Function loss: 0.024665221571922302\n",
      "Step: 110, Loss: 0.24635852873325348\n",
      "Data loss: 0.21878397464752197, Function loss: 0.0252578966319561\n",
      "Step: 111, Loss: 0.24404187500476837\n",
      "Data loss: 0.21503131091594696, Function loss: 0.02662483975291252\n",
      "Step: 112, Loss: 0.24165615439414978\n",
      "Data loss: 0.21155892312526703, Function loss: 0.02768709510564804\n",
      "Step: 113, Loss: 0.23924601078033447\n",
      "Data loss: 0.20856842398643494, Function loss: 0.02825319394469261\n",
      "Step: 114, Loss: 0.23682162165641785\n",
      "Data loss: 0.20484839379787445, Function loss: 0.029559029266238213\n",
      "Step: 115, Loss: 0.2344074249267578\n",
      "Data loss: 0.20148135721683502, Function loss: 0.03054390661418438\n",
      "Step: 116, Loss: 0.23202526569366455\n",
      "Data loss: 0.1976795494556427, Function loss: 0.032056745141744614\n",
      "Step: 117, Loss: 0.2297362983226776\n",
      "Data loss: 0.19393068552017212, Function loss: 0.03363494947552681\n",
      "Step: 118, Loss: 0.22756563127040863\n",
      "Data loss: 0.19125866889953613, Function loss: 0.034323856234550476\n",
      "Step: 119, Loss: 0.2255825251340866\n",
      "Data loss: 0.1859554499387741, Function loss: 0.037877317517995834\n",
      "Step: 120, Loss: 0.22383277118206024\n",
      "Data loss: 0.18637481331825256, Function loss: 0.03631773963570595\n",
      "Step: 121, Loss: 0.2226925492286682\n",
      "Data loss: 0.1749926209449768, Function loss: 0.04904275760054588\n",
      "Step: 122, Loss: 0.224035382270813\n",
      "Data loss: 0.19357435405254364, Function loss: 0.03786470368504524\n",
      "Step: 123, Loss: 0.23143905401229858\n",
      "Data loss: 0.16849005222320557, Function loss: 0.05597810819745064\n",
      "Step: 124, Loss: 0.2244681566953659\n",
      "Data loss: 0.17152394354343414, Function loss: 0.04739557206630707\n",
      "Step: 125, Loss: 0.2189195156097412\n",
      "Data loss: 0.18591685593128204, Function loss: 0.03930108994245529\n",
      "Step: 126, Loss: 0.22521793842315674\n",
      "Data loss: 0.16981182992458344, Function loss: 0.046892523765563965\n",
      "Step: 127, Loss: 0.2167043536901474\n",
      "Data loss: 0.16479766368865967, Function loss: 0.055590588599443436\n",
      "Step: 128, Loss: 0.2203882485628128\n",
      "Data loss: 0.17857243120670319, Function loss: 0.03887474909424782\n",
      "Step: 129, Loss: 0.2174471765756607\n",
      "Data loss: 0.17646603286266327, Function loss: 0.03908079117536545\n",
      "Step: 130, Loss: 0.21554681658744812\n",
      "Data loss: 0.1644931137561798, Function loss: 0.05229993909597397\n",
      "Step: 131, Loss: 0.21679306030273438\n",
      "Data loss: 0.16933435201644897, Function loss: 0.043180521577596664\n",
      "Step: 132, Loss: 0.21251487731933594\n",
      "Data loss: 0.17896701395511627, Function loss: 0.036131177097558975\n",
      "Step: 133, Loss: 0.21509818732738495\n",
      "Data loss: 0.17168095707893372, Function loss: 0.039291590452194214\n",
      "Step: 134, Loss: 0.21097254753112793\n",
      "Data loss: 0.16550233960151672, Function loss: 0.047971729189157486\n",
      "Step: 135, Loss: 0.2134740650653839\n",
      "Data loss: 0.1724189817905426, Function loss: 0.03747667372226715\n",
      "Step: 136, Loss: 0.20989565551280975\n",
      "Data loss: 0.1776285618543625, Function loss: 0.034018825739622116\n",
      "Step: 137, Loss: 0.2116473913192749\n",
      "Data loss: 0.1700291782617569, Function loss: 0.038882046937942505\n",
      "Step: 138, Loss: 0.2089112251996994\n",
      "Data loss: 0.16660365462303162, Function loss: 0.043476782739162445\n",
      "Step: 139, Loss: 0.21008044481277466\n",
      "Data loss: 0.17295138537883759, Function loss: 0.03505289554595947\n",
      "Step: 140, Loss: 0.20800428092479706\n",
      "Data loss: 0.17521582543849945, Function loss: 0.03330020606517792\n",
      "Step: 141, Loss: 0.20851603150367737\n",
      "Data loss: 0.1683458834886551, Function loss: 0.03865785896778107\n",
      "Step: 142, Loss: 0.20700374245643616\n",
      "Data loss: 0.16656441986560822, Function loss: 0.04047207161784172\n",
      "Step: 143, Loss: 0.20703649520874023\n",
      "Data loss: 0.17196811735630035, Function loss: 0.034098584204912186\n",
      "Step: 144, Loss: 0.20606669783592224\n",
      "Data loss: 0.17191077768802643, Function loss: 0.03369004651904106\n",
      "Step: 145, Loss: 0.2056008279323578\n",
      "Data loss: 0.16603145003318787, Function loss: 0.03905453905463219\n",
      "Step: 146, Loss: 0.20508599281311035\n",
      "Data loss: 0.1659504622220993, Function loss: 0.038260575383901596\n",
      "Step: 147, Loss: 0.2042110413312912\n",
      "Data loss: 0.17020702362060547, Function loss: 0.03390570357441902\n",
      "Step: 148, Loss: 0.2041127234697342\n",
      "Data loss: 0.16776429116725922, Function loss: 0.03516848012804985\n",
      "Step: 149, Loss: 0.20293277502059937\n",
      "Data loss: 0.1634892076253891, Function loss: 0.03953881561756134\n",
      "Step: 150, Loss: 0.20302802324295044\n",
      "Data loss: 0.16540005803108215, Function loss: 0.036393024027347565\n",
      "Step: 151, Loss: 0.20179307460784912\n",
      "Data loss: 0.1672724336385727, Function loss: 0.034497279673814774\n",
      "Step: 152, Loss: 0.20176970958709717\n",
      "Data loss: 0.1631905734539032, Function loss: 0.037616830319166183\n",
      "Step: 153, Loss: 0.20080740749835968\n",
      "Data loss: 0.16183415055274963, Function loss: 0.03850993514060974\n",
      "Step: 154, Loss: 0.20034408569335938\n",
      "Data loss: 0.16460078954696655, Function loss: 0.035261478275060654\n",
      "Step: 155, Loss: 0.1998622715473175\n",
      "Data loss: 0.16283828020095825, Function loss: 0.03611055016517639\n",
      "Step: 156, Loss: 0.19894883036613464\n",
      "Data loss: 0.15985402464866638, Function loss: 0.03887724503874779\n",
      "Step: 157, Loss: 0.19873127341270447\n",
      "Data loss: 0.1617131382226944, Function loss: 0.03608033433556557\n",
      "Step: 158, Loss: 0.19779346883296967\n",
      "Data loss: 0.16171717643737793, Function loss: 0.03559596836566925\n",
      "Step: 159, Loss: 0.19731314480304718\n",
      "Data loss: 0.1584647297859192, Function loss: 0.0383208766579628\n",
      "Step: 160, Loss: 0.1967855989933014\n",
      "Data loss: 0.15934869647026062, Function loss: 0.03659563139081001\n",
      "Step: 161, Loss: 0.19594432413578033\n",
      "Data loss: 0.16015808284282684, Function loss: 0.0353957936167717\n",
      "Step: 162, Loss: 0.19555386900901794\n",
      "Data loss: 0.15710191428661346, Function loss: 0.03773447498679161\n",
      "Step: 163, Loss: 0.19483639299869537\n",
      "Data loss: 0.1573060154914856, Function loss: 0.03686210513114929\n",
      "Step: 164, Loss: 0.1941681206226349\n",
      "Data loss: 0.15845146775245667, Function loss: 0.03525983914732933\n",
      "Step: 165, Loss: 0.1937113106250763\n",
      "Data loss: 0.1555238664150238, Function loss: 0.03744196891784668\n",
      "Step: 166, Loss: 0.19296583533287048\n",
      "Data loss: 0.15558487176895142, Function loss: 0.03673224523663521\n",
      "Step: 167, Loss: 0.19231711328029633\n",
      "Data loss: 0.15662628412246704, Function loss: 0.03516574949026108\n",
      "Step: 168, Loss: 0.19179204106330872\n",
      "Data loss: 0.15371790528297424, Function loss: 0.037363916635513306\n",
      "Step: 169, Loss: 0.19108182191848755\n",
      "Data loss: 0.1541426032781601, Function loss: 0.036240290850400925\n",
      "Step: 170, Loss: 0.19038289785385132\n",
      "Data loss: 0.15476097166538239, Function loss: 0.03505343571305275\n",
      "Step: 171, Loss: 0.18981440365314484\n",
      "Data loss: 0.15181797742843628, Function loss: 0.03735250234603882\n",
      "Step: 172, Loss: 0.1891704797744751\n",
      "Data loss: 0.15295779705047607, Function loss: 0.03546980768442154\n",
      "Step: 173, Loss: 0.18842759728431702\n",
      "Data loss: 0.15247979760169983, Function loss: 0.0352921187877655\n",
      "Step: 174, Loss: 0.18777191638946533\n",
      "Data loss: 0.15024271607398987, Function loss: 0.03691503778100014\n",
      "Step: 175, Loss: 0.1871577501296997\n",
      "Data loss: 0.15175172686576843, Function loss: 0.034722957760095596\n",
      "Step: 176, Loss: 0.18647468090057373\n",
      "Data loss: 0.14986295998096466, Function loss: 0.03588564321398735\n",
      "Step: 177, Loss: 0.1857486069202423\n",
      "Data loss: 0.14923453330993652, Function loss: 0.03583027422428131\n",
      "Step: 178, Loss: 0.18506480753421783\n",
      "Data loss: 0.14992885291576385, Function loss: 0.03448743745684624\n",
      "Step: 179, Loss: 0.1844162940979004\n",
      "Data loss: 0.14751869440078735, Function loss: 0.036234062165021896\n",
      "Step: 180, Loss: 0.18375276029109955\n",
      "Data loss: 0.14875459671020508, Function loss: 0.034299302846193314\n",
      "Step: 181, Loss: 0.1830538958311081\n",
      "Data loss: 0.14669616520404816, Function loss: 0.03564056009054184\n",
      "Step: 182, Loss: 0.1823367178440094\n",
      "Data loss: 0.14713400602340698, Function loss: 0.034480251371860504\n",
      "Step: 183, Loss: 0.1816142499446869\n",
      "Data loss: 0.1456851065158844, Function loss: 0.03520672768354416\n",
      "Step: 184, Loss: 0.18089184165000916\n",
      "Data loss: 0.1458151489496231, Function loss: 0.03436160832643509\n",
      "Step: 185, Loss: 0.1801767647266388\n",
      "Data loss: 0.14438845217227936, Function loss: 0.03507716953754425\n",
      "Step: 186, Loss: 0.1794656217098236\n",
      "Data loss: 0.1443493813276291, Function loss: 0.03439527750015259\n",
      "Step: 187, Loss: 0.17874465882778168\n",
      "Data loss: 0.14333954453468323, Function loss: 0.034688688814640045\n",
      "Step: 188, Loss: 0.17802822589874268\n",
      "Data loss: 0.14257074892520905, Function loss: 0.0347372405230999\n",
      "Step: 189, Loss: 0.17730799317359924\n",
      "Data loss: 0.14252689480781555, Function loss: 0.0340794213116169\n",
      "Step: 190, Loss: 0.17660631239414215\n",
      "Data loss: 0.1401667296886444, Function loss: 0.03578775376081467\n",
      "Step: 191, Loss: 0.17595449090003967\n",
      "Data loss: 0.14312376081943512, Function loss: 0.03245009481906891\n",
      "Step: 192, Loss: 0.17557385563850403\n",
      "Data loss: 0.1348639726638794, Function loss: 0.04160807654261589\n",
      "Step: 193, Loss: 0.17647205293178558\n",
      "Data loss: 0.15176105499267578, Function loss: 0.031378164887428284\n",
      "Step: 194, Loss: 0.18313921988010406\n",
      "Data loss: 0.12512770295143127, Function loss: 0.07509241253137589\n",
      "Step: 195, Loss: 0.20022010803222656\n",
      "Data loss: 0.1631125807762146, Function loss: 0.04426627978682518\n",
      "Step: 196, Loss: 0.20737886428833008\n",
      "Data loss: 0.14182184636592865, Function loss: 0.03380870446562767\n",
      "Step: 197, Loss: 0.17563055455684662\n",
      "Data loss: 0.12365604192018509, Function loss: 0.07163692265748978\n",
      "Step: 198, Loss: 0.19529296457767487\n",
      "Data loss: 0.15097260475158691, Function loss: 0.034084174782037735\n",
      "Step: 199, Loss: 0.18505677580833435\n",
      "Data loss: 0.1543997824192047, Function loss: 0.02737451158463955\n",
      "Step: 200, Loss: 0.1817742884159088\n",
      "Data loss: 0.13410936295986176, Function loss: 0.049238018691539764\n",
      "Step: 201, Loss: 0.18334737420082092\n",
      "Data loss: 0.13027699291706085, Function loss: 0.04740973562002182\n",
      "Step: 202, Loss: 0.17768672108650208\n",
      "Data loss: 0.15020005404949188, Function loss: 0.03020864725112915\n",
      "Step: 203, Loss: 0.18040870130062103\n",
      "Data loss: 0.15084302425384521, Function loss: 0.025624580681324005\n",
      "Step: 204, Loss: 0.17646759748458862\n",
      "Data loss: 0.13666392862796783, Function loss: 0.04046148434281349\n",
      "Step: 205, Loss: 0.17712540924549103\n",
      "Data loss: 0.1324429214000702, Function loss: 0.04248796030879021\n",
      "Step: 206, Loss: 0.1749308854341507\n",
      "Data loss: 0.14484351873397827, Function loss: 0.030428707599639893\n",
      "Step: 207, Loss: 0.17527222633361816\n",
      "Data loss: 0.14756187796592712, Function loss: 0.025393053889274597\n",
      "Step: 208, Loss: 0.17295493185520172\n",
      "Data loss: 0.1395203173160553, Function loss: 0.034450072795152664\n",
      "Step: 209, Loss: 0.17397038638591766\n",
      "Data loss: 0.13333623111248016, Function loss: 0.037502389401197433\n",
      "Step: 210, Loss: 0.1708386242389679\n",
      "Data loss: 0.1396186500787735, Function loss: 0.033494122326374054\n",
      "Step: 211, Loss: 0.17311277985572815\n",
      "Data loss: 0.1420440375804901, Function loss: 0.02675340697169304\n",
      "Step: 212, Loss: 0.16879744827747345\n",
      "Data loss: 0.1394321322441101, Function loss: 0.03274809196591377\n",
      "Step: 213, Loss: 0.17218022048473358\n",
      "Data loss: 0.13455763459205627, Function loss: 0.03268452733755112\n",
      "Step: 214, Loss: 0.167242169380188\n",
      "Data loss: 0.13604947924613953, Function loss: 0.03485274687409401\n",
      "Step: 215, Loss: 0.17090222239494324\n",
      "Data loss: 0.136049285531044, Function loss: 0.030169323086738586\n",
      "Step: 216, Loss: 0.1662186086177826\n",
      "Data loss: 0.13668842613697052, Function loss: 0.032811738550662994\n",
      "Step: 217, Loss: 0.1695001721382141\n",
      "Data loss: 0.13514511287212372, Function loss: 0.03036857396364212\n",
      "Step: 218, Loss: 0.16551369428634644\n",
      "Data loss: 0.13367629051208496, Function loss: 0.03421969339251518\n",
      "Step: 219, Loss: 0.16789598762989044\n",
      "Data loss: 0.13092584908008575, Function loss: 0.03408209607005119\n",
      "Step: 220, Loss: 0.16500794887542725\n",
      "Data loss: 0.13330671191215515, Function loss: 0.033063024282455444\n",
      "Step: 221, Loss: 0.1663697361946106\n",
      "Data loss: 0.13480183482170105, Function loss: 0.029716910794377327\n",
      "Step: 222, Loss: 0.16451874375343323\n",
      "Data loss: 0.13121624290943146, Function loss: 0.033701881766319275\n",
      "Step: 223, Loss: 0.16491812467575073\n",
      "Data loss: 0.12703081965446472, Function loss: 0.03701338917016983\n",
      "Step: 224, Loss: 0.16404420137405396\n",
      "Data loss: 0.13093677163124084, Function loss: 0.03266136720776558\n",
      "Step: 225, Loss: 0.16359813511371613\n",
      "Data loss: 0.13367000222206116, Function loss: 0.02976849675178528\n",
      "Step: 226, Loss: 0.16343849897384644\n",
      "Data loss: 0.12814809381961823, Function loss: 0.03431688994169235\n",
      "Step: 227, Loss: 0.16246497631072998\n",
      "Data loss: 0.12470898032188416, Function loss: 0.037995610386133194\n",
      "Step: 228, Loss: 0.16270458698272705\n",
      "Data loss: 0.1300158053636551, Function loss: 0.031602926552295685\n",
      "Step: 229, Loss: 0.16161873936653137\n",
      "Data loss: 0.1312386840581894, Function loss: 0.030492419376969337\n",
      "Step: 230, Loss: 0.16173110902309418\n",
      "Data loss: 0.12514494359493256, Function loss: 0.03574325516819954\n",
      "Step: 231, Loss: 0.1608881950378418\n",
      "Data loss: 0.12436302751302719, Function loss: 0.036368872970342636\n",
      "Step: 232, Loss: 0.16073189675807953\n",
      "Data loss: 0.12901176512241364, Function loss: 0.031219398602843285\n",
      "Step: 233, Loss: 0.16023115813732147\n",
      "Data loss: 0.1277616322040558, Function loss: 0.032063309103250504\n",
      "Step: 234, Loss: 0.159824937582016\n",
      "Data loss: 0.123393714427948, Function loss: 0.03605106472969055\n",
      "Step: 235, Loss: 0.15944477915763855\n",
      "Data loss: 0.12480317801237106, Function loss: 0.034237753599882126\n",
      "Step: 236, Loss: 0.1590409278869629\n",
      "Data loss: 0.12684963643550873, Function loss: 0.031726986169815063\n",
      "Step: 237, Loss: 0.1585766226053238\n",
      "Data loss: 0.12462367117404938, Function loss: 0.03368823230266571\n",
      "Step: 238, Loss: 0.1583119034767151\n",
      "Data loss: 0.12320562452077866, Function loss: 0.03454117104411125\n",
      "Step: 239, Loss: 0.1577467918395996\n",
      "Data loss: 0.12425293028354645, Function loss: 0.033218864351511\n",
      "Step: 240, Loss: 0.15747179090976715\n",
      "Data loss: 0.1240098774433136, Function loss: 0.03301479294896126\n",
      "Step: 241, Loss: 0.15702466666698456\n",
      "Data loss: 0.12320388853549957, Function loss: 0.03335264325141907\n",
      "Step: 242, Loss: 0.15655653178691864\n",
      "Data loss: 0.12297901511192322, Function loss: 0.03335493803024292\n",
      "Step: 243, Loss: 0.15633395314216614\n",
      "Data loss: 0.1220669150352478, Function loss: 0.0336218886077404\n",
      "Step: 244, Loss: 0.1556888073682785\n",
      "Data loss: 0.12247589975595474, Function loss: 0.033005572855472565\n",
      "Step: 245, Loss: 0.1554814726114273\n",
      "Data loss: 0.12274688482284546, Function loss: 0.03219081088900566\n",
      "Step: 246, Loss: 0.15493769943714142\n",
      "Data loss: 0.12075018137693405, Function loss: 0.03385593742132187\n",
      "Step: 247, Loss: 0.15460611879825592\n",
      "Data loss: 0.12068542838096619, Function loss: 0.03344263136386871\n",
      "Step: 248, Loss: 0.1541280597448349\n",
      "Data loss: 0.12224733829498291, Function loss: 0.031565241515636444\n",
      "Step: 249, Loss: 0.15381258726119995\n",
      "Data loss: 0.12031545490026474, Function loss: 0.03297565132379532\n",
      "Step: 250, Loss: 0.15329110622406006\n",
      "Data loss: 0.11904776096343994, Function loss: 0.0339147113263607\n",
      "Step: 251, Loss: 0.15296247601509094\n",
      "Data loss: 0.12103338539600372, Function loss: 0.0315009169280529\n",
      "Step: 252, Loss: 0.15253430604934692\n",
      "Data loss: 0.11997053772211075, Function loss: 0.032082170248031616\n",
      "Step: 253, Loss: 0.15205270051956177\n",
      "Data loss: 0.11805828660726547, Function loss: 0.03366940841078758\n",
      "Step: 254, Loss: 0.15172769129276276\n",
      "Data loss: 0.11947768926620483, Function loss: 0.03175467625260353\n",
      "Step: 255, Loss: 0.15123236179351807\n",
      "Data loss: 0.11924116313457489, Function loss: 0.03160427510738373\n",
      "Step: 256, Loss: 0.15084543824195862\n",
      "Data loss: 0.1174137219786644, Function loss: 0.03299373388290405\n",
      "Step: 257, Loss: 0.15040746331214905\n",
      "Data loss: 0.11817555874586105, Function loss: 0.031823448836803436\n",
      "Step: 258, Loss: 0.1499990075826645\n",
      "Data loss: 0.11792648583650589, Function loss: 0.031606364995241165\n",
      "Step: 259, Loss: 0.14953285455703735\n",
      "Data loss: 0.11698076874017715, Function loss: 0.03210904821753502\n",
      "Step: 260, Loss: 0.14908981323242188\n",
      "Data loss: 0.11712883412837982, Function loss: 0.03157198056578636\n",
      "Step: 261, Loss: 0.14870081841945648\n",
      "Data loss: 0.1164369210600853, Function loss: 0.03176320716738701\n",
      "Step: 262, Loss: 0.148200124502182\n",
      "Data loss: 0.11633840203285217, Function loss: 0.03143514692783356\n",
      "Step: 263, Loss: 0.14777354896068573\n",
      "Data loss: 0.11615816503763199, Function loss: 0.03118392452597618\n",
      "Step: 264, Loss: 0.14734208583831787\n",
      "Data loss: 0.11504162102937698, Function loss: 0.031827233731746674\n",
      "Step: 265, Loss: 0.14686885476112366\n",
      "Data loss: 0.11555509269237518, Function loss: 0.030861258506774902\n",
      "Step: 266, Loss: 0.14641635119915009\n",
      "Data loss: 0.11507532000541687, Function loss: 0.03087182529270649\n",
      "Step: 267, Loss: 0.1459471434354782\n",
      "Data loss: 0.11383815854787827, Function loss: 0.031664542853832245\n",
      "Step: 268, Loss: 0.1455027014017105\n",
      "Data loss: 0.11457186937332153, Function loss: 0.030452772974967957\n",
      "Step: 269, Loss: 0.1450246423482895\n",
      "Data loss: 0.11386139690876007, Function loss: 0.030675766989588737\n",
      "Step: 270, Loss: 0.14453716576099396\n",
      "Data loss: 0.11288075149059296, Function loss: 0.031199967488646507\n",
      "Step: 271, Loss: 0.14408071339130402\n",
      "Data loss: 0.11328422278165817, Function loss: 0.0303074661642313\n",
      "Step: 272, Loss: 0.14359168708324432\n",
      "Data loss: 0.11279068887233734, Function loss: 0.030317218974232674\n",
      "Step: 273, Loss: 0.14310790598392487\n",
      "Data loss: 0.11178478598594666, Function loss: 0.03084232471883297\n",
      "Step: 274, Loss: 0.14262710511684418\n",
      "Data loss: 0.11211903393268585, Function loss: 0.03001318871974945\n",
      "Step: 275, Loss: 0.1421322226524353\n",
      "Data loss: 0.11135272681713104, Function loss: 0.030284347012639046\n",
      "Step: 276, Loss: 0.14163707196712494\n",
      "Data loss: 0.11105617135763168, Function loss: 0.030067475512623787\n",
      "Step: 277, Loss: 0.14112365245819092\n",
      "Data loss: 0.11057273298501968, Function loss: 0.03004257008433342\n",
      "Step: 278, Loss: 0.1406152993440628\n",
      "Data loss: 0.1102554127573967, Function loss: 0.029856937006115913\n",
      "Step: 279, Loss: 0.14011235535144806\n",
      "Data loss: 0.10996770113706589, Function loss: 0.02963191084563732\n",
      "Step: 280, Loss: 0.13959960639476776\n",
      "Data loss: 0.10909413546323776, Function loss: 0.029983127489686012\n",
      "Step: 281, Loss: 0.13907726109027863\n",
      "Data loss: 0.1094622015953064, Function loss: 0.029090024530887604\n",
      "Step: 282, Loss: 0.1385522186756134\n",
      "Data loss: 0.10825437307357788, Function loss: 0.029764598235487938\n",
      "Step: 283, Loss: 0.13801896572113037\n",
      "Data loss: 0.10833397507667542, Function loss: 0.02913365513086319\n",
      "Step: 284, Loss: 0.137467622756958\n",
      "Data loss: 0.10796184092760086, Function loss: 0.028965434059500694\n",
      "Step: 285, Loss: 0.1369272768497467\n",
      "Data loss: 0.10705869644880295, Function loss: 0.02932531200349331\n",
      "Step: 286, Loss: 0.1363840103149414\n",
      "Data loss: 0.1073462963104248, Function loss: 0.028481852263212204\n",
      "Step: 287, Loss: 0.1358281522989273\n",
      "Data loss: 0.10618184506893158, Function loss: 0.029093127697706223\n",
      "Step: 288, Loss: 0.1352749764919281\n",
      "Data loss: 0.10647047311067581, Function loss: 0.028236746788024902\n",
      "Step: 289, Loss: 0.13470721244812012\n",
      "Data loss: 0.10538946092128754, Function loss: 0.02873954176902771\n",
      "Step: 290, Loss: 0.13412900269031525\n",
      "Data loss: 0.10557094216346741, Function loss: 0.02797078713774681\n",
      "Step: 291, Loss: 0.13354173302650452\n",
      "Data loss: 0.10460308194160461, Function loss: 0.02833900973200798\n",
      "Step: 292, Loss: 0.1329420953989029\n",
      "Data loss: 0.10454082489013672, Function loss: 0.02779845893383026\n",
      "Step: 293, Loss: 0.13233928382396698\n",
      "Data loss: 0.10393059253692627, Function loss: 0.027808377519249916\n",
      "Step: 294, Loss: 0.13173897564411163\n",
      "Data loss: 0.10343152284622192, Function loss: 0.027697037905454636\n",
      "Step: 295, Loss: 0.13112856447696686\n",
      "Data loss: 0.10326541215181351, Function loss: 0.027249587699770927\n",
      "Step: 296, Loss: 0.130514994263649\n",
      "Data loss: 0.10219895839691162, Function loss: 0.02770073525607586\n",
      "Step: 297, Loss: 0.12989969551563263\n",
      "Data loss: 0.10278598964214325, Function loss: 0.026506522670388222\n",
      "Step: 298, Loss: 0.12929251790046692\n",
      "Data loss: 0.10058925300836563, Function loss: 0.028150418773293495\n",
      "Step: 299, Loss: 0.12873966991901398\n",
      "Data loss: 0.10305219888687134, Function loss: 0.025306925177574158\n",
      "Step: 300, Loss: 0.1283591240644455\n",
      "Data loss: 0.0976671501994133, Function loss: 0.03075924701988697\n",
      "Step: 301, Loss: 0.12842640280723572\n",
      "Data loss: 0.10595852136611938, Function loss: 0.024035656824707985\n",
      "Step: 302, Loss: 0.12999418377876282\n",
      "Data loss: 0.0916866660118103, Function loss: 0.04322116822004318\n",
      "Step: 303, Loss: 0.13490784168243408\n",
      "Data loss: 0.11526443809270859, Function loss: 0.03264182433485985\n",
      "Step: 304, Loss: 0.14790625870227814\n",
      "Data loss: 0.08688876032829285, Function loss: 0.06538659334182739\n",
      "Step: 305, Loss: 0.15227535367012024\n",
      "Data loss: 0.11204613000154495, Function loss: 0.030042096972465515\n",
      "Step: 306, Loss: 0.14208823442459106\n",
      "Data loss: 0.09925484657287598, Function loss: 0.024798348546028137\n",
      "Step: 307, Loss: 0.12405319511890411\n",
      "Data loss: 0.08868343383073807, Function loss: 0.0477592758834362\n",
      "Step: 308, Loss: 0.13644270598888397\n",
      "Data loss: 0.11179831624031067, Function loss: 0.028279287740588188\n",
      "Step: 309, Loss: 0.140077605843544\n",
      "Data loss: 0.09956925362348557, Function loss: 0.02336103469133377\n",
      "Step: 310, Loss: 0.12293028831481934\n",
      "Data loss: 0.08763092756271362, Function loss: 0.0478920079767704\n",
      "Step: 311, Loss: 0.13552293181419373\n",
      "Data loss: 0.10800478607416153, Function loss: 0.023498814553022385\n",
      "Step: 312, Loss: 0.13150359690189362\n",
      "Data loss: 0.10347672551870346, Function loss: 0.019804341718554497\n",
      "Step: 313, Loss: 0.1232810690999031\n",
      "Data loss: 0.08747771382331848, Function loss: 0.04572228342294693\n",
      "Step: 314, Loss: 0.13319998979568481\n",
      "Data loss: 0.10031933337450027, Function loss: 0.02116876095533371\n",
      "Step: 315, Loss: 0.12148809432983398\n",
      "Data loss: 0.10633780807256699, Function loss: 0.01970638334751129\n",
      "Step: 316, Loss: 0.12604418396949768\n",
      "Data loss: 0.09138954430818558, Function loss: 0.03141387179493904\n",
      "Step: 317, Loss: 0.12280341982841492\n",
      "Data loss: 0.09171247482299805, Function loss: 0.028656121343374252\n",
      "Step: 318, Loss: 0.1203685998916626\n",
      "Data loss: 0.10418012738227844, Function loss: 0.018965644761919975\n",
      "Step: 319, Loss: 0.12314577400684357\n",
      "Data loss: 0.09769795089960098, Function loss: 0.019829757511615753\n",
      "Step: 320, Loss: 0.11752770841121674\n",
      "Data loss: 0.08779622614383698, Function loss: 0.034249309450387955\n",
      "Step: 321, Loss: 0.12204553186893463\n",
      "Data loss: 0.0971142053604126, Function loss: 0.019244898110628128\n",
      "Step: 322, Loss: 0.11635909974575043\n",
      "Data loss: 0.10080108046531677, Function loss: 0.01791664958000183\n",
      "Step: 323, Loss: 0.1187177300453186\n",
      "Data loss: 0.0896257758140564, Function loss: 0.026447683572769165\n",
      "Step: 324, Loss: 0.11607345938682556\n",
      "Data loss: 0.08972286432981491, Function loss: 0.02540590614080429\n",
      "Step: 325, Loss: 0.1151287704706192\n",
      "Data loss: 0.09832245111465454, Function loss: 0.01755698397755623\n",
      "Step: 326, Loss: 0.11587943136692047\n",
      "Data loss: 0.0933721587061882, Function loss: 0.019114485010504723\n",
      "Step: 327, Loss: 0.11248664557933807\n",
      "Data loss: 0.0862942785024643, Function loss: 0.02841932326555252\n",
      "Step: 328, Loss: 0.11471360176801682\n",
      "Data loss: 0.09244772791862488, Function loss: 0.018633965402841568\n",
      "Step: 329, Loss: 0.11108168959617615\n",
      "Data loss: 0.09473726898431778, Function loss: 0.017405172809958458\n",
      "Step: 330, Loss: 0.11214244365692139\n",
      "Data loss: 0.08649542927742004, Function loss: 0.024126948788762093\n",
      "Step: 331, Loss: 0.11062237620353699\n",
      "Data loss: 0.08695722371339798, Function loss: 0.022224178537726402\n",
      "Step: 332, Loss: 0.10918140411376953\n",
      "Data loss: 0.09258299320936203, Function loss: 0.017232365906238556\n",
      "Step: 333, Loss: 0.10981535911560059\n",
      "Data loss: 0.08762086182832718, Function loss: 0.019618507474660873\n",
      "Step: 334, Loss: 0.10723936557769775\n",
      "Data loss: 0.08351847529411316, Function loss: 0.024399328976869583\n",
      "Step: 335, Loss: 0.10791780054569244\n",
      "Data loss: 0.08870561420917511, Function loss: 0.017646217718720436\n",
      "Step: 336, Loss: 0.1063518300652504\n",
      "Data loss: 0.08765619993209839, Function loss: 0.01770445704460144\n",
      "Step: 337, Loss: 0.10536065697669983\n",
      "Data loss: 0.08189550787210464, Function loss: 0.023443464189767838\n",
      "Step: 338, Loss: 0.10533897578716278\n",
      "Data loss: 0.08479767292737961, Function loss: 0.01866007223725319\n",
      "Step: 339, Loss: 0.1034577488899231\n",
      "Data loss: 0.0863003358244896, Function loss: 0.017089225351810455\n",
      "Step: 340, Loss: 0.10338956117630005\n",
      "Data loss: 0.08073291182518005, Function loss: 0.0216476172208786\n",
      "Step: 341, Loss: 0.10238052904605865\n",
      "Data loss: 0.08158943057060242, Function loss: 0.019471699371933937\n",
      "Step: 342, Loss: 0.1010611280798912\n",
      "Data loss: 0.08401056379079819, Function loss: 0.016901781782507896\n",
      "Step: 343, Loss: 0.10091234743595123\n",
      "Data loss: 0.07942244410514832, Function loss: 0.02014416642487049\n",
      "Step: 344, Loss: 0.09956660866737366\n",
      "Data loss: 0.07900319993495941, Function loss: 0.019655661657452583\n",
      "Step: 345, Loss: 0.09865885972976685\n",
      "Data loss: 0.08159487694501877, Function loss: 0.016694335266947746\n",
      "Step: 346, Loss: 0.09828921407461166\n",
      "Data loss: 0.07759641110897064, Function loss: 0.019352173432707787\n",
      "Step: 347, Loss: 0.09694858640432358\n",
      "Data loss: 0.07686854898929596, Function loss: 0.01921350695192814\n",
      "Step: 348, Loss: 0.09608205407857895\n",
      "Data loss: 0.07914403080940247, Function loss: 0.016444819048047066\n",
      "Step: 349, Loss: 0.09558884799480438\n",
      "Data loss: 0.07536614686250687, Function loss: 0.018995599821209908\n",
      "Step: 350, Loss: 0.09436174482107162\n",
      "Data loss: 0.07504691183567047, Function loss: 0.01832091249525547\n",
      "Step: 351, Loss: 0.09336782246828079\n",
      "Data loss: 0.07657460868358612, Function loss: 0.01619122549891472\n",
      "Step: 352, Loss: 0.09276583790779114\n",
      "Data loss: 0.07306130975484848, Function loss: 0.01863507553935051\n",
      "Step: 353, Loss: 0.09169638156890869\n",
      "Data loss: 0.07335399091243744, Function loss: 0.01725252903997898\n",
      "Step: 354, Loss: 0.09060651808977127\n",
      "Data loss: 0.0739319920539856, Function loss: 0.015950169414281845\n",
      "Step: 355, Loss: 0.08988216519355774\n",
      "Data loss: 0.07065704464912415, Function loss: 0.018337160348892212\n",
      "Step: 356, Loss: 0.08899420499801636\n",
      "Data loss: 0.07182734459638596, Function loss: 0.01601979322731495\n",
      "Step: 357, Loss: 0.08784713596105576\n",
      "Data loss: 0.07087214291095734, Function loss: 0.016003359109163284\n",
      "Step: 358, Loss: 0.08687549829483032\n",
      "Data loss: 0.06871055066585541, Function loss: 0.017347019165754318\n",
      "Step: 359, Loss: 0.08605757355690002\n",
      "Data loss: 0.06989627331495285, Function loss: 0.015189147554337978\n",
      "Step: 360, Loss: 0.0850854218006134\n",
      "Data loss: 0.06777939945459366, Function loss: 0.01621602661907673\n",
      "Step: 361, Loss: 0.08399542421102524\n",
      "Data loss: 0.06709554046392441, Function loss: 0.015904856845736504\n",
      "Step: 362, Loss: 0.08300039917230606\n",
      "Data loss: 0.06738487631082535, Function loss: 0.01470771711319685\n",
      "Step: 363, Loss: 0.08209259063005447\n",
      "Data loss: 0.06489106267690659, Function loss: 0.016252676025032997\n",
      "Step: 364, Loss: 0.08114373683929443\n",
      "Data loss: 0.0656830444931984, Function loss: 0.014414649456739426\n",
      "Step: 365, Loss: 0.08009769022464752\n",
      "Data loss: 0.06398960947990417, Function loss: 0.015057080425322056\n",
      "Step: 366, Loss: 0.07904668897390366\n",
      "Data loss: 0.06309697031974792, Function loss: 0.014970487914979458\n",
      "Step: 367, Loss: 0.07806745916604996\n",
      "Data loss: 0.06323438137769699, Function loss: 0.013910429552197456\n",
      "Step: 368, Loss: 0.0771448090672493\n",
      "Data loss: 0.0609818696975708, Function loss: 0.015220665372908115\n",
      "Step: 369, Loss: 0.07620253413915634\n",
      "Data loss: 0.06171946972608566, Function loss: 0.013544771820306778\n",
      "Step: 370, Loss: 0.07526424527168274\n",
      "Data loss: 0.059543631970882416, Function loss: 0.01481358427554369\n",
      "Step: 371, Loss: 0.07435721904039383\n",
      "Data loss: 0.0597275048494339, Function loss: 0.013897323980927467\n",
      "Step: 372, Loss: 0.07362482696771622\n",
      "Data loss: 0.05846082791686058, Function loss: 0.01484117191284895\n",
      "Step: 373, Loss: 0.0733020007610321\n",
      "Data loss: 0.05809777230024338, Function loss: 0.015790581703186035\n",
      "Step: 374, Loss: 0.07388835400342941\n",
      "Data loss: 0.05764985457062721, Function loss: 0.0185661930590868\n",
      "Step: 375, Loss: 0.07621604949235916\n",
      "Data loss: 0.0581832081079483, Function loss: 0.022490765899419785\n",
      "Step: 376, Loss: 0.08067397773265839\n",
      "Data loss: 0.05588316172361374, Function loss: 0.0282081738114357\n",
      "Step: 377, Loss: 0.08409133553504944\n",
      "Data loss: 0.05999617278575897, Function loss: 0.021988188847899437\n",
      "Step: 378, Loss: 0.08198436349630356\n",
      "Data loss: 0.04889269545674324, Function loss: 0.027474280446767807\n",
      "Step: 379, Loss: 0.07636697590351105\n",
      "Data loss: 0.06057281047105789, Function loss: 0.015491273254156113\n",
      "Step: 380, Loss: 0.0760640799999237\n",
      "Data loss: 0.04852505028247833, Function loss: 0.024072274565696716\n",
      "Step: 381, Loss: 0.07259732484817505\n",
      "Data loss: 0.05305168032646179, Function loss: 0.012874938547611237\n",
      "Step: 382, Loss: 0.06592661887407303\n",
      "Data loss: 0.05525456741452217, Function loss: 0.011283472180366516\n",
      "Step: 383, Loss: 0.06653803586959839\n",
      "Data loss: 0.04541901499032974, Function loss: 0.026087552309036255\n",
      "Step: 384, Loss: 0.071506567299366\n",
      "Data loss: 0.055669136345386505, Function loss: 0.012927019037306309\n",
      "Step: 385, Loss: 0.06859615445137024\n",
      "Data loss: 0.04832005873322487, Function loss: 0.013729189522564411\n",
      "Step: 386, Loss: 0.062049247324466705\n",
      "Data loss: 0.04673590511083603, Function loss: 0.017030883580446243\n",
      "Step: 387, Loss: 0.06376679241657257\n",
      "Data loss: 0.05278148129582405, Function loss: 0.011899152770638466\n",
      "Step: 388, Loss: 0.06468063592910767\n",
      "Data loss: 0.045073606073856354, Function loss: 0.017051391303539276\n",
      "Step: 389, Loss: 0.06212499737739563\n",
      "Data loss: 0.048236116766929626, Function loss: 0.013902843929827213\n",
      "Step: 390, Loss: 0.062138959765434265\n",
      "Data loss: 0.04758239910006523, Function loss: 0.012450752779841423\n",
      "Step: 391, Loss: 0.060033150017261505\n",
      "Data loss: 0.04459767043590546, Function loss: 0.01303738635033369\n",
      "Step: 392, Loss: 0.05763505771756172\n",
      "Data loss: 0.04658323898911476, Function loss: 0.012979259714484215\n",
      "Step: 393, Loss: 0.05956249684095383\n",
      "Data loss: 0.04392564296722412, Function loss: 0.015795378014445305\n",
      "Step: 394, Loss: 0.059721022844314575\n",
      "Data loss: 0.04537606239318848, Function loss: 0.01116868481040001\n",
      "Step: 395, Loss: 0.056544747203588486\n",
      "Data loss: 0.041871633380651474, Function loss: 0.013651186600327492\n",
      "Step: 396, Loss: 0.055522821843624115\n",
      "Data loss: 0.04332566633820534, Function loss: 0.012266037985682487\n",
      "Step: 397, Loss: 0.055591702461242676\n",
      "Data loss: 0.042801111936569214, Function loss: 0.011581458151340485\n",
      "Step: 398, Loss: 0.0543825700879097\n",
      "Data loss: 0.0391375832259655, Function loss: 0.01514228992164135\n",
      "Step: 399, Loss: 0.0542798712849617\n",
      "Data loss: 0.043464645743370056, Function loss: 0.011390953324735165\n",
      "Step: 400, Loss: 0.054855599999427795\n",
      "Data loss: 0.03777511045336723, Function loss: 0.015512781217694283\n",
      "Step: 401, Loss: 0.053287893533706665\n",
      "Data loss: 0.040360771119594574, Function loss: 0.011164985597133636\n",
      "Step: 402, Loss: 0.05152575671672821\n",
      "Data loss: 0.03881221264600754, Function loss: 0.012393150478601456\n",
      "Step: 403, Loss: 0.051205363124608994\n",
      "Data loss: 0.03730219975113869, Function loss: 0.013492530211806297\n",
      "Step: 404, Loss: 0.050794728100299835\n",
      "Data loss: 0.03821282461285591, Function loss: 0.0113880829885602\n",
      "Step: 405, Loss: 0.049600906670093536\n",
      "Data loss: 0.03599283844232559, Function loss: 0.013122599571943283\n",
      "Step: 406, Loss: 0.049115438014268875\n",
      "Data loss: 0.03704831376671791, Function loss: 0.012645535171031952\n",
      "Step: 407, Loss: 0.04969384893774986\n",
      "Data loss: 0.034280817955732346, Function loss: 0.016152067109942436\n",
      "Step: 408, Loss: 0.05043288320302963\n",
      "Data loss: 0.03789811581373215, Function loss: 0.014203494414687157\n",
      "Step: 409, Loss: 0.05210161209106445\n",
      "Data loss: 0.03015601634979248, Function loss: 0.027130888774991035\n",
      "Step: 410, Loss: 0.05728690326213837\n",
      "Data loss: 0.04232721030712128, Function loss: 0.03004540130496025\n",
      "Step: 411, Loss: 0.07237261533737183\n",
      "Data loss: 0.025950951501727104, Function loss: 0.06121951714158058\n",
      "Step: 412, Loss: 0.08717046678066254\n",
      "Data loss: 0.04581553488969803, Function loss: 0.054931215941905975\n",
      "Step: 413, Loss: 0.100746750831604\n",
      "Data loss: 0.028930263593792915, Function loss: 0.03321375697851181\n",
      "Step: 414, Loss: 0.062144018709659576\n",
      "Data loss: 0.030165480449795723, Function loss: 0.017684709280729294\n",
      "Step: 415, Loss: 0.047850191593170166\n",
      "Data loss: 0.039968669414520264, Function loss: 0.028383392840623856\n",
      "Step: 416, Loss: 0.06835205852985382\n",
      "Data loss: 0.028185280039906502, Function loss: 0.03430788591504097\n",
      "Step: 417, Loss: 0.06249316781759262\n",
      "Data loss: 0.03184288367629051, Function loss: 0.015377535484731197\n",
      "Step: 418, Loss: 0.047220420092344284\n",
      "Data loss: 0.03603212535381317, Function loss: 0.016970407217741013\n",
      "Step: 419, Loss: 0.053002532571554184\n",
      "Data loss: 0.028206711634993553, Function loss: 0.026796147227287292\n",
      "Step: 420, Loss: 0.055002860724925995\n",
      "Data loss: 0.031476520001888275, Function loss: 0.015239845030009747\n",
      "Step: 421, Loss: 0.046716365963220596\n",
      "Data loss: 0.03392193093895912, Function loss: 0.013967988081276417\n",
      "Step: 422, Loss: 0.047889918088912964\n",
      "Data loss: 0.028188802301883698, Function loss: 0.021888423711061478\n",
      "Step: 423, Loss: 0.050077226012945175\n",
      "Data loss: 0.03055654466152191, Function loss: 0.014840174466371536\n",
      "Step: 424, Loss: 0.04539671912789345\n",
      "Data loss: 0.03222295269370079, Function loss: 0.01281299814581871\n",
      "Step: 425, Loss: 0.0450359508395195\n",
      "Data loss: 0.0279693566262722, Function loss: 0.018754204735159874\n",
      "Step: 426, Loss: 0.046723559498786926\n",
      "Data loss: 0.02943514473736286, Function loss: 0.014530249871313572\n",
      "Step: 427, Loss: 0.04396539553999901\n",
      "Data loss: 0.030762506648898125, Function loss: 0.012255966663360596\n",
      "Step: 428, Loss: 0.04301847517490387\n",
      "Data loss: 0.027421938255429268, Function loss: 0.01689784787595272\n",
      "Step: 429, Loss: 0.04431978613138199\n",
      "Data loss: 0.028402389958500862, Function loss: 0.014054353348910809\n",
      "Step: 430, Loss: 0.042456742376089096\n",
      "Data loss: 0.029235906898975372, Function loss: 0.011987054720520973\n",
      "Step: 431, Loss: 0.041222959756851196\n",
      "Data loss: 0.026619326323270798, Function loss: 0.015692846849560738\n",
      "Step: 432, Loss: 0.042312175035476685\n",
      "Data loss: 0.027430690824985504, Function loss: 0.01376896258443594\n",
      "Step: 433, Loss: 0.04119965434074402\n",
      "Data loss: 0.027857061475515366, Function loss: 0.011910911649465561\n",
      "Step: 434, Loss: 0.039767973124980927\n",
      "Data loss: 0.025583995506167412, Function loss: 0.014982488937675953\n",
      "Step: 435, Loss: 0.04056648537516594\n",
      "Data loss: 0.02670707181096077, Function loss: 0.013403674587607384\n",
      "Step: 436, Loss: 0.040110744535923004\n",
      "Data loss: 0.02626718208193779, Function loss: 0.012133234180510044\n",
      "Step: 437, Loss: 0.03840041533112526\n",
      "Data loss: 0.024664591997861862, Function loss: 0.014099571853876114\n",
      "Step: 438, Loss: 0.038764163851737976\n",
      "Data loss: 0.025989675894379616, Function loss: 0.01318954024463892\n",
      "Step: 439, Loss: 0.03917921707034111\n",
      "Data loss: 0.02469412423670292, Function loss: 0.012960202060639858\n",
      "Step: 440, Loss: 0.0376543253660202\n",
      "Data loss: 0.024044951424002647, Function loss: 0.012856251560151577\n",
      "Step: 441, Loss: 0.03690120205283165\n",
      "Data loss: 0.024886593222618103, Function loss: 0.012710883282124996\n",
      "Step: 442, Loss: 0.037597477436065674\n",
      "Data loss: 0.023290952667593956, Function loss: 0.014055896550416946\n",
      "Step: 443, Loss: 0.03734684735536575\n",
      "Data loss: 0.0238222386687994, Function loss: 0.012242432683706284\n",
      "Step: 444, Loss: 0.036064669489860535\n",
      "Data loss: 0.023191364482045174, Function loss: 0.012352845631539822\n",
      "Step: 445, Loss: 0.03554420918226242\n",
      "Data loss: 0.02253652922809124, Function loss: 0.0133358184248209\n",
      "Step: 446, Loss: 0.03587234765291214\n",
      "Data loss: 0.023237980902194977, Function loss: 0.012547656893730164\n",
      "Step: 447, Loss: 0.03578563779592514\n",
      "Data loss: 0.021640827879309654, Function loss: 0.013399509713053703\n",
      "Step: 448, Loss: 0.03504033759236336\n",
      "Data loss: 0.022460905835032463, Function loss: 0.011960789561271667\n",
      "Step: 449, Loss: 0.03442169725894928\n",
      "Data loss: 0.021502379328012466, Function loss: 0.012694763019680977\n",
      "Step: 450, Loss: 0.03419714421033859\n",
      "Data loss: 0.021334359422326088, Function loss: 0.012738578952848911\n",
      "Step: 451, Loss: 0.034072939306497574\n",
      "Data loss: 0.021646849811077118, Function loss: 0.012176034972071648\n",
      "Step: 452, Loss: 0.033822886645793915\n",
      "Data loss: 0.02016598731279373, Function loss: 0.013389353640377522\n",
      "Step: 453, Loss: 0.03355534002184868\n",
      "Data loss: 0.021558117121458054, Function loss: 0.01188315823674202\n",
      "Step: 454, Loss: 0.03344127535820007\n",
      "Data loss: 0.019395802170038223, Function loss: 0.013933679088950157\n",
      "Step: 455, Loss: 0.03332947939634323\n",
      "Data loss: 0.02117014490067959, Function loss: 0.011936045251786709\n",
      "Step: 456, Loss: 0.03310618922114372\n",
      "Data loss: 0.018958384171128273, Function loss: 0.013733909465372562\n",
      "Step: 457, Loss: 0.03269229456782341\n",
      "Data loss: 0.02053631655871868, Function loss: 0.011755003593862057\n",
      "Step: 458, Loss: 0.03229131922125816\n",
      "Data loss: 0.01857866160571575, Function loss: 0.013411329127848148\n",
      "Step: 459, Loss: 0.031989991664886475\n",
      "Data loss: 0.02005653828382492, Function loss: 0.011861185543239117\n",
      "Step: 460, Loss: 0.03191772475838661\n",
      "Data loss: 0.01804237626492977, Function loss: 0.01403691153973341\n",
      "Step: 461, Loss: 0.03207928687334061\n",
      "Data loss: 0.020057527348399162, Function loss: 0.01260189339518547\n",
      "Step: 462, Loss: 0.032659418880939484\n",
      "Data loss: 0.01704014465212822, Function loss: 0.016705242916941643\n",
      "Step: 463, Loss: 0.03374538570642471\n",
      "Data loss: 0.020885907113552094, Function loss: 0.015402320772409439\n",
      "Step: 464, Loss: 0.03628822788596153\n",
      "Data loss: 0.015306811779737473, Function loss: 0.025442304089665413\n",
      "Step: 465, Loss: 0.040749117732048035\n",
      "Data loss: 0.0232527032494545, Function loss: 0.0277666337788105\n",
      "Step: 466, Loss: 0.051019337028265\n",
      "Data loss: 0.0132117485627532, Function loss: 0.047964561730623245\n",
      "Step: 467, Loss: 0.06117631122469902\n",
      "Data loss: 0.02596883662045002, Function loss: 0.04942493140697479\n",
      "Step: 468, Loss: 0.07539376616477966\n",
      "Data loss: 0.0132909519597888, Function loss: 0.042714908719062805\n",
      "Step: 469, Loss: 0.05600586161017418\n",
      "Data loss: 0.01989791728556156, Function loss: 0.014374930411577225\n",
      "Step: 470, Loss: 0.034272849559783936\n",
      "Data loss: 0.018853165209293365, Function loss: 0.011860809288918972\n",
      "Step: 471, Loss: 0.030713975429534912\n",
      "Data loss: 0.013958444818854332, Function loss: 0.030833905562758446\n",
      "Step: 472, Loss: 0.04479235038161278\n",
      "Data loss: 0.022208329290151596, Function loss: 0.02588783949613571\n",
      "Step: 473, Loss: 0.04809616878628731\n",
      "Data loss: 0.01585051789879799, Function loss: 0.015204613097012043\n",
      "Step: 474, Loss: 0.031055130064487457\n",
      "Data loss: 0.015339463017880917, Function loss: 0.017055097967386246\n",
      "Step: 475, Loss: 0.03239456191658974\n",
      "Data loss: 0.021245384588837624, Function loss: 0.02162914350628853\n",
      "Step: 476, Loss: 0.0428745299577713\n",
      "Data loss: 0.015062673948705196, Function loss: 0.0184919573366642\n",
      "Step: 477, Loss: 0.03355463221669197\n",
      "Data loss: 0.016335798427462578, Function loss: 0.012254713103175163\n",
      "Step: 478, Loss: 0.02859051153063774\n",
      "Data loss: 0.01983482763171196, Function loss: 0.01603509858250618\n",
      "Step: 479, Loss: 0.03586992621421814\n",
      "Data loss: 0.014838742092251778, Function loss: 0.018151620402932167\n",
      "Step: 480, Loss: 0.032990362495183945\n",
      "Data loss: 0.01683422550559044, Function loss: 0.010732191614806652\n",
      "Step: 481, Loss: 0.027566418051719666\n",
      "Data loss: 0.01855003461241722, Function loss: 0.012539245188236237\n",
      "Step: 482, Loss: 0.031089279800653458\n",
      "Data loss: 0.01471730787307024, Function loss: 0.016953781247138977\n",
      "Step: 483, Loss: 0.03167108818888664\n",
      "Data loss: 0.01705545373260975, Function loss: 0.010400859639048576\n",
      "Step: 484, Loss: 0.027456313371658325\n",
      "Data loss: 0.017352759838104248, Function loss: 0.010739130899310112\n",
      "Step: 485, Loss: 0.02809189073741436\n",
      "Data loss: 0.01454279012978077, Function loss: 0.015550760552287102\n",
      "Step: 486, Loss: 0.03009355068206787\n",
      "Data loss: 0.017068056389689445, Function loss: 0.010626448318362236\n",
      "Step: 487, Loss: 0.02769450470805168\n",
      "Data loss: 0.016284821555018425, Function loss: 0.010157011449337006\n",
      "Step: 488, Loss: 0.02644183300435543\n",
      "Data loss: 0.014368405565619469, Function loss: 0.014111436903476715\n",
      "Step: 489, Loss: 0.028479842469096184\n",
      "Data loss: 0.01699676737189293, Function loss: 0.011099064722657204\n",
      "Step: 490, Loss: 0.028095832094550133\n",
      "Data loss: 0.01508381124585867, Function loss: 0.010680361650884151\n",
      "Step: 491, Loss: 0.02576417289674282\n",
      "Data loss: 0.0144684799015522, Function loss: 0.01183532364666462\n",
      "Step: 492, Loss: 0.02630380354821682\n",
      "Data loss: 0.016511093825101852, Function loss: 0.01091719139367342\n",
      "Step: 493, Loss: 0.027428284287452698\n",
      "Data loss: 0.01411798782646656, Function loss: 0.011822309345006943\n",
      "Step: 494, Loss: 0.025940297171473503\n",
      "Data loss: 0.014714076183736324, Function loss: 0.010110490955412388\n",
      "Step: 495, Loss: 0.024824567139148712\n",
      "Data loss: 0.01552784163504839, Function loss: 0.010063990950584412\n",
      "Step: 496, Loss: 0.025591831654310226\n",
      "Data loss: 0.013523783534765244, Function loss: 0.012247330509126186\n",
      "Step: 497, Loss: 0.025771114975214005\n",
      "Data loss: 0.014955651946365833, Function loss: 0.009745598770678043\n",
      "Step: 498, Loss: 0.024701250717043877\n",
      "Data loss: 0.0142982117831707, Function loss: 0.009744934737682343\n",
      "Step: 499, Loss: 0.024043146520853043\n",
      "Data loss: 0.013367628678679466, Function loss: 0.011095131747424603\n",
      "Step: 500, Loss: 0.024462759494781494\n",
      "Data loss: 0.014838195405900478, Function loss: 0.009908955544233322\n",
      "Step: 501, Loss: 0.024747151881456375\n",
      "Data loss: 0.01310439221560955, Function loss: 0.010965608060359955\n",
      "Step: 502, Loss: 0.024070000275969505\n",
      "Data loss: 0.01378941722214222, Function loss: 0.009535661898553371\n",
      "Step: 503, Loss: 0.023325078189373016\n",
      "Data loss: 0.013847711496055126, Function loss: 0.009429262951016426\n",
      "Step: 504, Loss: 0.023276973515748978\n",
      "Data loss: 0.012628702446818352, Function loss: 0.01100634504109621\n",
      "Step: 505, Loss: 0.023635048419237137\n",
      "Data loss: 0.01404768880456686, Function loss: 0.009622146375477314\n",
      "Step: 506, Loss: 0.023669835180044174\n",
      "Data loss: 0.012452183291316032, Function loss: 0.010656757280230522\n",
      "Step: 507, Loss: 0.023108940571546555\n",
      "Data loss: 0.013217052444815636, Function loss: 0.00928138755261898\n",
      "Step: 508, Loss: 0.022498439997434616\n",
      "Data loss: 0.012902817688882351, Function loss: 0.009326828643679619\n",
      "Step: 509, Loss: 0.022229645401239395\n",
      "Data loss: 0.012250302359461784, Function loss: 0.010051875375211239\n",
      "Step: 510, Loss: 0.02230217680335045\n",
      "Data loss: 0.01316989865154028, Function loss: 0.009272062219679356\n",
      "Step: 511, Loss: 0.022441960871219635\n",
      "Data loss: 0.011763384565711021, Function loss: 0.010608525015413761\n",
      "Step: 512, Loss: 0.022371910512447357\n",
      "Data loss: 0.012896890752017498, Function loss: 0.009204584173858166\n",
      "Step: 513, Loss: 0.022101474925875664\n",
      "Data loss: 0.011725527234375477, Function loss: 0.009988989681005478\n",
      "Step: 514, Loss: 0.02171451598405838\n",
      "Data loss: 0.012284699827432632, Function loss: 0.009087294340133667\n",
      "Step: 515, Loss: 0.0213719941675663\n",
      "Data loss: 0.011873526498675346, Function loss: 0.009291795082390308\n",
      "Step: 516, Loss: 0.02116532251238823\n",
      "Data loss: 0.011631003580987453, Function loss: 0.009531933814287186\n",
      "Step: 517, Loss: 0.021162938326597214\n",
      "Data loss: 0.012074988335371017, Function loss: 0.009400886483490467\n",
      "Step: 518, Loss: 0.02147587388753891\n",
      "Data loss: 0.011170714162290096, Function loss: 0.011217941530048847\n",
      "Step: 519, Loss: 0.022388655692338943\n",
      "Data loss: 0.012476402334868908, Function loss: 0.01215395051985979\n",
      "Step: 520, Loss: 0.0246303528547287\n",
      "Data loss: 0.011398469097912312, Function loss: 0.018470454961061478\n",
      "Step: 521, Loss: 0.029868923127651215\n",
      "Data loss: 0.014001188799738884, Function loss: 0.025618718937039375\n",
      "Step: 522, Loss: 0.03961990773677826\n",
      "Data loss: 0.013410387560725212, Function loss: 0.03648523613810539\n",
      "Step: 523, Loss: 0.049895621836185455\n",
      "Data loss: 0.013460585847496986, Function loss: 0.03241109102964401\n",
      "Step: 524, Loss: 0.04587167501449585\n",
      "Data loss: 0.013285127468407154, Function loss: 0.01865091361105442\n",
      "Step: 525, Loss: 0.03193604201078415\n",
      "Data loss: 0.008338228799402714, Function loss: 0.02595471777021885\n",
      "Step: 526, Loss: 0.03429294750094414\n",
      "Data loss: 0.015799932181835175, Function loss: 0.03280273824930191\n",
      "Step: 527, Loss: 0.048602670431137085\n",
      "Data loss: 0.008668048307299614, Function loss: 0.027765950188040733\n",
      "Step: 528, Loss: 0.03643399849534035\n",
      "Data loss: 0.011741847731173038, Function loss: 0.0100060710683465\n",
      "Step: 529, Loss: 0.02174791879951954\n",
      "Data loss: 0.01247077900916338, Function loss: 0.016190972179174423\n",
      "Step: 530, Loss: 0.02866175025701523\n",
      "Data loss: 0.009520042687654495, Function loss: 0.02051701955497265\n",
      "Step: 531, Loss: 0.030037062242627144\n",
      "Data loss: 0.01213875599205494, Function loss: 0.011444948613643646\n",
      "Step: 532, Loss: 0.023583704605698586\n",
      "Data loss: 0.011060907505452633, Function loss: 0.01854635402560234\n",
      "Step: 533, Loss: 0.029607262462377548\n",
      "Data loss: 0.011392854154109955, Function loss: 0.01616162434220314\n",
      "Step: 534, Loss: 0.027554478496313095\n",
      "Data loss: 0.010452361777424812, Function loss: 0.008572141639888287\n",
      "Step: 535, Loss: 0.019024502485990524\n",
      "Data loss: 0.01110605988651514, Function loss: 0.01405339129269123\n",
      "Step: 536, Loss: 0.025159452110528946\n",
      "Data loss: 0.011419713497161865, Function loss: 0.014135444536805153\n",
      "Step: 537, Loss: 0.025555158033967018\n",
      "Data loss: 0.009420442394912243, Function loss: 0.011469592340290546\n",
      "Step: 538, Loss: 0.02089003473520279\n",
      "Data loss: 0.011804365552961826, Function loss: 0.012108271941542625\n",
      "Step: 539, Loss: 0.023912638425827026\n",
      "Data loss: 0.010176734067499638, Function loss: 0.010759162716567516\n",
      "Step: 540, Loss: 0.020935896784067154\n",
      "Data loss: 0.009381383657455444, Function loss: 0.010655023157596588\n",
      "Step: 541, Loss: 0.020036406815052032\n",
      "Data loss: 0.011880297213792801, Function loss: 0.012300711125135422\n",
      "Step: 542, Loss: 0.024181008338928223\n",
      "Data loss: 0.009168433956801891, Function loss: 0.011317283846437931\n",
      "Step: 543, Loss: 0.020485717803239822\n",
      "Data loss: 0.010108442045748234, Function loss: 0.009110592305660248\n",
      "Step: 544, Loss: 0.019219033420085907\n",
      "Data loss: 0.010886759497225285, Function loss: 0.010086662136018276\n",
      "Step: 545, Loss: 0.02097342163324356\n",
      "Data loss: 0.00917129684239626, Function loss: 0.009896925650537014\n",
      "Step: 546, Loss: 0.019068222492933273\n",
      "Data loss: 0.010333101265132427, Function loss: 0.009692596271634102\n",
      "Step: 547, Loss: 0.020025696605443954\n",
      "Data loss: 0.009860940277576447, Function loss: 0.01050498615950346\n",
      "Step: 548, Loss: 0.02036592736840248\n",
      "Data loss: 0.009784329682588577, Function loss: 0.008022584021091461\n",
      "Step: 549, Loss: 0.01780691370368004\n",
      "Data loss: 0.009511485695838928, Function loss: 0.009017939679324627\n",
      "Step: 550, Loss: 0.01852942630648613\n",
      "Data loss: 0.009577722288668156, Function loss: 0.00954252015799284\n",
      "Step: 551, Loss: 0.019120242446660995\n",
      "Data loss: 0.009961663745343685, Function loss: 0.008143940009176731\n",
      "Step: 552, Loss: 0.018105603754520416\n",
      "Data loss: 0.008563927374780178, Function loss: 0.010272642597556114\n",
      "Step: 553, Loss: 0.018836569041013718\n",
      "Data loss: 0.010040710680186749, Function loss: 0.00842982716858387\n",
      "Step: 554, Loss: 0.018470536917448044\n",
      "Data loss: 0.00893402099609375, Function loss: 0.008123680017888546\n",
      "Step: 555, Loss: 0.01705770194530487\n",
      "Data loss: 0.008912935853004456, Function loss: 0.008575265295803547\n",
      "Step: 556, Loss: 0.017488200217485428\n",
      "Data loss: 0.009497422724962234, Function loss: 0.007971965707838535\n",
      "Step: 557, Loss: 0.017469387501478195\n",
      "Data loss: 0.00860000029206276, Function loss: 0.008296865038573742\n",
      "Step: 558, Loss: 0.016896866261959076\n",
      "Data loss: 0.009129033423960209, Function loss: 0.008313827216625214\n",
      "Step: 559, Loss: 0.017442859709262848\n",
      "Data loss: 0.008688222616910934, Function loss: 0.00869714468717575\n",
      "Step: 560, Loss: 0.017385367304086685\n",
      "Data loss: 0.009049834683537483, Function loss: 0.007593899499624968\n",
      "Step: 561, Loss: 0.016643734648823738\n",
      "Data loss: 0.008213725872337818, Function loss: 0.008492517285048962\n",
      "Step: 562, Loss: 0.01670624315738678\n",
      "Data loss: 0.008978690952062607, Function loss: 0.00764498021453619\n",
      "Step: 563, Loss: 0.01662367209792137\n",
      "Data loss: 0.008332012221217155, Function loss: 0.007707326207309961\n",
      "Step: 564, Loss: 0.01603933796286583\n",
      "Data loss: 0.008320484310388565, Function loss: 0.007699189707636833\n",
      "Step: 565, Loss: 0.016019674018025398\n",
      "Data loss: 0.008637874387204647, Function loss: 0.007528327405452728\n",
      "Step: 566, Loss: 0.01616620272397995\n",
      "Data loss: 0.007992823608219624, Function loss: 0.007887919433414936\n",
      "Step: 567, Loss: 0.01588074304163456\n",
      "Data loss: 0.008464154787361622, Function loss: 0.0074129109270870686\n",
      "Step: 568, Loss: 0.015877066180109978\n",
      "Data loss: 0.00787167064845562, Function loss: 0.008431592956185341\n",
      "Step: 569, Loss: 0.01630326360464096\n",
      "Data loss: 0.00863831676542759, Function loss: 0.00807314645498991\n",
      "Step: 570, Loss: 0.016711462289094925\n",
      "Data loss: 0.007175917737185955, Function loss: 0.0104044359177351\n",
      "Step: 571, Loss: 0.01758035272359848\n",
      "Data loss: 0.009412824176251888, Function loss: 0.010699694976210594\n",
      "Step: 572, Loss: 0.020112518221139908\n",
      "Data loss: 0.00622879434376955, Function loss: 0.018221614882349968\n",
      "Step: 573, Loss: 0.024450410157442093\n",
      "Data loss: 0.010885019786655903, Function loss: 0.022674091160297394\n",
      "Step: 574, Loss: 0.03355911001563072\n",
      "Data loss: 0.005650198087096214, Function loss: 0.038748208433389664\n",
      "Step: 575, Loss: 0.04439840465784073\n",
      "Data loss: 0.013149438425898552, Function loss: 0.04867621511220932\n",
      "Step: 576, Loss: 0.06182565540075302\n",
      "Data loss: 0.005596642382442951, Function loss: 0.050263695418834686\n",
      "Step: 577, Loss: 0.05586033686995506\n",
      "Data loss: 0.011245619505643845, Function loss: 0.028259096667170525\n",
      "Step: 578, Loss: 0.03950471431016922\n",
      "Data loss: 0.006802345626056194, Function loss: 0.010256750509142876\n",
      "Step: 579, Loss: 0.017059095203876495\n",
      "Data loss: 0.0066262236796319485, Function loss: 0.013511556200683117\n",
      "Step: 580, Loss: 0.020137779414653778\n",
      "Data loss: 0.01094470452517271, Function loss: 0.02557201497256756\n",
      "Step: 581, Loss: 0.036516718566417694\n",
      "Data loss: 0.005907041020691395, Function loss: 0.02500953897833824\n",
      "Step: 582, Loss: 0.030916579067707062\n",
      "Data loss: 0.008520389907062054, Function loss: 0.00833939854055643\n",
      "Step: 583, Loss: 0.016859788447618484\n",
      "Data loss: 0.008698890917003155, Function loss: 0.009142196737229824\n",
      "Step: 584, Loss: 0.01784108765423298\n",
      "Data loss: 0.006144829094409943, Function loss: 0.020549433305859566\n",
      "Step: 585, Loss: 0.02669426240026951\n",
      "Data loss: 0.009501415304839611, Function loss: 0.01354786567389965\n",
      "Step: 586, Loss: 0.023049280047416687\n",
      "Data loss: 0.007526465225964785, Function loss: 0.007237697020173073\n",
      "Step: 587, Loss: 0.01476416178047657\n",
      "Data loss: 0.006554576102644205, Function loss: 0.013301645405590534\n",
      "Step: 588, Loss: 0.019856221973896027\n",
      "Data loss: 0.009525856003165245, Function loss: 0.014143605716526508\n",
      "Step: 589, Loss: 0.023669462651014328\n",
      "Data loss: 0.007003490347415209, Function loss: 0.009436987340450287\n",
      "Step: 590, Loss: 0.01644047722220421\n",
      "Data loss: 0.0069952658377587795, Function loss: 0.008781745098531246\n",
      "Step: 591, Loss: 0.01577701047062874\n",
      "Data loss: 0.009076283313333988, Function loss: 0.011644156649708748\n",
      "Step: 592, Loss: 0.02072044089436531\n",
      "Data loss: 0.006744191981852055, Function loss: 0.010948006063699722\n",
      "Step: 593, Loss: 0.017692197114229202\n",
      "Data loss: 0.007365772034972906, Function loss: 0.00700912531465292\n",
      "Step: 594, Loss: 0.014374896883964539\n",
      "Data loss: 0.008453810587525368, Function loss: 0.008942295797169209\n",
      "Step: 595, Loss: 0.01739610731601715\n",
      "Data loss: 0.006597539875656366, Function loss: 0.011195054277777672\n",
      "Step: 596, Loss: 0.01779259368777275\n",
      "Data loss: 0.007656814530491829, Function loss: 0.006888826377689838\n",
      "Step: 597, Loss: 0.014545640908181667\n",
      "Data loss: 0.007795628160238266, Function loss: 0.00712184002622962\n",
      "Step: 598, Loss: 0.014917468652129173\n",
      "Data loss: 0.006526000332087278, Function loss: 0.010358882136642933\n",
      "Step: 599, Loss: 0.016884882003068924\n",
      "Data loss: 0.007813187316060066, Function loss: 0.007556595373898745\n",
      "Step: 600, Loss: 0.015369782224297523\n",
      "Data loss: 0.007171010132879019, Function loss: 0.006600580178201199\n",
      "Step: 601, Loss: 0.01377158984541893\n",
      "Data loss: 0.006540724076330662, Function loss: 0.008673540316522121\n",
      "Step: 602, Loss: 0.015214264392852783\n",
      "Data loss: 0.007784962188452482, Function loss: 0.007997492328286171\n",
      "Step: 603, Loss: 0.01578245498239994\n",
      "Data loss: 0.006606756243854761, Function loss: 0.0074182599782943726\n",
      "Step: 604, Loss: 0.014025015756487846\n",
      "Data loss: 0.00670000072568655, Function loss: 0.006917734630405903\n",
      "Step: 605, Loss: 0.013617735356092453\n",
      "Data loss: 0.007426449563354254, Function loss: 0.007319187745451927\n",
      "Step: 606, Loss: 0.014745637774467468\n",
      "Data loss: 0.0062746466137468815, Function loss: 0.00820170622318983\n",
      "Step: 607, Loss: 0.014476353302598\n",
      "Data loss: 0.006913030054420233, Function loss: 0.006431665271520615\n",
      "Step: 608, Loss: 0.013344695791602135\n",
      "Data loss: 0.006873070728033781, Function loss: 0.006529162172228098\n",
      "Step: 609, Loss: 0.013402232900261879\n",
      "Data loss: 0.006147705018520355, Function loss: 0.007922853343188763\n",
      "Step: 610, Loss: 0.014070558361709118\n",
      "Data loss: 0.007024374790489674, Function loss: 0.006815627217292786\n",
      "Step: 611, Loss: 0.01384000200778246\n",
      "Data loss: 0.006275109946727753, Function loss: 0.006777800619602203\n",
      "Step: 612, Loss: 0.013052910566329956\n",
      "Data loss: 0.006289052776992321, Function loss: 0.006657977122813463\n",
      "Step: 613, Loss: 0.012947030365467072\n",
      "Data loss: 0.006773840636014938, Function loss: 0.00662132166326046\n",
      "Step: 614, Loss: 0.013395162299275398\n",
      "Data loss: 0.005905859638005495, Function loss: 0.007482658606022596\n",
      "Step: 615, Loss: 0.013388518244028091\n",
      "Data loss: 0.006526039447635412, Function loss: 0.006351915653795004\n",
      "Step: 616, Loss: 0.012877955101430416\n",
      "Data loss: 0.006211054977029562, Function loss: 0.006350582931190729\n",
      "Step: 617, Loss: 0.012561637908220291\n",
      "Data loss: 0.005911488085985184, Function loss: 0.006832091603428125\n",
      "Step: 618, Loss: 0.012743579223752022\n",
      "Data loss: 0.006459751632064581, Function loss: 0.006445868872106075\n",
      "Step: 619, Loss: 0.012905620038509369\n",
      "Data loss: 0.005771331023424864, Function loss: 0.006898630876094103\n",
      "Step: 620, Loss: 0.012669961899518967\n",
      "Data loss: 0.006090171169489622, Function loss: 0.006252799648791552\n",
      "Step: 621, Loss: 0.012342970818281174\n",
      "Data loss: 0.006047451868653297, Function loss: 0.006240246817469597\n",
      "Step: 622, Loss: 0.012287698686122894\n",
      "Data loss: 0.0056402008049190044, Function loss: 0.006789229344576597\n",
      "Step: 623, Loss: 0.012429430149495602\n",
      "Data loss: 0.006139143370091915, Function loss: 0.006331338547170162\n",
      "Step: 624, Loss: 0.012470481917262077\n",
      "Data loss: 0.0055515156127512455, Function loss: 0.006744123063981533\n",
      "Step: 625, Loss: 0.012295639142394066\n",
      "Data loss: 0.005846249405294657, Function loss: 0.006221890915185213\n",
      "Step: 626, Loss: 0.01206814032047987\n",
      "Data loss: 0.005693085957318544, Function loss: 0.006255517713725567\n",
      "Step: 627, Loss: 0.011948604136705399\n",
      "Data loss: 0.0054859877564013, Function loss: 0.006475761532783508\n",
      "Step: 628, Loss: 0.011961748823523521\n",
      "Data loss: 0.005775321274995804, Function loss: 0.0062291985377669334\n",
      "Step: 629, Loss: 0.012004519812762737\n",
      "Data loss: 0.005308650899678469, Function loss: 0.006668205372989178\n",
      "Step: 630, Loss: 0.011976856738328934\n",
      "Data loss: 0.005633834283798933, Function loss: 0.0062292045913636684\n",
      "Step: 631, Loss: 0.011863038875162601\n",
      "Data loss: 0.005303327925503254, Function loss: 0.006415116135030985\n",
      "Step: 632, Loss: 0.011718444526195526\n",
      "Data loss: 0.005376613233238459, Function loss: 0.006237701512873173\n",
      "Step: 633, Loss: 0.011614315211772919\n",
      "Data loss: 0.005347503814846277, Function loss: 0.006228756625205278\n",
      "Step: 634, Loss: 0.011576260440051556\n",
      "Data loss: 0.005157645791769028, Function loss: 0.0064002047292888165\n",
      "Step: 635, Loss: 0.011557850986719131\n",
      "Data loss: 0.005306163802742958, Function loss: 0.006211553700268269\n",
      "Step: 636, Loss: 0.011517717503011227\n",
      "Data loss: 0.005036848597228527, Function loss: 0.006426539737731218\n",
      "Step: 637, Loss: 0.011463388800621033\n",
      "Data loss: 0.005178133957087994, Function loss: 0.00620720349252224\n",
      "Step: 638, Loss: 0.011385337449610233\n",
      "Data loss: 0.004999018739908934, Function loss: 0.006314316298812628\n",
      "Step: 639, Loss: 0.011313335038721561\n",
      "Data loss: 0.0049889846704900265, Function loss: 0.006266632582992315\n",
      "Step: 640, Loss: 0.011255617253482342\n",
      "Data loss: 0.004984727129340172, Function loss: 0.006230303086340427\n",
      "Step: 641, Loss: 0.0112150302156806\n",
      "Data loss: 0.004822057206183672, Function loss: 0.006359209306538105\n",
      "Step: 642, Loss: 0.011181266978383064\n",
      "Data loss: 0.004941083025187254, Function loss: 0.006209677550941706\n",
      "Step: 643, Loss: 0.01115076057612896\n",
      "Data loss: 0.004697496071457863, Function loss: 0.006414888426661491\n",
      "Step: 644, Loss: 0.011112384498119354\n",
      "Data loss: 0.004862064030021429, Function loss: 0.0062073334120213985\n",
      "Step: 645, Loss: 0.011069397442042828\n",
      "Data loss: 0.0046094441786408424, Function loss: 0.006408258341252804\n",
      "Step: 646, Loss: 0.011017702519893646\n",
      "Data loss: 0.004753109999001026, Function loss: 0.0062091294676065445\n",
      "Step: 647, Loss: 0.01096223946660757\n",
      "Data loss: 0.004554787650704384, Function loss: 0.006338820792734623\n",
      "Step: 648, Loss: 0.010893608443439007\n",
      "Data loss: 0.004622078035026789, Function loss: 0.006216543260961771\n",
      "Step: 649, Loss: 0.01083862129598856\n",
      "Data loss: 0.0045172651298344135, Function loss: 0.006269673816859722\n",
      "Step: 650, Loss: 0.010786939412355423\n",
      "Data loss: 0.004491041414439678, Function loss: 0.006249529775232077\n",
      "Step: 651, Loss: 0.010740570724010468\n",
      "Data loss: 0.004471507389098406, Function loss: 0.00622881343588233\n",
      "Step: 652, Loss: 0.010700320824980736\n",
      "Data loss: 0.004379903897643089, Function loss: 0.0062829721719026566\n",
      "Step: 653, Loss: 0.010662876069545746\n",
      "Data loss: 0.004408878739923239, Function loss: 0.0062115928158164024\n",
      "Step: 654, Loss: 0.010620471090078354\n",
      "Data loss: 0.004294306505471468, Function loss: 0.006287859752774239\n",
      "Step: 655, Loss: 0.01058216579258442\n",
      "Data loss: 0.004341785795986652, Function loss: 0.006205716170370579\n",
      "Step: 656, Loss: 0.010547501966357231\n",
      "Data loss: 0.0041852788999676704, Function loss: 0.0063382466323673725\n",
      "Step: 657, Loss: 0.01052352599799633\n",
      "Data loss: 0.00433008698746562, Function loss: 0.006189214065670967\n",
      "Step: 658, Loss: 0.010519301518797874\n",
      "Data loss: 0.004033584147691727, Function loss: 0.006501557771116495\n",
      "Step: 659, Loss: 0.010535141453146935\n",
      "Data loss: 0.004344405140727758, Function loss: 0.006231724284589291\n",
      "Step: 660, Loss: 0.010576128959655762\n",
      "Data loss: 0.003867350984364748, Function loss: 0.0068261935375630856\n",
      "Step: 661, Loss: 0.010693544521927834\n",
      "Data loss: 0.004422783851623535, Function loss: 0.0065291994251310825\n",
      "Step: 662, Loss: 0.01095198281109333\n",
      "Data loss: 0.0036277733743190765, Function loss: 0.007815397344529629\n",
      "Step: 663, Loss: 0.011443170718848705\n",
      "Data loss: 0.004660212900489569, Function loss: 0.007755789440125227\n",
      "Step: 664, Loss: 0.012416002340614796\n",
      "Data loss: 0.003285803599283099, Function loss: 0.010765142738819122\n",
      "Step: 665, Loss: 0.014050946570932865\n",
      "Data loss: 0.005187226925045252, Function loss: 0.012058975175023079\n",
      "Step: 666, Loss: 0.017246201634407043\n",
      "Data loss: 0.0028637999203056097, Function loss: 0.01909671165049076\n",
      "Step: 667, Loss: 0.021960511803627014\n",
      "Data loss: 0.006267322227358818, Function loss: 0.025219442322850227\n",
      "Step: 668, Loss: 0.031486764550209045\n",
      "Data loss: 0.0025169583968818188, Function loss: 0.03800329566001892\n",
      "Step: 669, Loss: 0.04052025452256203\n",
      "Data loss: 0.0075734383426606655, Function loss: 0.04606020078063011\n",
      "Step: 670, Loss: 0.053633637726306915\n",
      "Data loss: 0.0025302988942712545, Function loss: 0.04195564612746239\n",
      "Step: 671, Loss: 0.044485945254564285\n",
      "Data loss: 0.005990187171846628, Function loss: 0.02262837626039982\n",
      "Step: 672, Loss: 0.02861856296658516\n",
      "Data loss: 0.0034880235325545073, Function loss: 0.00805085152387619\n",
      "Step: 673, Loss: 0.011538875289261341\n",
      "Data loss: 0.003255790565162897, Function loss: 0.011000418104231358\n",
      "Step: 674, Loss: 0.014256209135055542\n",
      "Data loss: 0.005913098808377981, Function loss: 0.021085232496261597\n",
      "Step: 675, Loss: 0.026998331770300865\n",
      "Data loss: 0.002981762168928981, Function loss: 0.02159235253930092\n",
      "Step: 676, Loss: 0.02457411400973797\n",
      "Data loss: 0.0047517698258161545, Function loss: 0.00883499439805746\n",
      "Step: 677, Loss: 0.013586764223873615\n",
      "Data loss: 0.004395926836878061, Function loss: 0.006701834499835968\n",
      "Step: 678, Loss: 0.011097760871052742\n",
      "Data loss: 0.003222184255719185, Function loss: 0.015611650422215462\n",
      "Step: 679, Loss: 0.018833834677934647\n",
      "Data loss: 0.005475104320794344, Function loss: 0.014683343470096588\n",
      "Step: 680, Loss: 0.020158447325229645\n",
      "Data loss: 0.00369700463488698, Function loss: 0.008118431083858013\n",
      "Step: 681, Loss: 0.01181543618440628\n",
      "Data loss: 0.003688621101900935, Function loss: 0.008013437502086163\n",
      "Step: 682, Loss: 0.011702058836817741\n",
      "Data loss: 0.00526602054014802, Function loss: 0.01219156850129366\n",
      "Step: 683, Loss: 0.017457589507102966\n",
      "Data loss: 0.0034955632872879505, Function loss: 0.011294675059616566\n",
      "Step: 684, Loss: 0.014790238812565804\n",
      "Data loss: 0.004211411811411381, Function loss: 0.006042799912393093\n",
      "Step: 685, Loss: 0.010254211723804474\n",
      "Data loss: 0.004709524102509022, Function loss: 0.007690629456192255\n",
      "Step: 686, Loss: 0.012400154024362564\n",
      "Data loss: 0.003474632976576686, Function loss: 0.01112494058907032\n",
      "Step: 687, Loss: 0.01459957379847765\n",
      "Data loss: 0.004601366352289915, Function loss: 0.007299524266272783\n",
      "Step: 688, Loss: 0.011900890618562698\n",
      "Data loss: 0.004179581068456173, Function loss: 0.005903719458729029\n",
      "Step: 689, Loss: 0.010083300992846489\n",
      "Data loss: 0.003562057390809059, Function loss: 0.008791946806013584\n",
      "Step: 690, Loss: 0.012354004196822643\n",
      "Data loss: 0.004722102545201778, Function loss: 0.008260570466518402\n",
      "Step: 691, Loss: 0.01298267301172018\n",
      "Data loss: 0.003792221425101161, Function loss: 0.0066622416488826275\n",
      "Step: 692, Loss: 0.010454462841153145\n",
      "Data loss: 0.0037508842069655657, Function loss: 0.006507645361125469\n",
      "Step: 693, Loss: 0.010258529335260391\n",
      "Data loss: 0.004543663933873177, Function loss: 0.007457996718585491\n",
      "Step: 694, Loss: 0.012001660652458668\n",
      "Data loss: 0.0035490149166435003, Function loss: 0.007881177589297295\n",
      "Step: 695, Loss: 0.011430192738771439\n",
      "Data loss: 0.004011781420558691, Function loss: 0.005844421219080687\n",
      "Step: 696, Loss: 0.009856202639639378\n",
      "Data loss: 0.004156371578574181, Function loss: 0.0059998491778969765\n",
      "Step: 697, Loss: 0.010156220756471157\n",
      "Data loss: 0.0034619427751749754, Function loss: 0.007618694566190243\n",
      "Step: 698, Loss: 0.011080637574195862\n",
      "Data loss: 0.0041856542229652405, Function loss: 0.006364315282553434\n",
      "Step: 699, Loss: 0.010549969971179962\n",
      "Data loss: 0.0037444967310875654, Function loss: 0.005843737628310919\n",
      "Step: 700, Loss: 0.00958823412656784\n",
      "Data loss: 0.0035572675988078117, Function loss: 0.006287706550210714\n",
      "Step: 701, Loss: 0.009844973683357239\n",
      "Data loss: 0.004121169447898865, Function loss: 0.0063264877535402775\n",
      "Step: 702, Loss: 0.010447656735777855\n",
      "Data loss: 0.003470012918114662, Function loss: 0.006604137364774942\n",
      "Step: 703, Loss: 0.010074149817228317\n",
      "Data loss: 0.0037427206989377737, Function loss: 0.005688031204044819\n",
      "Step: 704, Loss: 0.009430752135813236\n",
      "Data loss: 0.003863164922222495, Function loss: 0.0057198102585971355\n",
      "Step: 705, Loss: 0.009582974947988987\n",
      "Data loss: 0.0033627361990511417, Function loss: 0.006634687073528767\n",
      "Step: 706, Loss: 0.009997423738241196\n",
      "Data loss: 0.003886373247951269, Function loss: 0.005909009836614132\n",
      "Step: 707, Loss: 0.009795382618904114\n",
      "Data loss: 0.0035179853439331055, Function loss: 0.005799069069325924\n",
      "Step: 708, Loss: 0.00931705441325903\n",
      "Data loss: 0.003469461342319846, Function loss: 0.005808480549603701\n",
      "Step: 709, Loss: 0.009277941659092903\n",
      "Data loss: 0.0037913788110017776, Function loss: 0.0057724760845303535\n",
      "Step: 710, Loss: 0.009563854895532131\n",
      "Data loss: 0.00329745770432055, Function loss: 0.0063153225928545\n",
      "Step: 711, Loss: 0.009612780064344406\n",
      "Data loss: 0.0036721935030072927, Function loss: 0.0056674606166779995\n",
      "Step: 712, Loss: 0.009339653886854649\n",
      "Data loss: 0.0034569483250379562, Function loss: 0.005637413822114468\n",
      "Step: 713, Loss: 0.009094362147152424\n",
      "Data loss: 0.003348712809383869, Function loss: 0.005800885614007711\n",
      "Step: 714, Loss: 0.009149597957730293\n",
      "Data loss: 0.0036314334720373154, Function loss: 0.005687641445547342\n",
      "Step: 715, Loss: 0.00931907445192337\n",
      "Data loss: 0.003233295865356922, Function loss: 0.006065372843295336\n",
      "Step: 716, Loss: 0.009298669174313545\n",
      "Data loss: 0.003506408305838704, Function loss: 0.005594626069068909\n",
      "Step: 717, Loss: 0.00910103414207697\n",
      "Data loss: 0.0033532993402332067, Function loss: 0.005601356737315655\n",
      "Step: 718, Loss: 0.008954656310379505\n",
      "Data loss: 0.003245825180783868, Function loss: 0.005737414117902517\n",
      "Step: 719, Loss: 0.008983239531517029\n",
      "Data loss: 0.003464742796495557, Function loss: 0.005613668356090784\n",
      "Step: 720, Loss: 0.009078411385416985\n",
      "Data loss: 0.0031366192270070314, Function loss: 0.0059415968134999275\n",
      "Step: 721, Loss: 0.009078215807676315\n",
      "Data loss: 0.003383265808224678, Function loss: 0.005596719682216644\n",
      "Step: 722, Loss: 0.008979985490441322\n",
      "Data loss: 0.003180562751367688, Function loss: 0.005672253668308258\n",
      "Step: 723, Loss: 0.008852816186845303\n",
      "Data loss: 0.003186263609677553, Function loss: 0.005610657390207052\n",
      "Step: 724, Loss: 0.008796920999884605\n",
      "Data loss: 0.003269101493060589, Function loss: 0.005557099357247353\n",
      "Step: 725, Loss: 0.008826200850307941\n",
      "Data loss: 0.0030480597633868456, Function loss: 0.005811899900436401\n",
      "Step: 726, Loss: 0.008859959430992603\n",
      "Data loss: 0.003246431704610586, Function loss: 0.00559195363894105\n",
      "Step: 727, Loss: 0.008838385343551636\n",
      "Data loss: 0.0030360198579728603, Function loss: 0.0057243891060352325\n",
      "Step: 728, Loss: 0.00876040942966938\n",
      "Data loss: 0.0031076231971383095, Function loss: 0.005577152129262686\n",
      "Step: 729, Loss: 0.008684774860739708\n",
      "Data loss: 0.003091245424002409, Function loss: 0.0055596064776182175\n",
      "Step: 730, Loss: 0.008650852367281914\n",
      "Data loss: 0.0029633534140884876, Function loss: 0.005698255263268948\n",
      "Step: 731, Loss: 0.008661609143018723\n",
      "Data loss: 0.003104229224845767, Function loss: 0.0055636572651565075\n",
      "Step: 732, Loss: 0.00866788625717163\n",
      "Data loss: 0.00290764681994915, Function loss: 0.005728589836508036\n",
      "Step: 733, Loss: 0.008636236190795898\n",
      "Data loss: 0.0030216830782592297, Function loss: 0.005557629279792309\n",
      "Step: 734, Loss: 0.008579311892390251\n",
      "Data loss: 0.0029242881573736668, Function loss: 0.005608448293060064\n",
      "Step: 735, Loss: 0.008532736450433731\n",
      "Data loss: 0.0029035976622253656, Function loss: 0.005599759519100189\n",
      "Step: 736, Loss: 0.008503356948494911\n",
      "Data loss: 0.0029499453958123922, Function loss: 0.005550396628677845\n",
      "Step: 737, Loss: 0.008500342257320881\n",
      "Data loss: 0.0028063596691936255, Function loss: 0.005700601264834404\n",
      "Step: 738, Loss: 0.008506961166858673\n",
      "Data loss: 0.0029302220791578293, Function loss: 0.005556575022637844\n",
      "Step: 739, Loss: 0.008486797101795673\n",
      "Data loss: 0.00277418433688581, Function loss: 0.005669422447681427\n",
      "Step: 740, Loss: 0.00844360701739788\n",
      "Data loss: 0.0028486207593232393, Function loss: 0.005551883950829506\n",
      "Step: 741, Loss: 0.008400504477322102\n",
      "Data loss: 0.0027917518746107817, Function loss: 0.005574213340878487\n",
      "Step: 742, Loss: 0.008365965448319912\n",
      "Data loss: 0.0027393028140068054, Function loss: 0.005617987364530563\n",
      "Step: 743, Loss: 0.008357290178537369\n",
      "Data loss: 0.0028151660226285458, Function loss: 0.005547163542360067\n",
      "Step: 744, Loss: 0.008362329564988613\n",
      "Data loss: 0.002668363507837057, Function loss: 0.0056842309422791\n",
      "Step: 745, Loss: 0.008352594450116158\n",
      "Data loss: 0.0027771370951086283, Function loss: 0.0055426061153411865\n",
      "Step: 746, Loss: 0.008319742977619171\n",
      "Data loss: 0.0026499733794480562, Function loss: 0.005632033105939627\n",
      "Step: 747, Loss: 0.008282006718218327\n",
      "Data loss: 0.002704530954360962, Function loss: 0.005541639402508736\n",
      "Step: 748, Loss: 0.008246170356869698\n",
      "Data loss: 0.002658715471625328, Function loss: 0.005566286854445934\n",
      "Step: 749, Loss: 0.008225002326071262\n",
      "Data loss: 0.0026254933327436447, Function loss: 0.005584518425166607\n",
      "Step: 750, Loss: 0.008210011757910252\n",
      "Data loss: 0.00266011874191463, Function loss: 0.0055375657975673676\n",
      "Step: 751, Loss: 0.008197684772312641\n",
      "Data loss: 0.0025618234649300575, Function loss: 0.005625223740935326\n",
      "Step: 752, Loss: 0.008187047205865383\n",
      "Data loss: 0.002643725834786892, Function loss: 0.005525215994566679\n",
      "Step: 753, Loss: 0.008168941363692284\n",
      "Data loss: 0.0025222208350896835, Function loss: 0.005625548772513866\n",
      "Step: 754, Loss: 0.00814776960760355\n",
      "Data loss: 0.002607864560559392, Function loss: 0.005511187016963959\n",
      "Step: 755, Loss: 0.008119051344692707\n",
      "Data loss: 0.0024970683734863997, Function loss: 0.005598592106252909\n",
      "Step: 756, Loss: 0.008095660246908665\n",
      "Data loss: 0.0025698021054267883, Function loss: 0.0055082631297409534\n",
      "Step: 757, Loss: 0.008078064769506454\n",
      "Data loss: 0.002471134066581726, Function loss: 0.005585276521742344\n",
      "Step: 758, Loss: 0.00805641058832407\n",
      "Data loss: 0.0025309897027909756, Function loss: 0.0055090500973165035\n",
      "Step: 759, Loss: 0.008040039800107479\n",
      "Data loss: 0.0024478186387568712, Function loss: 0.005570379085838795\n",
      "Step: 760, Loss: 0.008018197491765022\n",
      "Data loss: 0.0024948231875896454, Function loss: 0.005504156928509474\n",
      "Step: 761, Loss: 0.007998980581760406\n",
      "Data loss: 0.0024263139348477125, Function loss: 0.005543993320316076\n",
      "Step: 762, Loss: 0.007970307022333145\n",
      "Data loss: 0.00245116651058197, Function loss: 0.0054965573363006115\n",
      "Step: 763, Loss: 0.007947724312543869\n",
      "Data loss: 0.002416715957224369, Function loss: 0.005507956258952618\n",
      "Step: 764, Loss: 0.007924672216176987\n",
      "Data loss: 0.0024059878196567297, Function loss: 0.005498106591403484\n",
      "Step: 765, Loss: 0.007904094643890858\n",
      "Data loss: 0.0024031810462474823, Function loss: 0.00548368226736784\n",
      "Step: 766, Loss: 0.007886863313615322\n",
      "Data loss: 0.0023639036808162928, Function loss: 0.005507241003215313\n",
      "Step: 767, Loss: 0.007871144451200962\n",
      "Data loss: 0.002393854781985283, Function loss: 0.00546088395640254\n",
      "Step: 768, Loss: 0.00785473920404911\n",
      "Data loss: 0.0023180972784757614, Function loss: 0.0055235521867871284\n",
      "Step: 769, Loss: 0.00784164946526289\n",
      "Data loss: 0.0023914941120892763, Function loss: 0.005443941336125135\n",
      "Step: 770, Loss: 0.007835435681045055\n",
      "Data loss: 0.0022606640122830868, Function loss: 0.0055811433121562\n",
      "Step: 771, Loss: 0.007841806858778\n",
      "Data loss: 0.0024120421148836613, Function loss: 0.005460687913000584\n",
      "Step: 772, Loss: 0.007872730493545532\n",
      "Data loss: 0.0021835218649357557, Function loss: 0.005743124056607485\n",
      "Step: 773, Loss: 0.007926645688712597\n",
      "Data loss: 0.0024610799737274647, Function loss: 0.0055725970305502415\n",
      "Step: 774, Loss: 0.008033677004277706\n",
      "Data loss: 0.0020822626538574696, Function loss: 0.006129593588411808\n",
      "Step: 775, Loss: 0.008211856707930565\n",
      "Data loss: 0.002559284446761012, Function loss: 0.005981204099953175\n",
      "Step: 776, Loss: 0.00854048877954483\n",
      "Data loss: 0.0019363838946446776, Function loss: 0.007167181000113487\n",
      "Step: 777, Loss: 0.0091035645455122\n",
      "Data loss: 0.0027744071558117867, Function loss: 0.00742799136787653\n",
      "Step: 778, Loss: 0.010202398523688316\n",
      "Data loss: 0.00172712083440274, Function loss: 0.010200824588537216\n",
      "Step: 779, Loss: 0.011927945539355278\n",
      "Data loss: 0.003201510524377227, Function loss: 0.011910593137145042\n",
      "Step: 780, Loss: 0.015112103894352913\n",
      "Data loss: 0.0014938507229089737, Function loss: 0.017834484577178955\n",
      "Step: 781, Loss: 0.01932833530008793\n",
      "Data loss: 0.003937141038477421, Function loss: 0.022971604019403458\n",
      "Step: 782, Loss: 0.026908744126558304\n",
      "Data loss: 0.001349526341073215, Function loss: 0.03142322227358818\n",
      "Step: 783, Loss: 0.03277274966239929\n",
      "Data loss: 0.0045820861123502254, Function loss: 0.03574938327074051\n",
      "Step: 784, Loss: 0.04033146798610687\n",
      "Data loss: 0.0014061455149203539, Function loss: 0.03191070631146431\n",
      "Step: 785, Loss: 0.033316850662231445\n",
      "Data loss: 0.0036249381955713034, Function loss: 0.019242413341999054\n",
      "Step: 786, Loss: 0.022867351770401\n",
      "Data loss: 0.00207162625156343, Function loss: 0.008951433934271336\n",
      "Step: 787, Loss: 0.011023060418665409\n",
      "Data loss: 0.002143771853297949, Function loss: 0.008607580326497555\n",
      "Step: 788, Loss: 0.010751351714134216\n",
      "Data loss: 0.003467167727649212, Function loss: 0.014486720785498619\n",
      "Step: 789, Loss: 0.017953887581825256\n",
      "Data loss: 0.0017710222164168954, Function loss: 0.01719951070845127\n",
      "Step: 790, Loss: 0.018970532342791557\n",
      "Data loss: 0.0031166491098701954, Function loss: 0.010310523211956024\n",
      "Step: 791, Loss: 0.013427171856164932\n",
      "Data loss: 0.002445254009217024, Function loss: 0.00684514781460166\n",
      "Step: 792, Loss: 0.009290401823818684\n",
      "Data loss: 0.0022967110853642225, Function loss: 0.010125230066478252\n",
      "Step: 793, Loss: 0.012421941384673119\n",
      "Data loss: 0.003362323623150587, Function loss: 0.01169181615114212\n",
      "Step: 794, Loss: 0.015054140239953995\n",
      "Data loss: 0.002090673428028822, Function loss: 0.009057087823748589\n",
      "Step: 795, Loss: 0.011147761717438698\n",
      "Data loss: 0.0026051108725368977, Function loss: 0.0057371556758880615\n",
      "Step: 796, Loss: 0.008342266082763672\n",
      "Data loss: 0.002997581847012043, Function loss: 0.007830953225493431\n",
      "Step: 797, Loss: 0.010828535072505474\n",
      "Data loss: 0.002274492522701621, Function loss: 0.010079802013933659\n",
      "Step: 798, Loss: 0.012354294769465923\n",
      "Data loss: 0.002898421371355653, Function loss: 0.00665020477026701\n",
      "Step: 799, Loss: 0.009548625908792019\n",
      "Data loss: 0.002517556771636009, Function loss: 0.00535479374229908\n",
      "Step: 800, Loss: 0.007872350513935089\n",
      "Data loss: 0.002362927421927452, Function loss: 0.007691975682973862\n",
      "Step: 801, Loss: 0.010054903104901314\n",
      "Data loss: 0.003059346927329898, Function loss: 0.007856585085391998\n",
      "Step: 802, Loss: 0.01091593224555254\n",
      "Data loss: 0.002325216308236122, Function loss: 0.0062544080428779125\n",
      "Step: 803, Loss: 0.008579624816775322\n",
      "Data loss: 0.002429751679301262, Function loss: 0.005275201518088579\n",
      "Step: 804, Loss: 0.007704953197389841\n",
      "Data loss: 0.002862950088456273, Function loss: 0.006461872719228268\n",
      "Step: 805, Loss: 0.009324822574853897\n",
      "Data loss: 0.0022789265494793653, Function loss: 0.0074266791343688965\n",
      "Step: 806, Loss: 0.009705605916678905\n",
      "Data loss: 0.0026583021972328424, Function loss: 0.005503072869032621\n",
      "Step: 807, Loss: 0.008161375299096107\n",
      "Data loss: 0.002524149604141712, Function loss: 0.005044903140515089\n",
      "Step: 808, Loss: 0.007569052744656801\n",
      "Data loss: 0.002223712159320712, Function loss: 0.006428183522075415\n",
      "Step: 809, Loss: 0.008651895448565483\n",
      "Data loss: 0.0028084556106477976, Function loss: 0.00627308851107955\n",
      "Step: 810, Loss: 0.009081544354557991\n",
      "Data loss: 0.002257260726764798, Function loss: 0.005762061104178429\n",
      "Step: 811, Loss: 0.008019321598112583\n",
      "Data loss: 0.0023644259199500084, Function loss: 0.005026175174862146\n",
      "Step: 812, Loss: 0.007390601094812155\n",
      "Data loss: 0.0026119237300008535, Function loss: 0.005395690444856882\n",
      "Step: 813, Loss: 0.008007613942027092\n",
      "Data loss: 0.002162689110264182, Function loss: 0.006228483747690916\n",
      "Step: 814, Loss: 0.008391172625124454\n",
      "Data loss: 0.002551981247961521, Function loss: 0.0052516828291118145\n",
      "Step: 815, Loss: 0.007803664077073336\n",
      "Data loss: 0.0023297076113522053, Function loss: 0.004962955601513386\n",
      "Step: 816, Loss: 0.007292663212865591\n",
      "Data loss: 0.002203326905146241, Function loss: 0.005368339829146862\n",
      "Step: 817, Loss: 0.0075716665014624596\n",
      "Data loss: 0.0025517847388982773, Function loss: 0.005346875172108412\n",
      "Step: 818, Loss: 0.007898660376667976\n",
      "Data loss: 0.002172427950426936, Function loss: 0.005449935328215361\n",
      "Step: 819, Loss: 0.007622363045811653\n",
      "Data loss: 0.002329739974811673, Function loss: 0.004911854397505522\n",
      "Step: 820, Loss: 0.007241594605147839\n",
      "Data loss: 0.0023788546677678823, Function loss: 0.0049187196418643\n",
      "Step: 821, Loss: 0.007297574542462826\n",
      "Data loss: 0.002122738864272833, Function loss: 0.0054603153839707375\n",
      "Step: 822, Loss: 0.00758305424824357\n",
      "Data loss: 0.002456583082675934, Function loss: 0.00514958705753088\n",
      "Step: 823, Loss: 0.007606170140206814\n",
      "Data loss: 0.002157565439119935, Function loss: 0.0051833754405379295\n",
      "Step: 824, Loss: 0.007340940646827221\n",
      "Data loss: 0.0022427926305681467, Function loss: 0.004901071544736624\n",
      "Step: 825, Loss: 0.007143864408135414\n",
      "Data loss: 0.0023250053636729717, Function loss: 0.004886387847363949\n",
      "Step: 826, Loss: 0.0072113932110369205\n",
      "Data loss: 0.002080159028992057, Function loss: 0.0052844202145934105\n",
      "Step: 827, Loss: 0.007364579476416111\n",
      "Data loss: 0.0023614640813320875, Function loss: 0.004987041465938091\n",
      "Step: 828, Loss: 0.0073485057801008224\n",
      "Data loss: 0.002115281531587243, Function loss: 0.005049464292824268\n",
      "Step: 829, Loss: 0.007164745591580868\n",
      "Data loss: 0.00218381779268384, Function loss: 0.004865480121225119\n",
      "Step: 830, Loss: 0.0070492979139089584\n",
      "Data loss: 0.002253748709335923, Function loss: 0.004857243038713932\n",
      "Step: 831, Loss: 0.007110991515219212\n",
      "Data loss: 0.0020406947005540133, Function loss: 0.005172859411686659\n",
      "Step: 832, Loss: 0.007213554345071316\n",
      "Data loss: 0.0022786955814808607, Function loss: 0.004924807697534561\n",
      "Step: 833, Loss: 0.0072035035118460655\n",
      "Data loss: 0.002044642111286521, Function loss: 0.005055312067270279\n",
      "Step: 834, Loss: 0.0070999544113874435\n",
      "Data loss: 0.0021631221752613783, Function loss: 0.004873454570770264\n",
      "Step: 835, Loss: 0.007036576978862286\n",
      "Data loss: 0.002124769613146782, Function loss: 0.004918213468044996\n",
      "Step: 836, Loss: 0.007042983081191778\n",
      "Data loss: 0.0020629065111279488, Function loss: 0.004994327202439308\n",
      "Step: 837, Loss: 0.007057233713567257\n",
      "Data loss: 0.002137500559911132, Function loss: 0.004882827401161194\n",
      "Step: 838, Loss: 0.007020328193902969\n",
      "Data loss: 0.0020423252135515213, Function loss: 0.0049086930230259895\n",
      "Step: 839, Loss: 0.006951018236577511\n",
      "Data loss: 0.002064718632027507, Function loss: 0.004841971211135387\n",
      "Step: 840, Loss: 0.006906690075993538\n",
      "Data loss: 0.0020658208522945642, Function loss: 0.00485402625054121\n",
      "Step: 841, Loss: 0.006919846870005131\n",
      "Data loss: 0.002011066535487771, Function loss: 0.004929088521748781\n",
      "Step: 842, Loss: 0.006940155290067196\n",
      "Data loss: 0.002050642156973481, Function loss: 0.00488673010841012\n",
      "Step: 843, Loss: 0.006937372498214245\n",
      "Data loss: 0.002007213421165943, Function loss: 0.004896875470876694\n",
      "Step: 844, Loss: 0.006904088892042637\n",
      "Data loss: 0.00198829616419971, Function loss: 0.004880780354142189\n",
      "Step: 845, Loss: 0.006869076751172543\n",
      "Data loss: 0.0020215651020407677, Function loss: 0.004829802550375462\n",
      "Step: 846, Loss: 0.006851367652416229\n",
      "Data loss: 0.0019318175036460161, Function loss: 0.004904892761260271\n",
      "Step: 847, Loss: 0.006836710497736931\n",
      "Data loss: 0.002008498413488269, Function loss: 0.0048172068782150745\n",
      "Step: 848, Loss: 0.0068257050588727\n",
      "Data loss: 0.0019287730101495981, Function loss: 0.004869815427809954\n",
      "Step: 849, Loss: 0.0067985886707901955\n",
      "Data loss: 0.0019462211057543755, Function loss: 0.004834127612411976\n",
      "Step: 850, Loss: 0.006780348718166351\n",
      "Data loss: 0.00195505004376173, Function loss: 0.004826686345040798\n",
      "Step: 851, Loss: 0.006781736388802528\n",
      "Data loss: 0.0018779828678816557, Function loss: 0.004917174577713013\n",
      "Step: 852, Loss: 0.006795157678425312\n",
      "Data loss: 0.0019733398221433163, Function loss: 0.004840131849050522\n",
      "Step: 853, Loss: 0.006813471671193838\n",
      "Data loss: 0.0018300391966477036, Function loss: 0.004998901858925819\n",
      "Step: 854, Loss: 0.006828940939158201\n",
      "Data loss: 0.001969453878700733, Function loss: 0.004872501827776432\n",
      "Step: 855, Loss: 0.006841955706477165\n",
      "Data loss: 0.0018095971317961812, Function loss: 0.005024406127631664\n",
      "Step: 856, Loss: 0.006834003143012524\n",
      "Data loss: 0.0019417429575696588, Function loss: 0.004878238309174776\n",
      "Step: 857, Loss: 0.006819981150329113\n",
      "Data loss: 0.0018077687127515674, Function loss: 0.00499989977106452\n",
      "Step: 858, Loss: 0.006807668600231409\n",
      "Data loss: 0.0018973620608448982, Function loss: 0.004896800499409437\n",
      "Step: 859, Loss: 0.006794162560254335\n",
      "Data loss: 0.0018258990021422505, Function loss: 0.00495616439729929\n",
      "Step: 860, Loss: 0.006782063283026218\n",
      "Data loss: 0.0018523214384913445, Function loss: 0.004916811361908913\n",
      "Step: 861, Loss: 0.006769132800400257\n",
      "Data loss: 0.001817758078686893, Function loss: 0.0049285595305264\n",
      "Step: 862, Loss: 0.006746317725628614\n",
      "Data loss: 0.0018426942406222224, Function loss: 0.0048906211741268635\n",
      "Step: 863, Loss: 0.006733315531164408\n",
      "Data loss: 0.0017724779900163412, Function loss: 0.00494789844378829\n",
      "Step: 864, Loss: 0.006720376200973988\n",
      "Data loss: 0.0018621917115524411, Function loss: 0.004872343502938747\n",
      "Step: 865, Loss: 0.006734535098075867\n",
      "Data loss: 0.0017129034968093038, Function loss: 0.005047221668064594\n",
      "Step: 866, Loss: 0.006760125048458576\n",
      "Data loss: 0.0018812710186466575, Function loss: 0.004916275851428509\n",
      "Step: 867, Loss: 0.006797546986490488\n",
      "Data loss: 0.0016805630875751376, Function loss: 0.005140301771461964\n",
      "Step: 868, Loss: 0.006820864975452423\n",
      "Data loss: 0.0018750157905742526, Function loss: 0.004969386849552393\n",
      "Step: 869, Loss: 0.006844402756541967\n",
      "Data loss: 0.0016736502293497324, Function loss: 0.005185640417039394\n",
      "Step: 870, Loss: 0.006859290413558483\n",
      "Data loss: 0.0018515116535127163, Function loss: 0.005021766293793917\n",
      "Step: 871, Loss: 0.006873277947306633\n",
      "Data loss: 0.0016803842736408114, Function loss: 0.005213265307247639\n",
      "Step: 872, Loss: 0.006893649697303772\n",
      "Data loss: 0.0018352544866502285, Function loss: 0.005117443390190601\n",
      "Step: 873, Loss: 0.00695269787684083\n",
      "Data loss: 0.0016782782040536404, Function loss: 0.005372658371925354\n",
      "Step: 874, Loss: 0.007050936575978994\n",
      "Data loss: 0.0018577002920210361, Function loss: 0.005391190759837627\n",
      "Step: 875, Loss: 0.0072488910518586636\n",
      "Data loss: 0.00164916948415339, Function loss: 0.005946164019405842\n",
      "Step: 876, Loss: 0.007595333270728588\n",
      "Data loss: 0.001961412141099572, Function loss: 0.00625603087246418\n",
      "Step: 877, Loss: 0.008217442780733109\n",
      "Data loss: 0.0016137417405843735, Function loss: 0.007616372313350439\n",
      "Step: 878, Loss: 0.0092301145195961\n",
      "Data loss: 0.0022001657634973526, Function loss: 0.008792909793555737\n",
      "Step: 879, Loss: 0.01099307555705309\n",
      "Data loss: 0.0016383561305701733, Function loss: 0.011906703002750874\n",
      "Step: 880, Loss: 0.01354505866765976\n",
      "Data loss: 0.002667598892003298, Function loss: 0.015026070177555084\n",
      "Step: 881, Loss: 0.017693668603897095\n",
      "Data loss: 0.0018039278220385313, Function loss: 0.020385777577757835\n",
      "Step: 882, Loss: 0.022189704701304436\n",
      "Data loss: 0.003311876440420747, Function loss: 0.02510119043290615\n",
      "Step: 883, Loss: 0.028413066640496254\n",
      "Data loss: 0.0019239044049754739, Function loss: 0.028546974062919617\n",
      "Step: 884, Loss: 0.03047087788581848\n",
      "Data loss: 0.0033719323109835386, Function loss: 0.02655630372464657\n",
      "Step: 885, Loss: 0.029928235337138176\n",
      "Data loss: 0.0015457585686817765, Function loss: 0.018798796460032463\n",
      "Step: 886, Loss: 0.020344555377960205\n",
      "Data loss: 0.002266217488795519, Function loss: 0.008745838887989521\n",
      "Step: 887, Loss: 0.011012056842446327\n",
      "Data loss: 0.0016785681946203113, Function loss: 0.0051087383180856705\n",
      "Step: 888, Loss: 0.0067873066291213036\n",
      "Data loss: 0.0017124655423685908, Function loss: 0.008359726518392563\n",
      "Step: 889, Loss: 0.010072192177176476\n",
      "Data loss: 0.0026079018134623766, Function loss: 0.012732209637761116\n",
      "Step: 890, Loss: 0.015340111218392849\n",
      "Data loss: 0.0015633630100637674, Function loss: 0.013433460146188736\n",
      "Step: 891, Loss: 0.01499682292342186\n",
      "Data loss: 0.002320839324966073, Function loss: 0.008274880237877369\n",
      "Step: 892, Loss: 0.010595719330012798\n",
      "Data loss: 0.0017165080644190311, Function loss: 0.005223562475293875\n",
      "Step: 893, Loss: 0.006940070539712906\n",
      "Data loss: 0.0017880982486531138, Function loss: 0.006289353594183922\n",
      "Step: 894, Loss: 0.008077451959252357\n",
      "Data loss: 0.002388688502833247, Function loss: 0.00872872769832611\n",
      "Step: 895, Loss: 0.011117416433990002\n",
      "Data loss: 0.0016029319958761334, Function loss: 0.00944178830832243\n",
      "Step: 896, Loss: 0.011044720187783241\n",
      "Data loss: 0.002214920474216342, Function loss: 0.006239863112568855\n",
      "Step: 897, Loss: 0.008454783819615841\n",
      "Data loss: 0.001828645938076079, Function loss: 0.004929947666823864\n",
      "Step: 898, Loss: 0.006758593488484621\n",
      "Data loss: 0.0018075593980029225, Function loss: 0.006030040327459574\n",
      "Step: 899, Loss: 0.007837600074708462\n",
      "Data loss: 0.002285750349983573, Function loss: 0.007109815254807472\n",
      "Step: 900, Loss: 0.009395565837621689\n",
      "Data loss: 0.0016481884522363544, Function loss: 0.007125342730432749\n",
      "Step: 901, Loss: 0.008773530833423138\n",
      "Data loss: 0.0020894717890769243, Function loss: 0.005033798050135374\n",
      "Step: 902, Loss: 0.007123270072042942\n",
      "Data loss: 0.0019001682521775365, Function loss: 0.004754583351314068\n",
      "Step: 903, Loss: 0.0066547514870762825\n",
      "Data loss: 0.0017713619163259864, Function loss: 0.005836624652147293\n",
      "Step: 904, Loss: 0.007607986684888601\n",
      "Data loss: 0.0021759243682026863, Function loss: 0.006018778309226036\n",
      "Step: 905, Loss: 0.008194702677428722\n",
      "Data loss: 0.001673425897024572, Function loss: 0.005738185718655586\n",
      "Step: 906, Loss: 0.00741161173209548\n",
      "Data loss: 0.0019708466716110706, Function loss: 0.004564980044960976\n",
      "Step: 907, Loss: 0.006535826716572046\n",
      "Data loss: 0.0019222911214455962, Function loss: 0.004750996828079224\n",
      "Step: 908, Loss: 0.006673288065940142\n",
      "Data loss: 0.0017258101142942905, Function loss: 0.005527202505618334\n",
      "Step: 909, Loss: 0.007253012619912624\n",
      "Data loss: 0.002052348805591464, Function loss: 0.0051797134801745415\n",
      "Step: 910, Loss: 0.007232062518596649\n",
      "Data loss: 0.001698954845778644, Function loss: 0.004917665850371122\n",
      "Step: 911, Loss: 0.006616620812565088\n",
      "Data loss: 0.0018622908974066377, Function loss: 0.004470478743314743\n",
      "Step: 912, Loss: 0.006332769524306059\n",
      "Data loss: 0.0019108603009954095, Function loss: 0.004748305305838585\n",
      "Step: 913, Loss: 0.0066591654904186726\n",
      "Data loss: 0.0016833875561133027, Function loss: 0.005286147817969322\n",
      "Step: 914, Loss: 0.006969535257667303\n",
      "Data loss: 0.001971833175048232, Function loss: 0.004836986307054758\n",
      "Step: 915, Loss: 0.0068088192492723465\n",
      "Data loss: 0.0016805107006803155, Function loss: 0.00471346965059638\n",
      "Step: 916, Loss: 0.006393980234861374\n",
      "Data loss: 0.0018203037325292826, Function loss: 0.00441992562264204\n",
      "Step: 917, Loss: 0.006240229122340679\n",
      "Data loss: 0.001839052769355476, Function loss: 0.004572500474750996\n",
      "Step: 918, Loss: 0.006411553360521793\n",
      "Data loss: 0.0016696217935532331, Function loss: 0.004923192318528891\n",
      "Step: 919, Loss: 0.00659281387925148\n",
      "Data loss: 0.0019048170652240515, Function loss: 0.00465428875759244\n",
      "Step: 920, Loss: 0.0065591055899858475\n",
      "Data loss: 0.001643895055167377, Function loss: 0.004687499720603228\n",
      "Step: 921, Loss: 0.0063313948921859264\n",
      "Data loss: 0.0018064993200823665, Function loss: 0.004347334150224924\n",
      "Step: 922, Loss: 0.006153833586722612\n",
      "Data loss: 0.0017452059546485543, Function loss: 0.004403873346745968\n",
      "Step: 923, Loss: 0.0061490791849792\n",
      "Data loss: 0.001677116728387773, Function loss: 0.004572709556668997\n",
      "Step: 924, Loss: 0.006249826401472092\n",
      "Data loss: 0.0018255055183544755, Function loss: 0.004489593673497438\n",
      "Step: 925, Loss: 0.006315099075436592\n",
      "Data loss: 0.0016201455146074295, Function loss: 0.004634879529476166\n",
      "Step: 926, Loss: 0.006255025044083595\n",
      "Data loss: 0.0017851947341114283, Function loss: 0.00434667756780982\n",
      "Step: 927, Loss: 0.006131872534751892\n",
      "Data loss: 0.001668409793637693, Function loss: 0.004376733209937811\n",
      "Step: 928, Loss: 0.006045143119990826\n",
      "Data loss: 0.0016794687835499644, Function loss: 0.004374082665890455\n",
      "Step: 929, Loss: 0.0060535515658557415\n",
      "Data loss: 0.0017403807723894715, Function loss: 0.0043672979809343815\n",
      "Step: 930, Loss: 0.006107678636908531\n",
      "Data loss: 0.0015973723493516445, Function loss: 0.0045564561150968075\n",
      "Step: 931, Loss: 0.006153828464448452\n",
      "Data loss: 0.0017680017044767737, Function loss: 0.004398444201797247\n",
      "Step: 932, Loss: 0.0061664460226893425\n",
      "Data loss: 0.0015637987526133657, Function loss: 0.004581758752465248\n",
      "Step: 933, Loss: 0.006145557388663292\n",
      "Data loss: 0.0017398659838363528, Function loss: 0.00434838468208909\n",
      "Step: 934, Loss: 0.006088250782340765\n",
      "Data loss: 0.0015814054058864713, Function loss: 0.004434564616531134\n",
      "Step: 935, Loss: 0.006015969906002283\n",
      "Data loss: 0.0016616802895441651, Function loss: 0.004295049700886011\n",
      "Step: 936, Loss: 0.005956729874014854\n",
      "Data loss: 0.0016358491266146302, Function loss: 0.004304064903408289\n",
      "Step: 937, Loss: 0.005939913913607597\n",
      "Data loss: 0.0015741817187517881, Function loss: 0.004388453438878059\n",
      "Step: 938, Loss: 0.005962635390460491\n",
      "Data loss: 0.0016785543411970139, Function loss: 0.0043182168155908585\n",
      "Step: 939, Loss: 0.005996771156787872\n",
      "Data loss: 0.001524950610473752, Function loss: 0.004487298894673586\n",
      "Step: 940, Loss: 0.006012249737977982\n",
      "Data loss: 0.00166707718744874, Function loss: 0.004320103675127029\n",
      "Step: 941, Loss: 0.005987180862575769\n",
      "Data loss: 0.0015335208736360073, Function loss: 0.004396926611661911\n",
      "Step: 942, Loss: 0.005930447485297918\n",
      "Data loss: 0.0015998106682673097, Function loss: 0.004278764594346285\n",
      "Step: 943, Loss: 0.005878575146198273\n",
      "Data loss: 0.0015800499822944403, Function loss: 0.004280183929949999\n",
      "Step: 944, Loss: 0.0058602336794137955\n",
      "Data loss: 0.001522372360341251, Function loss: 0.004356894176453352\n",
      "Step: 945, Loss: 0.005879266653209925\n",
      "Data loss: 0.0016143922694027424, Function loss: 0.004287852440029383\n",
      "Step: 946, Loss: 0.005902244709432125\n",
      "Data loss: 0.0014827230479568243, Function loss: 0.004426700994372368\n",
      "Step: 947, Loss: 0.005909424275159836\n",
      "Data loss: 0.0016032708808779716, Function loss: 0.004288538359105587\n",
      "Step: 948, Loss: 0.005891809239983559\n",
      "Data loss: 0.0014827908016741276, Function loss: 0.004373762756586075\n",
      "Step: 949, Loss: 0.005856553558260202\n",
      "Data loss: 0.0015600092010572553, Function loss: 0.004261829890310764\n",
      "Step: 950, Loss: 0.005821838974952698\n",
      "Data loss: 0.0014976535458117723, Function loss: 0.004301554523408413\n",
      "Step: 951, Loss: 0.005799207836389542\n",
      "Data loss: 0.0015154150314629078, Function loss: 0.0042694504372775555\n",
      "Step: 952, Loss: 0.005784865468740463\n",
      "Data loss: 0.0015125544741749763, Function loss: 0.004268373362720013\n",
      "Step: 953, Loss: 0.005780927836894989\n",
      "Data loss: 0.0014769453555345535, Function loss: 0.0043014585971832275\n",
      "Step: 954, Loss: 0.005778403952717781\n",
      "Data loss: 0.0015158122405409813, Function loss: 0.004256478976458311\n",
      "Step: 955, Loss: 0.005772291216999292\n",
      "Data loss: 0.001455211080610752, Function loss: 0.00430653290823102\n",
      "Step: 956, Loss: 0.005761743988841772\n",
      "Data loss: 0.0015013805823400617, Function loss: 0.004250871017575264\n",
      "Step: 957, Loss: 0.005752251483500004\n",
      "Data loss: 0.0014488683082163334, Function loss: 0.004283363930881023\n",
      "Step: 958, Loss: 0.005732232239097357\n",
      "Data loss: 0.0014772797003388405, Function loss: 0.004245988558977842\n",
      "Step: 959, Loss: 0.005723268259316683\n",
      "Data loss: 0.001447702175937593, Function loss: 0.004266713280230761\n",
      "Step: 960, Loss: 0.005714415572583675\n",
      "Data loss: 0.0014572500949725509, Function loss: 0.004250190686434507\n",
      "Step: 961, Loss: 0.00570744089782238\n",
      "Data loss: 0.0014393663732334971, Function loss: 0.004262926056981087\n",
      "Step: 962, Loss: 0.005702292546629906\n",
      "Data loss: 0.0014489918248727918, Function loss: 0.004247857723385096\n",
      "Step: 963, Loss: 0.0056968494318425655\n",
      "Data loss: 0.0014197343261912465, Function loss: 0.004275499377399683\n",
      "Step: 964, Loss: 0.005695233587175608\n",
      "Data loss: 0.0014495188370347023, Function loss: 0.0042438870295882225\n",
      "Step: 965, Loss: 0.005693405866622925\n",
      "Data loss: 0.0013914374867454171, Function loss: 0.004304380156099796\n",
      "Step: 966, Loss: 0.005695817526429892\n",
      "Data loss: 0.0014577151741832495, Function loss: 0.0042464397847652435\n",
      "Step: 967, Loss: 0.005704155191779137\n",
      "Data loss: 0.0013647949090227485, Function loss: 0.004336962476372719\n",
      "Step: 968, Loss: 0.005701757501810789\n",
      "Data loss: 0.001457725651562214, Function loss: 0.00424590241163969\n",
      "Step: 969, Loss: 0.005703628063201904\n",
      "Data loss: 0.001346898963674903, Function loss: 0.004352072719484568\n",
      "Step: 970, Loss: 0.005698971450328827\n",
      "Data loss: 0.00145658478140831, Function loss: 0.0042503587901592255\n",
      "Step: 971, Loss: 0.005706943571567535\n",
      "Data loss: 0.0013270789058879018, Function loss: 0.004386251326650381\n",
      "Step: 972, Loss: 0.005713330116122961\n",
      "Data loss: 0.0014582612784579396, Function loss: 0.004271767567843199\n",
      "Step: 973, Loss: 0.005730028729885817\n",
      "Data loss: 0.0013101870426908135, Function loss: 0.004427036270499229\n",
      "Step: 974, Loss: 0.005737223196774721\n",
      "Data loss: 0.0014521701959893107, Function loss: 0.00428547291085124\n",
      "Step: 975, Loss: 0.005737643223255873\n",
      "Data loss: 0.0013028566027060151, Function loss: 0.00442789401859045\n",
      "Step: 976, Loss: 0.005730750504881144\n",
      "Data loss: 0.0014399758074432611, Function loss: 0.0042779333889484406\n",
      "Step: 977, Loss: 0.005717908963561058\n",
      "Data loss: 0.0012976243160665035, Function loss: 0.004399356432259083\n",
      "Step: 978, Loss: 0.005696980748325586\n",
      "Data loss: 0.0014237083960324526, Function loss: 0.004250780213624239\n",
      "Step: 979, Loss: 0.005674488842487335\n",
      "Data loss: 0.0012904657050967216, Function loss: 0.0043675778433680534\n",
      "Step: 980, Loss: 0.005658043548464775\n",
      "Data loss: 0.0014171418733894825, Function loss: 0.0042349412105977535\n",
      "Step: 981, Loss: 0.005652083083987236\n",
      "Data loss: 0.0012766047148033977, Function loss: 0.004369271919131279\n",
      "Step: 982, Loss: 0.0056458767503499985\n",
      "Data loss: 0.0014136568643152714, Function loss: 0.00422579376026988\n",
      "Step: 983, Loss: 0.005639450624585152\n",
      "Data loss: 0.00126765260938555, Function loss: 0.00435590697452426\n",
      "Step: 984, Loss: 0.005623559467494488\n",
      "Data loss: 0.0014018075307831168, Function loss: 0.004198750481009483\n",
      "Step: 985, Loss: 0.005600558128207922\n",
      "Data loss: 0.0012676496990025043, Function loss: 0.00431142607703805\n",
      "Step: 986, Loss: 0.005579075776040554\n",
      "Data loss: 0.001385286683216691, Function loss: 0.004177446011453867\n",
      "Step: 987, Loss: 0.005562732927501202\n",
      "Data loss: 0.0012660571373999119, Function loss: 0.004283179994672537\n",
      "Step: 988, Loss: 0.005549237132072449\n",
      "Data loss: 0.001376891741529107, Function loss: 0.004164897371083498\n",
      "Step: 989, Loss: 0.005541789345443249\n",
      "Data loss: 0.0012550756800919771, Function loss: 0.0042860247194767\n",
      "Step: 990, Loss: 0.005541100166738033\n",
      "Data loss: 0.0013762388844043016, Function loss: 0.0041720145381987095\n",
      "Step: 991, Loss: 0.005548253655433655\n",
      "Data loss: 0.0012376658851280808, Function loss: 0.004324764013290405\n",
      "Step: 992, Loss: 0.005562429782003164\n",
      "Data loss: 0.0013851121766492724, Function loss: 0.004209590144455433\n",
      "Step: 993, Loss: 0.005594702437520027\n",
      "Data loss: 0.0012105247005820274, Function loss: 0.0044385772198438644\n",
      "Step: 994, Loss: 0.005649101920425892\n",
      "Data loss: 0.0014124037697911263, Function loss: 0.004336201120167971\n",
      "Step: 995, Loss: 0.005748604889959097\n",
      "Data loss: 0.0011656098067760468, Function loss: 0.004760043229907751\n",
      "Step: 996, Loss: 0.005925653036683798\n",
      "Data loss: 0.001481642248108983, Function loss: 0.0047957636415958405\n",
      "Step: 997, Loss: 0.00627740565687418\n",
      "Data loss: 0.0010911814169958234, Function loss: 0.005792130250483751\n",
      "Step: 998, Loss: 0.006883311551064253\n",
      "Data loss: 0.0016418893355876207, Function loss: 0.006465099286288023\n",
      "Step: 999, Loss: 0.008106988854706287\n",
      "Data loss: 0.000986242201179266, Function loss: 0.00926720630377531\n",
      "Step: 1000, Loss: 0.01025344803929329\n",
      "Data loss: 0.0020371489226818085, Function loss: 0.01279306411743164\n",
      "Step: 1001, Loss: 0.014830213040113449\n",
      "Data loss: 0.0009192025172524154, Function loss: 0.020856590941548347\n",
      "Step: 1002, Loss: 0.02177579328417778\n",
      "Data loss: 0.002923327963799238, Function loss: 0.032536186277866364\n",
      "Step: 1003, Loss: 0.03545951470732689\n",
      "Data loss: 0.0010808412916958332, Function loss: 0.044930074363946915\n",
      "Step: 1004, Loss: 0.04601091518998146\n",
      "Data loss: 0.0038099477533251047, Function loss: 0.05580706521868706\n",
      "Step: 1005, Loss: 0.05961701273918152\n",
      "Data loss: 0.0011900158133357763, Function loss: 0.04035874828696251\n",
      "Step: 1006, Loss: 0.04154876247048378\n",
      "Data loss: 0.002285793423652649, Function loss: 0.01679057627916336\n",
      "Step: 1007, Loss: 0.01907636970281601\n",
      "Data loss: 0.0014717559097334743, Function loss: 0.0044173686765134335\n",
      "Step: 1008, Loss: 0.0058891247026622295\n",
      "Data loss: 0.001191627117805183, Function loss: 0.014739210717380047\n",
      "Step: 1009, Loss: 0.015930837020277977\n",
      "Data loss: 0.0027986483182758093, Function loss: 0.025456871837377548\n",
      "Step: 1010, Loss: 0.028255520388484\n",
      "Data loss: 0.0013492435682564974, Function loss: 0.01566917635500431\n",
      "Step: 1011, Loss: 0.01701842062175274\n",
      "Data loss: 0.001737956772558391, Function loss: 0.004434575326740742\n",
      "Step: 1012, Loss: 0.006172532215714455\n",
      "Data loss: 0.002210540696978569, Function loss: 0.00946105644106865\n",
      "Step: 1013, Loss: 0.011671597138047218\n",
      "Data loss: 0.001493998570367694, Function loss: 0.015399127267301083\n",
      "Step: 1014, Loss: 0.01689312607049942\n",
      "Data loss: 0.0022419567685574293, Function loss: 0.008390416391193867\n",
      "Step: 1015, Loss: 0.01063237339258194\n",
      "Data loss: 0.0019004634814336896, Function loss: 0.0042260331101715565\n",
      "Step: 1016, Loss: 0.006126496475189924\n",
      "Data loss: 0.0016232490306720138, Function loss: 0.010148048400878906\n",
      "Step: 1017, Loss: 0.011771297082304955\n",
      "Data loss: 0.0024375340435653925, Function loss: 0.010188921354711056\n",
      "Step: 1018, Loss: 0.012626455165445805\n",
      "Data loss: 0.0017835742328315973, Function loss: 0.004650765564292669\n",
      "Step: 1019, Loss: 0.00643434002995491\n",
      "Data loss: 0.001721602980978787, Function loss: 0.006319703534245491\n",
      "Step: 1020, Loss: 0.008041306398808956\n",
      "Data loss: 0.002435153815895319, Function loss: 0.00929574016481638\n",
      "Step: 1021, Loss: 0.011730894446372986\n",
      "Data loss: 0.0017260950990021229, Function loss: 0.006035059690475464\n",
      "Step: 1022, Loss: 0.007761154789477587\n",
      "Data loss: 0.0018229838460683823, Function loss: 0.004274521488696337\n",
      "Step: 1023, Loss: 0.006097505334764719\n",
      "Data loss: 0.0022990263532847166, Function loss: 0.006942179519683123\n",
      "Step: 1024, Loss: 0.009241205640137196\n",
      "Data loss: 0.0016915721353143454, Function loss: 0.006518762558698654\n",
      "Step: 1025, Loss: 0.008210334926843643\n",
      "Data loss: 0.0019215497886762023, Function loss: 0.003870956366881728\n",
      "Step: 1026, Loss: 0.005792506039142609\n",
      "Data loss: 0.0021207735408097506, Function loss: 0.005116597283631563\n",
      "Step: 1027, Loss: 0.007237371057271957\n",
      "Data loss: 0.0016474162694066763, Function loss: 0.006298406515270472\n",
      "Step: 1028, Loss: 0.007945822551846504\n",
      "Data loss: 0.0019844314083456993, Function loss: 0.004122300073504448\n",
      "Step: 1029, Loss: 0.006106731481850147\n",
      "Data loss: 0.0019331293879076838, Function loss: 0.004053311888128519\n",
      "Step: 1030, Loss: 0.005986441392451525\n",
      "Data loss: 0.0016249770997092128, Function loss: 0.005607666913419962\n",
      "Step: 1031, Loss: 0.0072326441295444965\n",
      "Data loss: 0.0020031577441841364, Function loss: 0.004557723645120859\n",
      "Step: 1032, Loss: 0.006560881622135639\n",
      "Data loss: 0.0017625497421249747, Function loss: 0.0037695032078772783\n",
      "Step: 1033, Loss: 0.005532052833586931\n",
      "Data loss: 0.001627470599487424, Function loss: 0.00473033357411623\n",
      "Step: 1034, Loss: 0.00635780394077301\n",
      "Data loss: 0.001974424347281456, Function loss: 0.004786662291735411\n",
      "Step: 1035, Loss: 0.006761086639016867\n",
      "Data loss: 0.0016306540928781033, Function loss: 0.00406263442710042\n",
      "Step: 1036, Loss: 0.005693288519978523\n",
      "Data loss: 0.0016456854064017534, Function loss: 0.003936174791306257\n",
      "Step: 1037, Loss: 0.005581860430538654\n",
      "Data loss: 0.001887642196379602, Function loss: 0.004479014314711094\n",
      "Step: 1038, Loss: 0.006366656627506018\n",
      "Data loss: 0.0015445903409272432, Function loss: 0.004536616150289774\n",
      "Step: 1039, Loss: 0.006081206724047661\n",
      "Data loss: 0.0016965888207778335, Function loss: 0.0036846052389591932\n",
      "Step: 1040, Loss: 0.005381193943321705\n",
      "Data loss: 0.001749584567733109, Function loss: 0.00381844793446362\n",
      "Step: 1041, Loss: 0.005568032618612051\n",
      "Data loss: 0.0015007612528279424, Function loss: 0.004518914967775345\n",
      "Step: 1042, Loss: 0.006019676104187965\n",
      "Data loss: 0.0017548511968925595, Function loss: 0.003998473286628723\n",
      "Step: 1043, Loss: 0.005753324367105961\n",
      "Data loss: 0.0016118494095280766, Function loss: 0.003693344769999385\n",
      "Step: 1044, Loss: 0.005305194295942783\n",
      "Data loss: 0.0015053977258503437, Function loss: 0.003986039664596319\n",
      "Step: 1045, Loss: 0.005491437390446663\n",
      "Data loss: 0.0017541253473609686, Function loss: 0.004023989662528038\n",
      "Step: 1046, Loss: 0.00577811524271965\n",
      "Data loss: 0.0014950118493288755, Function loss: 0.004032018128782511\n",
      "Step: 1047, Loss: 0.005527029745280743\n",
      "Data loss: 0.0015681402292102575, Function loss: 0.0036942879669368267\n",
      "Step: 1048, Loss: 0.005262427963316441\n",
      "Data loss: 0.0016779343131929636, Function loss: 0.003753687022253871\n",
      "Step: 1049, Loss: 0.0054316213354468346\n",
      "Data loss: 0.0014234835980460048, Function loss: 0.004184176214039326\n",
      "Step: 1050, Loss: 0.005607659928500652\n",
      "Data loss: 0.0016605326673015952, Function loss: 0.003788806963711977\n",
      "Step: 1051, Loss: 0.005449339747428894\n",
      "Data loss: 0.0015098745934665203, Function loss: 0.003704066388309002\n",
      "Step: 1052, Loss: 0.005213940981775522\n",
      "Data loss: 0.0014646361814811826, Function loss: 0.003791511058807373\n",
      "Step: 1053, Loss: 0.005256147123873234\n",
      "Data loss: 0.001642525428906083, Function loss: 0.0037978969048708677\n",
      "Step: 1054, Loss: 0.005440422333776951\n",
      "Data loss: 0.0013952295994386077, Function loss: 0.0040285601280629635\n",
      "Step: 1055, Loss: 0.005423789843916893\n",
      "Data loss: 0.0015577602898702025, Function loss: 0.0036646423395723104\n",
      "Step: 1056, Loss: 0.005222402513027191\n",
      "Data loss: 0.001504547894001007, Function loss: 0.0036453153006732464\n",
      "Step: 1057, Loss: 0.0051498631946742535\n",
      "Data loss: 0.0013989544240757823, Function loss: 0.0038655241951346397\n",
      "Step: 1058, Loss: 0.005264478735625744\n",
      "Data loss: 0.0015671789878979325, Function loss: 0.0037438964936882257\n",
      "Step: 1059, Loss: 0.00531107559800148\n",
      "Data loss: 0.001389636192470789, Function loss: 0.003818339668214321\n",
      "Step: 1060, Loss: 0.00520797586068511\n",
      "Data loss: 0.0014631125377491117, Function loss: 0.003645991673693061\n",
      "Step: 1061, Loss: 0.005109104327857494\n",
      "Data loss: 0.0014732015551999211, Function loss: 0.0036520890425890684\n",
      "Step: 1062, Loss: 0.005125290714204311\n",
      "Data loss: 0.0013646814040839672, Function loss: 0.00381216942332685\n",
      "Step: 1063, Loss: 0.0051768505945801735\n",
      "Data loss: 0.0014835253823548555, Function loss: 0.0036765800323337317\n",
      "Step: 1064, Loss: 0.005160105414688587\n",
      "Data loss: 0.001373506966046989, Function loss: 0.003719903063029051\n",
      "Step: 1065, Loss: 0.005093410145491362\n",
      "Data loss: 0.0014007841236889362, Function loss: 0.0036628867965191603\n",
      "Step: 1066, Loss: 0.005063670687377453\n",
      "Data loss: 0.0014221752062439919, Function loss: 0.0036573754623532295\n",
      "Step: 1067, Loss: 0.005079550668597221\n",
      "Data loss: 0.0013384345220401883, Function loss: 0.00375592103227973\n",
      "Step: 1068, Loss: 0.005094355437904596\n",
      "Data loss: 0.0014161875005811453, Function loss: 0.003660770133137703\n",
      "Step: 1069, Loss: 0.005076957866549492\n",
      "Data loss: 0.0013384787598624825, Function loss: 0.0036986577324569225\n",
      "Step: 1070, Loss: 0.005037136375904083\n",
      "Data loss: 0.0013602664694190025, Function loss: 0.0036608674563467503\n",
      "Step: 1071, Loss: 0.005021133925765753\n",
      "Data loss: 0.0013614681083709002, Function loss: 0.0036648979876190424\n",
      "Step: 1072, Loss: 0.0050263660959899426\n",
      "Data loss: 0.0013170037418603897, Function loss: 0.0037179484497755766\n",
      "Step: 1073, Loss: 0.00503495242446661\n",
      "Data loss: 0.0013548205606639385, Function loss: 0.0036751427687704563\n",
      "Step: 1074, Loss: 0.005029963329434395\n",
      "Data loss: 0.0013067602412775159, Function loss: 0.0037045315839350224\n",
      "Step: 1075, Loss: 0.005011291708797216\n",
      "Data loss: 0.001319356495514512, Function loss: 0.0036710496060550213\n",
      "Step: 1076, Loss: 0.004990406334400177\n",
      "Data loss: 0.0013160706730559468, Function loss: 0.003658299334347248\n",
      "Step: 1077, Loss: 0.004974369890987873\n",
      "Data loss: 0.0012774051865562797, Function loss: 0.0036933247465640306\n",
      "Step: 1078, Loss: 0.0049707298167049885\n",
      "Data loss: 0.0013160814996808767, Function loss: 0.0036557090934365988\n",
      "Step: 1079, Loss: 0.0049717905931174755\n",
      "Data loss: 0.0012657978804782033, Function loss: 0.0036965396720916033\n",
      "Step: 1080, Loss: 0.004962337668985128\n",
      "Data loss: 0.0012813688954338431, Function loss: 0.0036674062721431255\n",
      "Step: 1081, Loss: 0.0049487752839922905\n",
      "Data loss: 0.001281747012399137, Function loss: 0.0036603757180273533\n",
      "Step: 1082, Loss: 0.004942122846841812\n",
      "Data loss: 0.0012391653144732118, Function loss: 0.0037082917988300323\n",
      "Step: 1083, Loss: 0.004947456996887922\n",
      "Data loss: 0.0012918064603582025, Function loss: 0.0036591915413737297\n",
      "Step: 1084, Loss: 0.00495099788531661\n",
      "Data loss: 0.0012172946007922292, Function loss: 0.0037249072920531034\n",
      "Step: 1085, Loss: 0.0049422020092606544\n",
      "Data loss: 0.0012728580040857196, Function loss: 0.0036510564386844635\n",
      "Step: 1086, Loss: 0.004923914559185505\n",
      "Data loss: 0.0012206081300973892, Function loss: 0.0036826585419476032\n",
      "Step: 1087, Loss: 0.0049032666720449924\n",
      "Data loss: 0.0012345622526481748, Function loss: 0.0036544196773320436\n",
      "Step: 1088, Loss: 0.00488898204639554\n",
      "Data loss: 0.0012343220878392458, Function loss: 0.0036486205644905567\n",
      "Step: 1089, Loss: 0.004882942885160446\n",
      "Data loss: 0.0012023416347801685, Function loss: 0.0036805521231144667\n",
      "Step: 1090, Loss: 0.0048828935250639915\n",
      "Data loss: 0.001235954463481903, Function loss: 0.0036446386948227882\n",
      "Step: 1091, Loss: 0.004880593158304691\n",
      "Data loss: 0.0011876900680363178, Function loss: 0.003683388000354171\n",
      "Step: 1092, Loss: 0.004871077835559845\n",
      "Data loss: 0.001218562014400959, Function loss: 0.003643099684268236\n",
      "Step: 1093, Loss: 0.004861661698669195\n",
      "Data loss: 0.001193532138131559, Function loss: 0.003656452987343073\n",
      "Step: 1094, Loss: 0.004849985241889954\n",
      "Data loss: 0.001187641522847116, Function loss: 0.0036557798739522696\n",
      "Step: 1095, Loss: 0.004843421280384064\n",
      "Data loss: 0.0012005472090095282, Function loss: 0.0036394644994288683\n",
      "Step: 1096, Loss: 0.0048400117084383965\n",
      "Data loss: 0.0011680983006954193, Function loss: 0.0036672349087893963\n",
      "Step: 1097, Loss: 0.004835333209484816\n",
      "Data loss: 0.0011929324828088284, Function loss: 0.0036341482773423195\n",
      "Step: 1098, Loss: 0.004827080760151148\n",
      "Data loss: 0.0011635150294750929, Function loss: 0.003652971936389804\n",
      "Step: 1099, Loss: 0.004816486965864897\n",
      "Data loss: 0.0011724765645340085, Function loss: 0.003637553658336401\n",
      "Step: 1100, Loss: 0.004810030106455088\n",
      "Data loss: 0.0011646371567621827, Function loss: 0.0036412652116268873\n",
      "Step: 1101, Loss: 0.004805902484804392\n",
      "Data loss: 0.0011574485106393695, Function loss: 0.003642717609182\n",
      "Step: 1102, Loss: 0.004800166003406048\n",
      "Data loss: 0.0011567071778699756, Function loss: 0.0036372561007738113\n",
      "Step: 1103, Loss: 0.004793963395059109\n",
      "Data loss: 0.001149340532720089, Function loss: 0.0036398423835635185\n",
      "Step: 1104, Loss: 0.0047891829162836075\n",
      "Data loss: 0.0011468905722722411, Function loss: 0.003633511485531926\n",
      "Step: 1105, Loss: 0.0047804019413888454\n",
      "Data loss: 0.0011438517831265926, Function loss: 0.0036292201839387417\n",
      "Step: 1106, Loss: 0.004773071967065334\n",
      "Data loss: 0.0011366258841007948, Function loss: 0.003632296109572053\n",
      "Step: 1107, Loss: 0.004768921993672848\n",
      "Data loss: 0.0011358237825334072, Function loss: 0.0036288045812398195\n",
      "Step: 1108, Loss: 0.00476462859660387\n",
      "Data loss: 0.0011309009278193116, Function loss: 0.0036334481555968523\n",
      "Step: 1109, Loss: 0.004764349199831486\n",
      "Data loss: 0.0011289279209449887, Function loss: 0.0036326772533357143\n",
      "Step: 1110, Loss: 0.004761605057865381\n",
      "Data loss: 0.0011210569646209478, Function loss: 0.0036312879528850317\n",
      "Step: 1111, Loss: 0.0047523449175059795\n",
      "Data loss: 0.0011270421091467142, Function loss: 0.003621852956712246\n",
      "Step: 1112, Loss: 0.0047488948330283165\n",
      "Data loss: 0.0011068765306845307, Function loss: 0.0036371059250086546\n",
      "Step: 1113, Loss: 0.004743982572108507\n",
      "Data loss: 0.0011277212761342525, Function loss: 0.0036134039983153343\n",
      "Step: 1114, Loss: 0.004741125274449587\n",
      "Data loss: 0.0010958088096231222, Function loss: 0.003643237752839923\n",
      "Step: 1115, Loss: 0.004739046562463045\n",
      "Data loss: 0.001123411930166185, Function loss: 0.0036134584806859493\n",
      "Step: 1116, Loss: 0.004736870527267456\n",
      "Data loss: 0.001087356242351234, Function loss: 0.0036490398924797773\n",
      "Step: 1117, Loss: 0.0047363960184156895\n",
      "Data loss: 0.001116810250096023, Function loss: 0.003613529959693551\n",
      "Step: 1118, Loss: 0.004730340093374252\n",
      "Data loss: 0.0010846793884411454, Function loss: 0.0036427052691578865\n",
      "Step: 1119, Loss: 0.00472738454118371\n",
      "Data loss: 0.0011056209914386272, Function loss: 0.0036189670208841562\n",
      "Step: 1120, Loss: 0.004724588245153427\n",
      "Data loss: 0.0010845208307728171, Function loss: 0.0036393585614860058\n",
      "Step: 1121, Loss: 0.004723879508674145\n",
      "Data loss: 0.0010970475850626826, Function loss: 0.0036360472440719604\n",
      "Step: 1122, Loss: 0.004733094945549965\n",
      "Data loss: 0.0010809656232595444, Function loss: 0.003664995776489377\n",
      "Step: 1123, Loss: 0.004745961166918278\n",
      "Data loss: 0.0010992547031491995, Function loss: 0.003668372053653002\n",
      "Step: 1124, Loss: 0.004767626523971558\n",
      "Data loss: 0.0010726276086643338, Function loss: 0.0037334223743528128\n",
      "Step: 1125, Loss: 0.004806050099432468\n",
      "Data loss: 0.0011109684128314257, Function loss: 0.0037570728454738855\n",
      "Step: 1126, Loss: 0.004868041258305311\n",
      "Data loss: 0.001070328988134861, Function loss: 0.003890864085406065\n",
      "Step: 1127, Loss: 0.004961193073540926\n",
      "Data loss: 0.0011366326361894608, Function loss: 0.003966717515140772\n",
      "Step: 1128, Loss: 0.005103350151330233\n",
      "Data loss: 0.0010786715429276228, Function loss: 0.004239893052726984\n",
      "Step: 1129, Loss: 0.005318564362823963\n",
      "Data loss: 0.0011922860285267234, Function loss: 0.004453766159713268\n",
      "Step: 1130, Loss: 0.0056460523046553135\n",
      "Data loss: 0.0011217541759833694, Function loss: 0.005018867552280426\n",
      "Step: 1131, Loss: 0.006140621844679117\n",
      "Data loss: 0.0013017401797696948, Function loss: 0.005547547712922096\n",
      "Step: 1132, Loss: 0.006849288009107113\n",
      "Data loss: 0.0012345927534624934, Function loss: 0.006644940935075283\n",
      "Step: 1133, Loss: 0.007879533804953098\n",
      "Data loss: 0.0015169443795457482, Function loss: 0.007724332623183727\n",
      "Step: 1134, Loss: 0.009241277351975441\n",
      "Data loss: 0.0014351190766319633, Function loss: 0.009431499056518078\n",
      "Step: 1135, Loss: 0.010866617783904076\n",
      "Data loss: 0.001801379956305027, Function loss: 0.010548392310738564\n",
      "Step: 1136, Loss: 0.01234977226704359\n",
      "Data loss: 0.0015893472591415048, Function loss: 0.01147292461246252\n",
      "Step: 1137, Loss: 0.01306227222084999\n",
      "Data loss: 0.0017789117991924286, Function loss: 0.010428814217448235\n",
      "Step: 1138, Loss: 0.012207726016640663\n",
      "Data loss: 0.001379945082589984, Function loss: 0.008427179418504238\n",
      "Step: 1139, Loss: 0.009807124733924866\n",
      "Data loss: 0.0012517592404037714, Function loss: 0.0055998628959059715\n",
      "Step: 1140, Loss: 0.006851621903479099\n",
      "Data loss: 0.0011175285326316953, Function loss: 0.0038664687890559435\n",
      "Step: 1141, Loss: 0.004983997438102961\n",
      "Data loss: 0.0009872254449874163, Function loss: 0.004050085321068764\n",
      "Step: 1142, Loss: 0.005037310533225536\n",
      "Data loss: 0.0013228240422904491, Function loss: 0.005026814993470907\n",
      "Step: 1143, Loss: 0.006349639035761356\n",
      "Data loss: 0.0011688719969242811, Function loss: 0.0062658414244651794\n",
      "Step: 1144, Loss: 0.007434713654220104\n",
      "Data loss: 0.0013665269361808896, Function loss: 0.005911634769290686\n",
      "Step: 1145, Loss: 0.007278161589056253\n",
      "Data loss: 0.0011655909474939108, Function loss: 0.004983867052942514\n",
      "Step: 1146, Loss: 0.0061494577676057816\n",
      "Data loss: 0.001043132389895618, Function loss: 0.0041224234737455845\n",
      "Step: 1147, Loss: 0.005165555980056524\n",
      "Data loss: 0.0012009654892608523, Function loss: 0.0039115347899496555\n",
      "Step: 1148, Loss: 0.00511250039562583\n",
      "Data loss: 0.0009666758705861866, Function loss: 0.004741075448691845\n",
      "Step: 1149, Loss: 0.005707751493901014\n",
      "Data loss: 0.0013329637004062533, Function loss: 0.004791483283042908\n",
      "Step: 1150, Loss: 0.006124447099864483\n",
      "Data loss: 0.001024687779136002, Function loss: 0.004812049213796854\n",
      "Step: 1151, Loss: 0.005836736876517534\n",
      "Data loss: 0.0011855150805786252, Function loss: 0.0038946005515754223\n",
      "Step: 1152, Loss: 0.005080115515738726\n",
      "Data loss: 0.0010722323786467314, Function loss: 0.0035038823261857033\n",
      "Step: 1153, Loss: 0.004576114937663078\n",
      "Data loss: 0.0010056906612589955, Function loss: 0.0037303841672837734\n",
      "Step: 1154, Loss: 0.004736074712127447\n",
      "Data loss: 0.0012312200851738453, Function loss: 0.004029795993119478\n",
      "Step: 1155, Loss: 0.0052610160782933235\n",
      "Data loss: 0.000983109581284225, Function loss: 0.004574076272547245\n",
      "Step: 1156, Loss: 0.005557185970246792\n",
      "Data loss: 0.0012449083151295781, Function loss: 0.004116488620638847\n",
      "Step: 1157, Loss: 0.005361396819353104\n",
      "Data loss: 0.000987683655694127, Function loss: 0.00389083637855947\n",
      "Step: 1158, Loss: 0.004878520034253597\n",
      "Data loss: 0.0011035811621695757, Function loss: 0.0034494593273848295\n",
      "Step: 1159, Loss: 0.004553040489554405\n",
      "Data loss: 0.0010814705165103078, Function loss: 0.0035112572368234396\n",
      "Step: 1160, Loss: 0.004592727869749069\n",
      "Data loss: 0.0010304286843165755, Function loss: 0.0038128404412418604\n",
      "Step: 1161, Loss: 0.004843269009143114\n",
      "Data loss: 0.0011735283769667149, Function loss: 0.0038241983857005835\n",
      "Step: 1162, Loss: 0.004997726529836655\n",
      "Data loss: 0.0010065422393381596, Function loss: 0.003912467509508133\n",
      "Step: 1163, Loss: 0.0049190097488462925\n",
      "Data loss: 0.0011300636688247323, Function loss: 0.0035578410606831312\n",
      "Step: 1164, Loss: 0.004687904845923185\n",
      "Data loss: 0.0010161928366869688, Function loss: 0.0034837457351386547\n",
      "Step: 1165, Loss: 0.00449993833899498\n",
      "Data loss: 0.001052839681506157, Function loss: 0.0034306426532566547\n",
      "Step: 1166, Loss: 0.004483482334762812\n",
      "Data loss: 0.001084907678887248, Function loss: 0.003524419851601124\n",
      "Step: 1167, Loss: 0.0046093277633190155\n",
      "Data loss: 0.0010256379609927535, Function loss: 0.0037121952045708895\n",
      "Step: 1168, Loss: 0.004737833049148321\n",
      "Data loss: 0.0011126591125503182, Function loss: 0.0036408575251698494\n",
      "Step: 1169, Loss: 0.004753516521304846\n",
      "Data loss: 0.0010245464509353042, Function loss: 0.0036222345661371946\n",
      "Step: 1170, Loss: 0.004646780900657177\n",
      "Data loss: 0.0010575157357379794, Function loss: 0.003452638164162636\n",
      "Step: 1171, Loss: 0.004510154016315937\n",
      "Data loss: 0.0010488013504073024, Function loss: 0.00338756968267262\n",
      "Step: 1172, Loss: 0.0044363709166646\n",
      "Data loss: 0.0010003822389990091, Function loss: 0.0034504500217735767\n",
      "Step: 1173, Loss: 0.0044508324936032295\n",
      "Data loss: 0.0010862495983019471, Function loss: 0.003421241417527199\n",
      "Step: 1174, Loss: 0.004507490899413824\n",
      "Data loss: 0.0009830525377765298, Function loss: 0.0035605730954557657\n",
      "Step: 1175, Loss: 0.004543625749647617\n",
      "Data loss: 0.0010839595925062895, Function loss: 0.0034399935975670815\n",
      "Step: 1176, Loss: 0.004523953422904015\n",
      "Data loss: 0.0009926401544362307, Function loss: 0.0034661898389458656\n",
      "Step: 1177, Loss: 0.004458829760551453\n",
      "Data loss: 0.0010412749834358692, Function loss: 0.003359026974067092\n",
      "Step: 1178, Loss: 0.0044003017246723175\n",
      "Data loss: 0.00101941276807338, Function loss: 0.003356252796947956\n",
      "Step: 1179, Loss: 0.004375665448606014\n",
      "Data loss: 0.001000892836600542, Function loss: 0.003387897275388241\n",
      "Step: 1180, Loss: 0.004388790111988783\n",
      "Data loss: 0.0010470951674506068, Function loss: 0.0033701618667691946\n",
      "Step: 1181, Loss: 0.00441725691780448\n",
      "Data loss: 0.0009812219068408012, Function loss: 0.0034559646155685186\n",
      "Step: 1182, Loss: 0.0044371867552399635\n",
      "Data loss: 0.0010485962266102433, Function loss: 0.0033717777114361525\n",
      "Step: 1183, Loss: 0.004420374054461718\n",
      "Data loss: 0.000980429002083838, Function loss: 0.0034107633400708437\n",
      "Step: 1184, Loss: 0.0043911924585700035\n",
      "Data loss: 0.00102413899730891, Function loss: 0.0033356104977428913\n",
      "Step: 1185, Loss: 0.004359749611467123\n",
      "Data loss: 0.0009922001045197248, Function loss: 0.003353381995111704\n",
      "Step: 1186, Loss: 0.004345581866800785\n",
      "Data loss: 0.0010012974962592125, Function loss: 0.00334666739217937\n",
      "Step: 1187, Loss: 0.004347965121269226\n",
      "Data loss: 0.0010045366361737251, Function loss: 0.0033635334111750126\n",
      "Step: 1188, Loss: 0.004368070047348738\n",
      "Data loss: 0.0009924662299454212, Function loss: 0.003386091673746705\n",
      "Step: 1189, Loss: 0.00437855813652277\n",
      "Data loss: 0.001000469783321023, Function loss: 0.003379696747288108\n",
      "Step: 1190, Loss: 0.004380166530609131\n",
      "Data loss: 0.000994309550151229, Function loss: 0.0033757868222892284\n",
      "Step: 1191, Loss: 0.004370096139609814\n",
      "Data loss: 0.0009796563535928726, Function loss: 0.0033771353773772717\n",
      "Step: 1192, Loss: 0.004356791730970144\n",
      "Data loss: 0.0010079271160066128, Function loss: 0.003344431519508362\n",
      "Step: 1193, Loss: 0.004352358635514975\n",
      "Data loss: 0.0009501402382738888, Function loss: 0.003407785203307867\n",
      "Step: 1194, Loss: 0.004357925616204739\n",
      "Data loss: 0.0010285841999575496, Function loss: 0.0033573354594409466\n",
      "Step: 1195, Loss: 0.004385919775813818\n",
      "Data loss: 0.0009216281468980014, Function loss: 0.0035023700911551714\n",
      "Step: 1196, Loss: 0.004423998296260834\n",
      "Data loss: 0.001050340011715889, Function loss: 0.003426475217565894\n",
      "Step: 1197, Loss: 0.004476815462112427\n",
      "Data loss: 0.0008978981641121209, Function loss: 0.003642306663095951\n",
      "Step: 1198, Loss: 0.004540205001831055\n",
      "Data loss: 0.0010672260541468859, Function loss: 0.003549498040229082\n",
      "Step: 1199, Loss: 0.004616724327206612\n",
      "Data loss: 0.0008830853039398789, Function loss: 0.003819888923317194\n",
      "Step: 1200, Loss: 0.004702974110841751\n",
      "Data loss: 0.0010801935568451881, Function loss: 0.00376329617574811\n",
      "Step: 1201, Loss: 0.004843489732593298\n",
      "Data loss: 0.0008766422397457063, Function loss: 0.004177008289843798\n",
      "Step: 1202, Loss: 0.005053650587797165\n",
      "Data loss: 0.0011178840650245547, Function loss: 0.0042882151901721954\n",
      "Step: 1203, Loss: 0.005406099371612072\n",
      "Data loss: 0.0008646623464301229, Function loss: 0.005016031675040722\n",
      "Step: 1204, Loss: 0.005880693905055523\n",
      "Data loss: 0.0012022966984659433, Function loss: 0.005454492289572954\n",
      "Step: 1205, Loss: 0.006656789220869541\n",
      "Data loss: 0.0008450709865428507, Function loss: 0.006806116551160812\n",
      "Step: 1206, Loss: 0.007651187479496002\n",
      "Data loss: 0.0013600870734080672, Function loss: 0.008010724559426308\n",
      "Step: 1207, Loss: 0.00937081128358841\n",
      "Data loss: 0.0008251620456576347, Function loss: 0.010692887008190155\n",
      "Step: 1208, Loss: 0.01151804905384779\n",
      "Data loss: 0.0016366312047466636, Function loss: 0.013544294051826\n",
      "Step: 1209, Loss: 0.015180924907326698\n",
      "Data loss: 0.0008361989166587591, Function loss: 0.017380602657794952\n",
      "Step: 1210, Loss: 0.018216801807284355\n",
      "Data loss: 0.0019449128303676844, Function loss: 0.02073129266500473\n",
      "Step: 1211, Loss: 0.02267620526254177\n",
      "Data loss: 0.0008710063411854208, Function loss: 0.021802131086587906\n",
      "Step: 1212, Loss: 0.022673137485980988\n",
      "Data loss: 0.0019134636968374252, Function loss: 0.0197715163230896\n",
      "Step: 1213, Loss: 0.021684980019927025\n",
      "Data loss: 0.0008557735709473491, Function loss: 0.013697338290512562\n",
      "Step: 1214, Loss: 0.014553111977875233\n",
      "Data loss: 0.0013275815872475505, Function loss: 0.006616277154535055\n",
      "Step: 1215, Loss: 0.00794385839253664\n",
      "Data loss: 0.0010049649281427264, Function loss: 0.0033804383128881454\n",
      "Step: 1216, Loss: 0.004385403357446194\n",
      "Data loss: 0.0009193370351567864, Function loss: 0.004898342303931713\n",
      "Step: 1217, Loss: 0.005817679222673178\n",
      "Data loss: 0.001463657128624618, Function loss: 0.008144316263496876\n",
      "Step: 1218, Loss: 0.009607973508536816\n",
      "Data loss: 0.0009256675257347524, Function loss: 0.009981293231248856\n",
      "Step: 1219, Loss: 0.010906960815191269\n",
      "Data loss: 0.001461758860386908, Function loss: 0.0075589935295283794\n",
      "Step: 1220, Loss: 0.009020752273499966\n",
      "Data loss: 0.0010245244484394789, Function loss: 0.004455756861716509\n",
      "Step: 1221, Loss: 0.005480281077325344\n",
      "Data loss: 0.0010640639811754227, Function loss: 0.0034212083555758\n",
      "Step: 1222, Loss: 0.004485272336751223\n",
      "Data loss: 0.0013375314883887768, Function loss: 0.004919006954878569\n",
      "Step: 1223, Loss: 0.006256538443267345\n",
      "Data loss: 0.0009698799694888294, Function loss: 0.006794634275138378\n",
      "Step: 1224, Loss: 0.007764514070004225\n",
      "Data loss: 0.001410215045325458, Function loss: 0.005881232675164938\n",
      "Step: 1225, Loss: 0.007291447836905718\n",
      "Data loss: 0.0010567435529083014, Function loss: 0.004252041224390268\n",
      "Step: 1226, Loss: 0.005308784544467926\n",
      "Data loss: 0.0011214356636628509, Function loss: 0.0032607403118163347\n",
      "Step: 1227, Loss: 0.004382175859063864\n",
      "Data loss: 0.0012822659919038415, Function loss: 0.003891156753525138\n",
      "Step: 1228, Loss: 0.005173422861844301\n",
      "Data loss: 0.0009995083091780543, Function loss: 0.0052118487656116486\n",
      "Step: 1229, Loss: 0.006211357191205025\n",
      "Data loss: 0.0013466911623254418, Function loss: 0.004819673020392656\n",
      "Step: 1230, Loss: 0.006166364066302776\n",
      "Data loss: 0.0010664036963135004, Function loss: 0.00401806877925992\n",
      "Step: 1231, Loss: 0.005084472708404064\n",
      "Data loss: 0.0011374210007488728, Function loss: 0.003229046007618308\n",
      "Step: 1232, Loss: 0.004366466775536537\n",
      "Data loss: 0.0012199764605611563, Function loss: 0.003359244903549552\n",
      "Step: 1233, Loss: 0.004579221364110708\n",
      "Data loss: 0.0010123496176674962, Function loss: 0.0041683451272547245\n",
      "Step: 1234, Loss: 0.005180694628506899\n",
      "Data loss: 0.0012813904322683811, Function loss: 0.004148825537413359\n",
      "Step: 1235, Loss: 0.00543021596968174\n",
      "Data loss: 0.0010449242545291781, Function loss: 0.003980888053774834\n",
      "Step: 1236, Loss: 0.005025812424719334\n",
      "Data loss: 0.0011517342645674944, Function loss: 0.00333865056745708\n",
      "Step: 1237, Loss: 0.004490384832024574\n",
      "Data loss: 0.0011552078649401665, Function loss: 0.0031647065188735723\n",
      "Step: 1238, Loss: 0.0043199146166443825\n",
      "Data loss: 0.0010143840918317437, Function loss: 0.003542830003425479\n",
      "Step: 1239, Loss: 0.0045572142116725445\n",
      "Data loss: 0.0012202489888295531, Function loss: 0.0035946350544691086\n",
      "Step: 1240, Loss: 0.0048148841597139835\n",
      "Data loss: 0.001008547144010663, Function loss: 0.003755704965442419\n",
      "Step: 1241, Loss: 0.004764252342283726\n",
      "Data loss: 0.0011484930291771889, Function loss: 0.003342163050547242\n",
      "Step: 1242, Loss: 0.004490655846893787\n",
      "Data loss: 0.0010883123613893986, Function loss: 0.003163800109177828\n",
      "Step: 1243, Loss: 0.004252112470567226\n",
      "Data loss: 0.0010286024771630764, Function loss: 0.003219182603061199\n",
      "Step: 1244, Loss: 0.004247785080224276\n",
      "Data loss: 0.0011609828798100352, Function loss: 0.003240263322368264\n",
      "Step: 1245, Loss: 0.004401246085762978\n",
      "Data loss: 0.0009820222621783614, Function loss: 0.0035254363901913166\n",
      "Step: 1246, Loss: 0.004507458768785\n",
      "Data loss: 0.0011475150240585208, Function loss: 0.0033292160369455814\n",
      "Step: 1247, Loss: 0.004476731177419424\n",
      "Data loss: 0.0010207159211859107, Function loss: 0.003320115152746439\n",
      "Step: 1248, Loss: 0.0043408311903476715\n",
      "Data loss: 0.0010668063769116998, Function loss: 0.0031505718361586332\n",
      "Step: 1249, Loss: 0.004217378329485655\n",
      "Data loss: 0.0010846720542758703, Function loss: 0.003089310135692358\n",
      "Step: 1250, Loss: 0.004173981957137585\n",
      "Data loss: 0.0009954352863132954, Function loss: 0.003202989464625716\n",
      "Step: 1251, Loss: 0.004198424518108368\n",
      "Data loss: 0.001113717327825725, Function loss: 0.003135220380499959\n",
      "Step: 1252, Loss: 0.004248937591910362\n",
      "Data loss: 0.0009705422562547028, Function loss: 0.003323119366541505\n",
      "Step: 1253, Loss: 0.004293661564588547\n",
      "Data loss: 0.0011028286535292864, Function loss: 0.0032204187009483576\n",
      "Step: 1254, Loss: 0.004323247354477644\n",
      "Data loss: 0.0009865222964435816, Function loss: 0.003340479452162981\n",
      "Step: 1255, Loss: 0.004327001981437206\n",
      "Data loss: 0.0010701405117288232, Function loss: 0.0032355785369873047\n",
      "Step: 1256, Loss: 0.004305718932300806\n",
      "Data loss: 0.0010105222463607788, Function loss: 0.0032355564180761576\n",
      "Step: 1257, Loss: 0.004246078431606293\n",
      "Data loss: 0.0010272267973050475, Function loss: 0.003137902356684208\n",
      "Step: 1258, Loss: 0.004165129270404577\n",
      "Data loss: 0.0010196237126365304, Function loss: 0.00306686176918447\n",
      "Step: 1259, Loss: 0.004086485598236322\n",
      "Data loss: 0.0009944000048562884, Function loss: 0.0030546090565621853\n",
      "Step: 1260, Loss: 0.0040490091778337955\n",
      "Data loss: 0.0010171800386160612, Function loss: 0.003033677116036415\n",
      "Step: 1261, Loss: 0.00405085738748312\n",
      "Data loss: 0.0009854707168415189, Function loss: 0.003095822175964713\n",
      "Step: 1262, Loss: 0.004081293009221554\n",
      "Data loss: 0.0010131564922630787, Function loss: 0.003099540714174509\n",
      "Step: 1263, Loss: 0.004112697206437588\n",
      "Data loss: 0.00098281295504421, Function loss: 0.003135588951408863\n",
      "Step: 1264, Loss: 0.004118402022868395\n",
      "Data loss: 0.0009992835111916065, Function loss: 0.0030941790901124477\n",
      "Step: 1265, Loss: 0.004093462601304054\n",
      "Data loss: 0.0009783842833712697, Function loss: 0.0030748951248824596\n",
      "Step: 1266, Loss: 0.0040532792918384075\n",
      "Data loss: 0.0009833358926698565, Function loss: 0.003040254581719637\n",
      "Step: 1267, Loss: 0.004023590590804815\n",
      "Data loss: 0.0009696913184598088, Function loss: 0.0030354869086295366\n",
      "Step: 1268, Loss: 0.004005178343504667\n",
      "Data loss: 0.0009757287334650755, Function loss: 0.0030213347636163235\n",
      "Step: 1269, Loss: 0.003997063264250755\n",
      "Data loss: 0.0009602244826965034, Function loss: 0.003033653600141406\n",
      "Step: 1270, Loss: 0.00399387814104557\n",
      "Data loss: 0.0009705201955512166, Function loss: 0.003026876598596573\n",
      "Step: 1271, Loss: 0.003997396677732468\n",
      "Data loss: 0.000952125177718699, Function loss: 0.003043859265744686\n",
      "Step: 1272, Loss: 0.003995984327048063\n",
      "Data loss: 0.0009636606555432081, Function loss: 0.00302967824973166\n",
      "Step: 1273, Loss: 0.003993338905274868\n",
      "Data loss: 0.0009447875781916082, Function loss: 0.0030428571626544\n",
      "Step: 1274, Loss: 0.003987644799053669\n",
      "Data loss: 0.0009530523093417287, Function loss: 0.003028149250894785\n",
      "Step: 1275, Loss: 0.003981201443821192\n",
      "Data loss: 0.0009419012349098921, Function loss: 0.0030335169285535812\n",
      "Step: 1276, Loss: 0.003975418396294117\n",
      "Data loss: 0.0009365694131702185, Function loss: 0.0030277129262685776\n",
      "Step: 1277, Loss: 0.00396428257226944\n",
      "Data loss: 0.0009438311099074781, Function loss: 0.0030147891957312822\n",
      "Step: 1278, Loss: 0.003958620131015778\n",
      "Data loss: 0.0009190773707814515, Function loss: 0.003033275483176112\n",
      "Step: 1279, Loss: 0.003952352795749903\n",
      "Data loss: 0.0009437442640773952, Function loss: 0.003006104612722993\n",
      "Step: 1280, Loss: 0.003949848935008049\n",
      "Data loss: 0.0009082615142688155, Function loss: 0.003038280177861452\n",
      "Step: 1281, Loss: 0.0039465418085455894\n",
      "Data loss: 0.0009384119766764343, Function loss: 0.003005577716976404\n",
      "Step: 1282, Loss: 0.003943989519029856\n",
      "Data loss: 0.0009035408147610724, Function loss: 0.003030293621122837\n",
      "Step: 1283, Loss: 0.0039338343776762486\n",
      "Data loss: 0.0009230207651853561, Function loss: 0.0030011136550456285\n",
      "Step: 1284, Loss: 0.003924134187400341\n",
      "Data loss: 0.000907727750018239, Function loss: 0.003013244364410639\n",
      "Step: 1285, Loss: 0.0039209723472595215\n",
      "Data loss: 0.0009051535162143409, Function loss: 0.0030107367783784866\n",
      "Step: 1286, Loss: 0.003915890119969845\n",
      "Data loss: 0.0009143077186308801, Function loss: 0.0030016021337360144\n",
      "Step: 1287, Loss: 0.003915909677743912\n",
      "Data loss: 0.0008906422299332917, Function loss: 0.0030277252662926912\n",
      "Step: 1288, Loss: 0.003918367438018322\n",
      "Data loss: 0.0009160168701782823, Function loss: 0.0030014077201485634\n",
      "Step: 1289, Loss: 0.003917424473911524\n",
      "Data loss: 0.0008810338331386447, Function loss: 0.003033553482964635\n",
      "Step: 1290, Loss: 0.003914587199687958\n",
      "Data loss: 0.0009139618487097323, Function loss: 0.002997345756739378\n",
      "Step: 1291, Loss: 0.003911307547241449\n",
      "Data loss: 0.0008737167809158564, Function loss: 0.003030327847227454\n",
      "Step: 1292, Loss: 0.0039040446281433105\n",
      "Data loss: 0.000907649053260684, Function loss: 0.002988453721627593\n",
      "Step: 1293, Loss: 0.003896102774888277\n",
      "Data loss: 0.0008717437740415335, Function loss: 0.003016873961314559\n",
      "Step: 1294, Loss: 0.0038886177353560925\n",
      "Data loss: 0.000894112978130579, Function loss: 0.002986552892252803\n",
      "Step: 1295, Loss: 0.003880665870383382\n",
      "Data loss: 0.0008767877006903291, Function loss: 0.002997869160026312\n",
      "Step: 1296, Loss: 0.003874656744301319\n",
      "Data loss: 0.0008775562164373696, Function loss: 0.002993960864841938\n",
      "Step: 1297, Loss: 0.0038715170230716467\n",
      "Data loss: 0.0008837152854539454, Function loss: 0.0029841954819858074\n",
      "Step: 1298, Loss: 0.003867910709232092\n",
      "Data loss: 0.000862853426951915, Function loss: 0.0030007183086127043\n",
      "Step: 1299, Loss: 0.0038635716773569584\n",
      "Data loss: 0.000885391200426966, Function loss: 0.0029752906411886215\n",
      "Step: 1300, Loss: 0.0038606817834079266\n",
      "Data loss: 0.0008552398649044335, Function loss: 0.0030022214632481337\n",
      "Step: 1301, Loss: 0.0038574612699449062\n",
      "Data loss: 0.0008818915230222046, Function loss: 0.002971144625917077\n",
      "Step: 1302, Loss: 0.003853036090731621\n",
      "Data loss: 0.0008492894121445715, Function loss: 0.0030020219273865223\n",
      "Step: 1303, Loss: 0.003851311281323433\n",
      "Data loss: 0.0008808363927528262, Function loss: 0.0029706726782023907\n",
      "Step: 1304, Loss: 0.0038515091873705387\n",
      "Data loss: 0.0008388231508433819, Function loss: 0.003019252559170127\n",
      "Step: 1305, Loss: 0.003858075710013509\n",
      "Data loss: 0.0008884599665179849, Function loss: 0.0029878998175263405\n",
      "Step: 1306, Loss: 0.0038763596676290035\n",
      "Data loss: 0.0008179359138011932, Function loss: 0.003111934754997492\n",
      "Step: 1307, Loss: 0.003929870668798685\n",
      "Data loss: 0.000915585202164948, Function loss: 0.003119578119367361\n",
      "Step: 1308, Loss: 0.004035163205116987\n",
      "Data loss: 0.0007862975471653044, Function loss: 0.0034284566063433886\n",
      "Step: 1309, Loss: 0.004214754328131676\n",
      "Data loss: 0.0009636908071115613, Function loss: 0.0035333659034222364\n",
      "Step: 1310, Loss: 0.00449705682694912\n",
      "Data loss: 0.0007549113943241537, Function loss: 0.004110726993530989\n",
      "Step: 1311, Loss: 0.004865638446062803\n",
      "Data loss: 0.001030117622576654, Function loss: 0.004378282930701971\n",
      "Step: 1312, Loss: 0.005408400669693947\n",
      "Data loss: 0.0007285394822247326, Function loss: 0.005371739622205496\n",
      "Step: 1313, Loss: 0.006100279279053211\n",
      "Data loss: 0.0011345843086019158, Function loss: 0.006097395904362202\n",
      "Step: 1314, Loss: 0.007231980096548796\n",
      "Data loss: 0.0007204440771602094, Function loss: 0.007919220253825188\n",
      "Step: 1315, Loss: 0.008639664389193058\n",
      "Data loss: 0.0013196364743635058, Function loss: 0.009797380305826664\n",
      "Step: 1316, Loss: 0.011117016896605492\n",
      "Data loss: 0.0007657877285964787, Function loss: 0.013223608955740929\n",
      "Step: 1317, Loss: 0.013989396393299103\n",
      "Data loss: 0.0016503749648109078, Function loss: 0.017355330288410187\n",
      "Step: 1318, Loss: 0.019005704671144485\n",
      "Data loss: 0.0009102034964598715, Function loss: 0.02159292809665203\n",
      "Step: 1319, Loss: 0.02250313200056553\n",
      "Data loss: 0.001973783364519477, Function loss: 0.024994483217597008\n",
      "Step: 1320, Loss: 0.02696826681494713\n",
      "Data loss: 0.0009934059344232082, Function loss: 0.022776039317250252\n",
      "Step: 1321, Loss: 0.023769445717334747\n",
      "Data loss: 0.0016672390047460794, Function loss: 0.01663801819086075\n",
      "Step: 1322, Loss: 0.018305256962776184\n",
      "Data loss: 0.0008529244223609567, Function loss: 0.00816414412111044\n",
      "Step: 1323, Loss: 0.009017068892717361\n",
      "Data loss: 0.000966372957918793, Function loss: 0.0031138251069933176\n",
      "Step: 1324, Loss: 0.004080198239535093\n",
      "Data loss: 0.0011166566982865334, Function loss: 0.00437341071665287\n",
      "Step: 1325, Loss: 0.0054900674149394035\n",
      "Data loss: 0.0008786491816863418, Function loss: 0.009166538715362549\n",
      "Step: 1326, Loss: 0.010045187547802925\n",
      "Data loss: 0.0015285039553418756, Function loss: 0.01125402096658945\n",
      "Step: 1327, Loss: 0.012782525271177292\n",
      "Data loss: 0.0009459108696319163, Function loss: 0.008706459775567055\n",
      "Step: 1328, Loss: 0.00965237058699131\n",
      "Data loss: 0.0011731402482837439, Function loss: 0.004209786653518677\n",
      "Step: 1329, Loss: 0.005382927134633064\n",
      "Data loss: 0.0010721851140260696, Function loss: 0.0029786904342472553\n",
      "Step: 1330, Loss: 0.004050875548273325\n",
      "Data loss: 0.0009261518134735525, Function loss: 0.005348747596144676\n",
      "Step: 1331, Loss: 0.00627489946782589\n",
      "Data loss: 0.0013893982395529747, Function loss: 0.007164568640291691\n",
      "Step: 1332, Loss: 0.008553966879844666\n",
      "Data loss: 0.0009511411772109568, Function loss: 0.006381260231137276\n",
      "Step: 1333, Loss: 0.007332401350140572\n",
      "Data loss: 0.00117880932521075, Function loss: 0.0036284499801695347\n",
      "Step: 1334, Loss: 0.004807259421795607\n",
      "Data loss: 0.0010867170058190823, Function loss: 0.0028552734293043613\n",
      "Step: 1335, Loss: 0.003941990435123444\n",
      "Data loss: 0.0009723392431624234, Function loss: 0.004313281737267971\n",
      "Step: 1336, Loss: 0.005285621155053377\n",
      "Data loss: 0.0013042055070400238, Function loss: 0.005258157383650541\n",
      "Step: 1337, Loss: 0.006562362890690565\n",
      "Data loss: 0.0009655327303335071, Function loss: 0.004793462343513966\n",
      "Step: 1338, Loss: 0.0057589951902627945\n",
      "Data loss: 0.0011411246377974749, Function loss: 0.003120939712971449\n",
      "Step: 1339, Loss: 0.004262064583599567\n",
      "Data loss: 0.0010871319100260735, Function loss: 0.002862386405467987\n",
      "Step: 1340, Loss: 0.0039495183154940605\n",
      "Data loss: 0.0009880615398287773, Function loss: 0.0038648273330181837\n",
      "Step: 1341, Loss: 0.004852889105677605\n",
      "Data loss: 0.0012226366670802236, Function loss: 0.004197216127067804\n",
      "Step: 1342, Loss: 0.005419852677732706\n",
      "Data loss: 0.0009611191344447434, Function loss: 0.0038329046219587326\n",
      "Step: 1343, Loss: 0.004794023931026459\n",
      "Data loss: 0.0011028890730813146, Function loss: 0.002892590593546629\n",
      "Step: 1344, Loss: 0.003995479550212622\n",
      "Data loss: 0.0010518149938434362, Function loss: 0.002901926636695862\n",
      "Step: 1345, Loss: 0.003953741863369942\n",
      "Data loss: 0.0010038494365289807, Function loss: 0.003437831997871399\n",
      "Step: 1346, Loss: 0.004441681317985058\n",
      "Data loss: 0.0011260696919634938, Function loss: 0.0034885602071881294\n",
      "Step: 1347, Loss: 0.004614629782736301\n",
      "Data loss: 0.0009726833086460829, Function loss: 0.003225137246772647\n",
      "Step: 1348, Loss: 0.00419782055541873\n",
      "Data loss: 0.0010361282620579004, Function loss: 0.0027585260104388\n",
      "Step: 1349, Loss: 0.0037946542724967003\n",
      "Data loss: 0.0010284122545272112, Function loss: 0.0028448954690247774\n",
      "Step: 1350, Loss: 0.0038733077235519886\n",
      "Data loss: 0.0009740504901856184, Function loss: 0.003259573131799698\n",
      "Step: 1351, Loss: 0.00423362385481596\n",
      "Data loss: 0.001075016101822257, Function loss: 0.003307825420051813\n",
      "Step: 1352, Loss: 0.0043828412890434265\n",
      "Data loss: 0.0009679914219304919, Function loss: 0.0031722248531877995\n",
      "Step: 1353, Loss: 0.004140216391533613\n",
      "Data loss: 0.000997650669887662, Function loss: 0.0028234568890184164\n",
      "Step: 1354, Loss: 0.0038211075589060783\n",
      "Data loss: 0.0010074101155623794, Function loss: 0.0027329137083142996\n",
      "Step: 1355, Loss: 0.003740323707461357\n",
      "Data loss: 0.0009243067470379174, Function loss: 0.002981069963425398\n",
      "Step: 1356, Loss: 0.0039053766522556543\n",
      "Data loss: 0.0010439574252814054, Function loss: 0.0030287865083664656\n",
      "Step: 1357, Loss: 0.004072743933647871\n",
      "Data loss: 0.0009238277561962605, Function loss: 0.003114319872111082\n",
      "Step: 1358, Loss: 0.0040381476283073425\n",
      "Data loss: 0.0009957192232832313, Function loss: 0.002871145959943533\n",
      "Step: 1359, Loss: 0.003866865299642086\n",
      "Data loss: 0.0009657064219936728, Function loss: 0.0027585402131080627\n",
      "Step: 1360, Loss: 0.0037242467515170574\n",
      "Data loss: 0.0009168137912638485, Function loss: 0.0028134072199463844\n",
      "Step: 1361, Loss: 0.003730220953002572\n",
      "Data loss: 0.0010102527448907495, Function loss: 0.002823174698278308\n",
      "Step: 1362, Loss: 0.003833427559584379\n",
      "Data loss: 0.0008789062267169356, Function loss: 0.0030412934720516205\n",
      "Step: 1363, Loss: 0.003920199815183878\n",
      "Data loss: 0.001011942746117711, Function loss: 0.002909630537033081\n",
      "Step: 1364, Loss: 0.0039215730503201485\n",
      "Data loss: 0.0008830766309984028, Function loss: 0.002934675198048353\n",
      "Step: 1365, Loss: 0.003817751770839095\n",
      "Data loss: 0.0009622022625990212, Function loss: 0.0027375321369618177\n",
      "Step: 1366, Loss: 0.003699734341353178\n",
      "Data loss: 0.0009183803922496736, Function loss: 0.002728249179199338\n",
      "Step: 1367, Loss: 0.0036466296296566725\n",
      "Data loss: 0.0009035553666763008, Function loss: 0.002768845297396183\n",
      "Step: 1368, Loss: 0.0036724007222801447\n",
      "Data loss: 0.0009571803966537118, Function loss: 0.0027877241373062134\n",
      "Step: 1369, Loss: 0.0037449044175446033\n",
      "Data loss: 0.0008652950637042522, Function loss: 0.002937162760645151\n",
      "Step: 1370, Loss: 0.0038024578243494034\n",
      "Data loss: 0.0009686144767329097, Function loss: 0.0028468144591897726\n",
      "Step: 1371, Loss: 0.0038154288195073605\n",
      "Data loss: 0.0008528138278052211, Function loss: 0.0029212874360382557\n",
      "Step: 1372, Loss: 0.0037741013802587986\n",
      "Data loss: 0.0009474879479967058, Function loss: 0.0027648773975670338\n",
      "Step: 1373, Loss: 0.0037123654037714005\n",
      "Data loss: 0.0008618718129582703, Function loss: 0.0027981367893517017\n",
      "Step: 1374, Loss: 0.003660008544102311\n",
      "Data loss: 0.0009199437336064875, Function loss: 0.002718539210036397\n",
      "Step: 1375, Loss: 0.0036384828854352236\n",
      "Data loss: 0.0008717463933862746, Function loss: 0.002772473730146885\n",
      "Step: 1376, Loss: 0.0036442200653254986\n",
      "Data loss: 0.0009014602983370423, Function loss: 0.0027548864018172026\n",
      "Step: 1377, Loss: 0.0036563468165695667\n",
      "Data loss: 0.0008762318757362664, Function loss: 0.002795008709654212\n",
      "Step: 1378, Loss: 0.0036712405271828175\n",
      "Data loss: 0.0008908963645808399, Function loss: 0.002775445580482483\n",
      "Step: 1379, Loss: 0.0036663420032709837\n",
      "Data loss: 0.0008654913399368525, Function loss: 0.0027794567868113518\n",
      "Step: 1380, Loss: 0.0036449481267482042\n",
      "Data loss: 0.000886509835254401, Function loss: 0.0027316571213304996\n",
      "Step: 1381, Loss: 0.0036181670147925615\n",
      "Data loss: 0.0008461533579975367, Function loss: 0.002753617474809289\n",
      "Step: 1382, Loss: 0.0035997708328068256\n",
      "Data loss: 0.0008854606421664357, Function loss: 0.0027077123522758484\n",
      "Step: 1383, Loss: 0.0035931728780269623\n",
      "Data loss: 0.0008318079635500908, Function loss: 0.0027625507209450006\n",
      "Step: 1384, Loss: 0.0035943586844950914\n",
      "Data loss: 0.0008790603606030345, Function loss: 0.0027139626909047365\n",
      "Step: 1385, Loss: 0.003593022935092449\n",
      "Data loss: 0.0008286575903184712, Function loss: 0.002755375113338232\n",
      "Step: 1386, Loss: 0.0035840326454490423\n",
      "Data loss: 0.000862418208271265, Function loss: 0.002709153341129422\n",
      "Step: 1387, Loss: 0.0035715715494006872\n",
      "Data loss: 0.000834411068353802, Function loss: 0.0027216363232582808\n",
      "Step: 1388, Loss: 0.003556047333404422\n",
      "Data loss: 0.0008398853242397308, Function loss: 0.0027059579733759165\n",
      "Step: 1389, Loss: 0.0035458432976156473\n",
      "Data loss: 0.0008435900672338903, Function loss: 0.0026996468659490347\n",
      "Step: 1390, Loss: 0.003543236991390586\n",
      "Data loss: 0.000817728869151324, Function loss: 0.0027282112278044224\n",
      "Step: 1391, Loss: 0.0035459401551634073\n",
      "Data loss: 0.0008516053785569966, Function loss: 0.002704167738556862\n",
      "Step: 1392, Loss: 0.0035557730589061975\n",
      "Data loss: 0.0008014653576537967, Function loss: 0.0027585013303905725\n",
      "Step: 1393, Loss: 0.0035599665716290474\n",
      "Data loss: 0.0008544084266759455, Function loss: 0.0027149647939950228\n",
      "Step: 1394, Loss: 0.0035693731624633074\n",
      "Data loss: 0.0007886630482971668, Function loss: 0.0027869234327226877\n",
      "Step: 1395, Loss: 0.0035755864810198545\n",
      "Data loss: 0.000857519858982414, Function loss: 0.0027301704976707697\n",
      "Step: 1396, Loss: 0.0035876904148608446\n",
      "Data loss: 0.000777147535700351, Function loss: 0.002816720400005579\n",
      "Step: 1397, Loss: 0.003593867877498269\n",
      "Data loss: 0.0008573619998060167, Function loss: 0.0027368192095309496\n",
      "Step: 1398, Loss: 0.003594181267544627\n",
      "Data loss: 0.0007727528573013842, Function loss: 0.0028127862606197596\n",
      "Step: 1399, Loss: 0.003585539059713483\n",
      "Data loss: 0.000851316552143544, Function loss: 0.0027316254563629627\n",
      "Step: 1400, Loss: 0.0035829420667141676\n",
      "Data loss: 0.000769298174418509, Function loss: 0.0028175755869597197\n",
      "Step: 1401, Loss: 0.0035868738777935505\n",
      "Data loss: 0.0008509843028150499, Function loss: 0.002743000630289316\n",
      "Step: 1402, Loss: 0.003593984991312027\n",
      "Data loss: 0.0007630382897332311, Function loss: 0.002854366088286042\n",
      "Step: 1403, Loss: 0.003617404494434595\n",
      "Data loss: 0.000859817024320364, Function loss: 0.0027946620248258114\n",
      "Step: 1404, Loss: 0.0036544790491461754\n",
      "Data loss: 0.0007493724697269499, Function loss: 0.0029817009344697\n",
      "Step: 1405, Loss: 0.003731073345988989\n",
      "Data loss: 0.000894456694368273, Function loss: 0.002978553995490074\n",
      "Step: 1406, Loss: 0.0038730106316506863\n",
      "Data loss: 0.0007302581798285246, Function loss: 0.0033813442569226027\n",
      "Step: 1407, Loss: 0.004111602436751127\n",
      "Data loss: 0.0009761821129359305, Function loss: 0.003536080475896597\n",
      "Step: 1408, Loss: 0.0045122625306248665\n",
      "Data loss: 0.0007252357318066061, Function loss: 0.004424970597028732\n",
      "Step: 1409, Loss: 0.005150206387042999\n",
      "Data loss: 0.0011431503808125854, Function loss: 0.005010626278817654\n",
      "Step: 1410, Loss: 0.006153776776045561\n",
      "Data loss: 0.0007854433497413993, Function loss: 0.006877180654555559\n",
      "Step: 1411, Loss: 0.00766262412071228\n",
      "Data loss: 0.001467397902160883, Function loss: 0.00838463194668293\n",
      "Step: 1412, Loss: 0.009852029383182526\n",
      "Data loss: 0.0009726207936182618, Function loss: 0.011465812101960182\n",
      "Step: 1413, Loss: 0.01243843324482441\n",
      "Data loss: 0.001860685646533966, Function loss: 0.013018792495131493\n",
      "Step: 1414, Loss: 0.014879478141665459\n",
      "Data loss: 0.001087950193323195, Function loss: 0.014471963979303837\n",
      "Step: 1415, Loss: 0.015559914521872997\n",
      "Data loss: 0.0017249598167836666, Function loss: 0.0118849603459239\n",
      "Step: 1416, Loss: 0.01360991969704628\n",
      "Data loss: 0.0007734021637588739, Function loss: 0.008419067598879337\n",
      "Step: 1417, Loss: 0.009192469529807568\n",
      "Data loss: 0.0010268515907227993, Function loss: 0.004398294258862734\n",
      "Step: 1418, Loss: 0.005425145849585533\n",
      "Data loss: 0.0008055779617279768, Function loss: 0.003664014395326376\n",
      "Step: 1419, Loss: 0.004469592124223709\n",
      "Data loss: 0.0009542256011627614, Function loss: 0.005186499562114477\n",
      "Step: 1420, Loss: 0.0061407252214848995\n",
      "Data loss: 0.0012897530104964972, Function loss: 0.006589504890143871\n",
      "Step: 1421, Loss: 0.007879258133471012\n",
      "Data loss: 0.0009823301807045937, Function loss: 0.0065285093151032925\n",
      "Step: 1422, Loss: 0.007510839495807886\n",
      "Data loss: 0.0010716619435697794, Function loss: 0.004263891372829676\n",
      "Step: 1423, Loss: 0.005335553549230099\n",
      "Data loss: 0.0007786183268763125, Function loss: 0.002903267042711377\n",
      "Step: 1424, Loss: 0.0036818853113800287\n",
      "Data loss: 0.0008620534790679812, Function loss: 0.0031496777664870024\n",
      "Step: 1425, Loss: 0.004011731129139662\n",
      "Data loss: 0.0010457535972818732, Function loss: 0.00443225959315896\n",
      "Step: 1426, Loss: 0.005478013306856155\n",
      "Data loss: 0.0009808688191697001, Function loss: 0.005183446686714888\n",
      "Step: 1427, Loss: 0.00616431562229991\n",
      "Data loss: 0.0010313962120562792, Function loss: 0.004223639145493507\n",
      "Step: 1428, Loss: 0.005255035124719143\n",
      "Data loss: 0.0008580444846302271, Function loss: 0.0030425211880356073\n",
      "Step: 1429, Loss: 0.0039005656726658344\n",
      "Data loss: 0.0008086166344583035, Function loss: 0.002698589814826846\n",
      "Step: 1430, Loss: 0.0035072064492851496\n",
      "Data loss: 0.0009537460864521563, Function loss: 0.003189139999449253\n",
      "Step: 1431, Loss: 0.0041428860276937485\n",
      "Data loss: 0.0008647110080346465, Function loss: 0.0038937435019761324\n",
      "Step: 1432, Loss: 0.004758454393595457\n",
      "Data loss: 0.0009834854863584042, Function loss: 0.003554563270881772\n",
      "Step: 1433, Loss: 0.00453804899007082\n",
      "Data loss: 0.0008450204622931778, Function loss: 0.0029624977614730597\n",
      "Step: 1434, Loss: 0.0038075181655585766\n",
      "Data loss: 0.0008082559797912836, Function loss: 0.0026707719080150127\n",
      "Step: 1435, Loss: 0.0034790278878062963\n",
      "Data loss: 0.000919931975658983, Function loss: 0.0028774498496204615\n",
      "Step: 1436, Loss: 0.0037973818834871054\n",
      "Data loss: 0.0008008408476598561, Function loss: 0.0034112015273422003\n",
      "Step: 1437, Loss: 0.0042120423167943954\n",
      "Data loss: 0.0009575109579600394, Function loss: 0.003209947142750025\n",
      "Step: 1438, Loss: 0.004167458042502403\n",
      "Data loss: 0.0008096873061731458, Function loss: 0.00291842152364552\n",
      "Step: 1439, Loss: 0.003728108946233988\n",
      "Data loss: 0.0008213176624849439, Function loss: 0.002596395555883646\n",
      "Step: 1440, Loss: 0.003417713101953268\n",
      "Data loss: 0.000877913145814091, Function loss: 0.0026775337755680084\n",
      "Step: 1441, Loss: 0.0035554468631744385\n",
      "Data loss: 0.0007704092422500253, Function loss: 0.0031235318165272474\n",
      "Step: 1442, Loss: 0.0038939411751925945\n",
      "Data loss: 0.0009388207690790296, Function loss: 0.0030478336848318577\n",
      "Step: 1443, Loss: 0.003986654337495565\n",
      "Data loss: 0.0007672402425669134, Function loss: 0.0029682947788387537\n",
      "Step: 1444, Loss: 0.003735535079613328\n",
      "Data loss: 0.0008518684189766645, Function loss: 0.0025943592190742493\n",
      "Step: 1445, Loss: 0.003446227638050914\n",
      "Data loss: 0.0008135783718898892, Function loss: 0.0025805295445024967\n",
      "Step: 1446, Loss: 0.003394107799977064\n",
      "Data loss: 0.000790066143963486, Function loss: 0.002764822682365775\n",
      "Step: 1447, Loss: 0.0035548887681216\n",
      "Data loss: 0.0008848593570291996, Function loss: 0.0028081072960048914\n",
      "Step: 1448, Loss: 0.003692966653034091\n",
      "Data loss: 0.0007668106118217111, Function loss: 0.002888372400775552\n",
      "Step: 1449, Loss: 0.0036551831290125847\n",
      "Data loss: 0.0008570966310799122, Function loss: 0.0026340631302446127\n",
      "Step: 1450, Loss: 0.003491159761324525\n",
      "Data loss: 0.0007730762008577585, Function loss: 0.002616120968014002\n",
      "Step: 1451, Loss: 0.0033891971688717604\n",
      "Data loss: 0.000809329329058528, Function loss: 0.002615015022456646\n",
      "Step: 1452, Loss: 0.003424344351515174\n",
      "Data loss: 0.000818907399661839, Function loss: 0.0027150746900588274\n",
      "Step: 1453, Loss: 0.0035339822061359882\n",
      "Data loss: 0.0007988683646544814, Function loss: 0.002809827448800206\n",
      "Step: 1454, Loss: 0.0036086956970393658\n",
      "Data loss: 0.0008220901363529265, Function loss: 0.0027504630852490664\n",
      "Step: 1455, Loss: 0.003572553163394332\n",
      "Data loss: 0.0007972182356752455, Function loss: 0.002665855921804905\n",
      "Step: 1456, Loss: 0.0034630740992724895\n",
      "Data loss: 0.0007764254696667194, Function loss: 0.0025984575040638447\n",
      "Step: 1457, Loss: 0.003374882973730564\n",
      "Data loss: 0.0008164385799318552, Function loss: 0.0025505640078336\n",
      "Step: 1458, Loss: 0.0033670025877654552\n",
      "Data loss: 0.0007421594928018749, Function loss: 0.002679335419088602\n",
      "Step: 1459, Loss: 0.003421494970098138\n",
      "Data loss: 0.0008418856887146831, Function loss: 0.0026335841976106167\n",
      "Step: 1460, Loss: 0.0034754700027406216\n",
      "Data loss: 0.0007281647413037717, Function loss: 0.0027566018979996443\n",
      "Step: 1461, Loss: 0.003484766697511077\n",
      "Data loss: 0.0008359673665836453, Function loss: 0.002633933676406741\n",
      "Step: 1462, Loss: 0.0034699011594057083\n",
      "Data loss: 0.0007269558846019208, Function loss: 0.0027502314187586308\n",
      "Step: 1463, Loss: 0.0034771873615682125\n",
      "Data loss: 0.0008279525791294873, Function loss: 0.002731087151914835\n",
      "Step: 1464, Loss: 0.0035590396728366613\n",
      "Data loss: 0.0007319090655073524, Function loss: 0.003003677586093545\n",
      "Step: 1465, Loss: 0.0037355865351855755\n",
      "Data loss: 0.0008599624270573258, Function loss: 0.0031778360716998577\n",
      "Step: 1466, Loss: 0.004037798382341862\n",
      "Data loss: 0.0007127446588128805, Function loss: 0.0037238753866404295\n",
      "Step: 1467, Loss: 0.00443662004545331\n",
      "Data loss: 0.0009347103768959641, Function loss: 0.004110645968466997\n",
      "Step: 1468, Loss: 0.0050453562289476395\n",
      "Data loss: 0.0006612927536480129, Function loss: 0.005243128631263971\n",
      "Step: 1469, Loss: 0.0059044212102890015\n",
      "Data loss: 0.0010663842549547553, Function loss: 0.006275733467191458\n",
      "Step: 1470, Loss: 0.007342117838561535\n",
      "Data loss: 0.0006257738568820059, Function loss: 0.008329763077199459\n",
      "Step: 1471, Loss: 0.008955537341535091\n",
      "Data loss: 0.0012435317039489746, Function loss: 0.010120125487446785\n",
      "Step: 1472, Loss: 0.01136365719139576\n",
      "Data loss: 0.0006433575763367116, Function loss: 0.012330175377428532\n",
      "Step: 1473, Loss: 0.012973533011972904\n",
      "Data loss: 0.0013841246254742146, Function loss: 0.013686356134712696\n",
      "Step: 1474, Loss: 0.015070481225848198\n",
      "Data loss: 0.0006971933180466294, Function loss: 0.01392451673746109\n",
      "Step: 1475, Loss: 0.014621710404753685\n",
      "Data loss: 0.001335582579486072, Function loss: 0.012358106672763824\n",
      "Step: 1476, Loss: 0.013693689368665218\n",
      "Data loss: 0.0007204050198197365, Function loss: 0.009376578964293003\n",
      "Step: 1477, Loss: 0.01009698398411274\n",
      "Data loss: 0.00103643664624542, Function loss: 0.005588124040514231\n",
      "Step: 1478, Loss: 0.0066245608031749725\n",
      "Data loss: 0.000755449291318655, Function loss: 0.0031956699676811695\n",
      "Step: 1479, Loss: 0.0039511192589998245\n",
      "Data loss: 0.0007685896707698703, Function loss: 0.002602664288133383\n",
      "Step: 1480, Loss: 0.003371254075318575\n",
      "Data loss: 0.0009473851532675326, Function loss: 0.0036636453587561846\n",
      "Step: 1481, Loss: 0.0046110306866467\n",
      "Data loss: 0.0007338671130128205, Function loss: 0.005571196787059307\n",
      "Step: 1482, Loss: 0.00630506407469511\n",
      "Data loss: 0.001103825867176056, Function loss: 0.006146453786641359\n",
      "Step: 1483, Loss: 0.007250279653817415\n",
      "Data loss: 0.0007517114863730967, Function loss: 0.0056830537505447865\n",
      "Step: 1484, Loss: 0.006434765178710222\n",
      "Data loss: 0.0009810455376282334, Function loss: 0.003882374381646514\n",
      "Step: 1485, Loss: 0.004863420035690069\n",
      "Data loss: 0.0007915761671029031, Function loss: 0.0027381577529013157\n",
      "Step: 1486, Loss: 0.0035297339782118797\n",
      "Data loss: 0.0008111495990306139, Function loss: 0.0025679299142211676\n",
      "Step: 1487, Loss: 0.0033790795132517815\n",
      "Data loss: 0.0009430040372535586, Function loss: 0.0032279323786497116\n",
      "Step: 1488, Loss: 0.004170936532318592\n",
      "Data loss: 0.0007687757606618106, Function loss: 0.00415507797151804\n",
      "Step: 1489, Loss: 0.004923853557556868\n",
      "Data loss: 0.0010015659499913454, Function loss: 0.00400863541290164\n",
      "Step: 1490, Loss: 0.005010201595723629\n",
      "Data loss: 0.0007736796396784484, Function loss: 0.0035281828604638577\n",
      "Step: 1491, Loss: 0.004301862325519323\n",
      "Data loss: 0.0008914898498915136, Function loss: 0.002646807814016938\n",
      "Step: 1492, Loss: 0.0035382977221161127\n",
      "Data loss: 0.0008384444518014789, Function loss: 0.0024303828831762075\n",
      "Step: 1493, Loss: 0.0032688272185623646\n",
      "Data loss: 0.0007952734595164657, Function loss: 0.002795769600197673\n",
      "Step: 1494, Loss: 0.0035910429432988167\n",
      "Data loss: 0.0009367898455820978, Function loss: 0.003190589603036642\n",
      "Step: 1495, Loss: 0.004127379506826401\n",
      "Data loss: 0.0007667439640499651, Function loss: 0.003571882611140609\n",
      "Step: 1496, Loss: 0.004338626749813557\n",
      "Data loss: 0.0009348684689030051, Function loss: 0.0031930389814078808\n",
      "Step: 1497, Loss: 0.004127907566726208\n",
      "Data loss: 0.0007757614366710186, Function loss: 0.0028815227560698986\n",
      "Step: 1498, Loss: 0.003657284192740917\n",
      "Data loss: 0.0008503007120452821, Function loss: 0.002453081076964736\n",
      "Step: 1499, Loss: 0.003303381847217679\n",
      "Data loss: 0.0008286707452498376, Function loss: 0.002411884954199195\n",
      "Step: 1500, Loss: 0.0032405557576566935\n",
      "Data loss: 0.0007831318071112037, Function loss: 0.002645693952217698\n",
      "Step: 1501, Loss: 0.00342882564291358\n",
      "Data loss: 0.0008853520848788321, Function loss: 0.002789860125631094\n",
      "Step: 1502, Loss: 0.003675212152302265\n",
      "Data loss: 0.0007588107837364078, Function loss: 0.003028564853593707\n",
      "Step: 1503, Loss: 0.003787375520914793\n",
      "Data loss: 0.0008867730502970517, Function loss: 0.0028574957977980375\n",
      "Step: 1504, Loss: 0.0037442687898874283\n",
      "Data loss: 0.0007621385157108307, Function loss: 0.00279855914413929\n",
      "Step: 1505, Loss: 0.0035606976598501205\n",
      "Data loss: 0.0008416867931373417, Function loss: 0.002521749585866928\n",
      "Step: 1506, Loss: 0.003363436320796609\n",
      "Data loss: 0.0007864668732509017, Function loss: 0.002436951268464327\n",
      "Step: 1507, Loss: 0.0032234182581305504\n",
      "Data loss: 0.000790795253124088, Function loss: 0.0023975602816790342\n",
      "Step: 1508, Loss: 0.003188355593010783\n",
      "Data loss: 0.0008206022321246564, Function loss: 0.002424896927550435\n",
      "Step: 1509, Loss: 0.0032454992178827524\n",
      "Data loss: 0.0007572020404040813, Function loss: 0.0025905140209943056\n",
      "Step: 1510, Loss: 0.003347716061398387\n",
      "Data loss: 0.000843751709908247, Function loss: 0.002600343432277441\n",
      "Step: 1511, Loss: 0.003444095142185688\n",
      "Data loss: 0.0007424447103403509, Function loss: 0.0027535324916243553\n",
      "Step: 1512, Loss: 0.003495977260172367\n",
      "Data loss: 0.0008456613286398351, Function loss: 0.0026604323647916317\n",
      "Step: 1513, Loss: 0.0035060937516391277\n",
      "Data loss: 0.0007388158701360226, Function loss: 0.0027309798169881105\n",
      "Step: 1514, Loss: 0.003469795687124133\n",
      "Data loss: 0.0008306143572553992, Function loss: 0.0025715685915201902\n",
      "Step: 1515, Loss: 0.0034021828323602676\n",
      "Data loss: 0.0007413603598251939, Function loss: 0.002566708019003272\n",
      "Step: 1516, Loss: 0.0033080684952437878\n",
      "Data loss: 0.0008030926692299545, Function loss: 0.0024199073668569326\n",
      "Step: 1517, Loss: 0.003223000094294548\n",
      "Data loss: 0.0007540900842286646, Function loss: 0.0024105042684823275\n",
      "Step: 1518, Loss: 0.003164594294503331\n",
      "Data loss: 0.0007712434744462371, Function loss: 0.002373629482463002\n",
      "Step: 1519, Loss: 0.0031448728404939175\n",
      "Data loss: 0.000774388900026679, Function loss: 0.002381758065894246\n",
      "Step: 1520, Loss: 0.003156146965920925\n",
      "Data loss: 0.0007428760291077197, Function loss: 0.0024439282715320587\n",
      "Step: 1521, Loss: 0.0031868042424321175\n",
      "Data loss: 0.000793516228441149, Function loss: 0.0024330916348844767\n",
      "Step: 1522, Loss: 0.0032266078051179647\n",
      "Data loss: 0.0007209807517938316, Function loss: 0.0025439471937716007\n",
      "Step: 1523, Loss: 0.0032649280037730932\n",
      "Data loss: 0.0008021719986572862, Function loss: 0.0024914301466196775\n",
      "Step: 1524, Loss: 0.0032936022616922855\n",
      "Data loss: 0.0007093888125382364, Function loss: 0.0025996998883783817\n",
      "Step: 1525, Loss: 0.003309088759124279\n",
      "Data loss: 0.0008002612739801407, Function loss: 0.0025149525608867407\n",
      "Step: 1526, Loss: 0.0033152138348668814\n",
      "Data loss: 0.0007072528824210167, Function loss: 0.0025867437943816185\n",
      "Step: 1527, Loss: 0.003293996676802635\n",
      "Data loss: 0.0007879273616708815, Function loss: 0.002482315292581916\n",
      "Step: 1528, Loss: 0.0032702425960451365\n",
      "Data loss: 0.0007104694959707558, Function loss: 0.0025228741578757763\n",
      "Step: 1529, Loss: 0.003233343595638871\n",
      "Data loss: 0.0007717019179835916, Function loss: 0.002422690624371171\n",
      "Step: 1530, Loss: 0.0031943926587700844\n",
      "Data loss: 0.0007150557939894497, Function loss: 0.002440245822072029\n",
      "Step: 1531, Loss: 0.003155301557853818\n",
      "Data loss: 0.0007548595895059407, Function loss: 0.0023698043078184128\n",
      "Step: 1532, Loss: 0.0031246638391166925\n",
      "Data loss: 0.0007202876731753349, Function loss: 0.0023862039670348167\n",
      "Step: 1533, Loss: 0.0031064916402101517\n",
      "Data loss: 0.0007390406099148095, Function loss: 0.0023569543845951557\n",
      "Step: 1534, Loss: 0.0030959949363023043\n",
      "Data loss: 0.0007267703185789287, Function loss: 0.0023643309250473976\n",
      "Step: 1535, Loss: 0.0030911013018339872\n",
      "Data loss: 0.0007212073542177677, Function loss: 0.00236808555200696\n",
      "Step: 1536, Loss: 0.0030892929062247276\n",
      "Data loss: 0.0007361564785242081, Function loss: 0.002361823571845889\n",
      "Step: 1537, Loss: 0.003097980050370097\n",
      "Data loss: 0.0007037612958811224, Function loss: 0.00240646512247622\n",
      "Step: 1538, Loss: 0.0031102264765650034\n",
      "Data loss: 0.0007453822763636708, Function loss: 0.0023854158353060484\n",
      "Step: 1539, Loss: 0.003130798228085041\n",
      "Data loss: 0.0006882313173264265, Function loss: 0.0024736712221056223\n",
      "Step: 1540, Loss: 0.003161902539432049\n",
      "Data loss: 0.0007569123990833759, Function loss: 0.002458873437717557\n",
      "Step: 1541, Loss: 0.003215785836800933\n",
      "Data loss: 0.0006708407308906317, Function loss: 0.002635020762681961\n",
      "Step: 1542, Loss: 0.0033058614935725927\n",
      "Data loss: 0.000778115470893681, Function loss: 0.002661199076101184\n",
      "Step: 1543, Loss: 0.0034393146634101868\n",
      "Data loss: 0.000652844668366015, Function loss: 0.0029648339841514826\n",
      "Step: 1544, Loss: 0.0036176787689328194\n",
      "Data loss: 0.0008111092611216009, Function loss: 0.0030732613522559404\n",
      "Step: 1545, Loss: 0.0038843706715852022\n",
      "Data loss: 0.0006357021047733724, Function loss: 0.0035733534023165703\n",
      "Step: 1546, Loss: 0.004209055565297604\n",
      "Data loss: 0.0008566745673306286, Function loss: 0.003782553132623434\n",
      "Step: 1547, Loss: 0.0046392278745770454\n",
      "Data loss: 0.0006236551562324166, Function loss: 0.004463168792426586\n",
      "Step: 1548, Loss: 0.005086823832243681\n",
      "Data loss: 0.0009133663843385875, Function loss: 0.004819931928068399\n",
      "Step: 1549, Loss: 0.005733298137784004\n",
      "Data loss: 0.0006178705953061581, Function loss: 0.005732440389692783\n",
      "Step: 1550, Loss: 0.006350310984998941\n",
      "Data loss: 0.000986240222118795, Function loss: 0.006289813667535782\n",
      "Step: 1551, Loss: 0.007276054006069899\n",
      "Data loss: 0.0006178462645038962, Function loss: 0.00735876290127635\n",
      "Step: 1552, Loss: 0.007976609282195568\n",
      "Data loss: 0.0010589727899059653, Function loss: 0.00788821280002594\n",
      "Step: 1553, Loss: 0.00894718524068594\n",
      "Data loss: 0.0006216761539690197, Function loss: 0.00856113899499178\n",
      "Step: 1554, Loss: 0.009182815439999104\n",
      "Data loss: 0.0010831387480720878, Function loss: 0.008418150246143341\n",
      "Step: 1555, Loss: 0.009501288644969463\n",
      "Data loss: 0.000623032043222338, Function loss: 0.008118215948343277\n",
      "Step: 1556, Loss: 0.008741248399019241\n",
      "Data loss: 0.0010187976295128465, Function loss: 0.0068633900955319405\n",
      "Step: 1557, Loss: 0.007882188074290752\n",
      "Data loss: 0.0006282153190113604, Function loss: 0.005667402874678373\n",
      "Step: 1558, Loss: 0.006295618135482073\n",
      "Data loss: 0.0008788498234935105, Function loss: 0.0039917356334626675\n",
      "Step: 1559, Loss: 0.004870585631579161\n",
      "Data loss: 0.0006663736421614885, Function loss: 0.0029832059517502785\n",
      "Step: 1560, Loss: 0.003649579593911767\n",
      "Data loss: 0.0007387915975414217, Function loss: 0.002325112000107765\n",
      "Step: 1561, Loss: 0.0030639036558568478\n",
      "Data loss: 0.0007683807634748518, Function loss: 0.002437219023704529\n",
      "Step: 1562, Loss: 0.0032055997289717197\n",
      "Data loss: 0.0006686009583063424, Function loss: 0.003127787495031953\n",
      "Step: 1563, Loss: 0.0037963883951306343\n",
      "Data loss: 0.000868568429723382, Function loss: 0.003561674617230892\n",
      "Step: 1564, Loss: 0.0044302428141236305\n",
      "Data loss: 0.0006563756032846868, Function loss: 0.00408514030277729\n",
      "Step: 1565, Loss: 0.004741515964269638\n",
      "Data loss: 0.0008893996709957719, Function loss: 0.0038464167155325413\n",
      "Step: 1566, Loss: 0.004735816270112991\n",
      "Data loss: 0.0006664058309979737, Function loss: 0.0036382300313562155\n",
      "Step: 1567, Loss: 0.004304635804146528\n",
      "Data loss: 0.0008276430889964104, Function loss: 0.0029373839497566223\n",
      "Step: 1568, Loss: 0.0037650270387530327\n",
      "Data loss: 0.0007025954546406865, Function loss: 0.002564147114753723\n",
      "Step: 1569, Loss: 0.003266742452979088\n",
      "Data loss: 0.000742768810596317, Function loss: 0.0022898383904248476\n",
      "Step: 1570, Loss: 0.0030326072592288256\n",
      "Data loss: 0.000762792828027159, Function loss: 0.0023143375292420387\n",
      "Step: 1571, Loss: 0.003077130299061537\n",
      "Data loss: 0.0006918205181136727, Function loss: 0.002605773275718093\n",
      "Step: 1572, Loss: 0.0032975939102470875\n",
      "Data loss: 0.0008122613071464002, Function loss: 0.0027359994128346443\n",
      "Step: 1573, Loss: 0.0035482607781887054\n",
      "Data loss: 0.0006769133033230901, Function loss: 0.0030024689622223377\n",
      "Step: 1574, Loss: 0.003679382149130106\n",
      "Data loss: 0.000816327054053545, Function loss: 0.002847219118848443\n",
      "Step: 1575, Loss: 0.003663546172901988\n",
      "Data loss: 0.0006833606166765094, Function loss: 0.0028132700826972723\n",
      "Step: 1576, Loss: 0.00349663058295846\n",
      "Data loss: 0.0007819934980943799, Function loss: 0.0025175889022648335\n",
      "Step: 1577, Loss: 0.0032995822839438915\n",
      "Data loss: 0.0006994571886025369, Function loss: 0.0024203364737331867\n",
      "Step: 1578, Loss: 0.0031197937205433846\n",
      "Data loss: 0.0007402881165035069, Function loss: 0.002267224946990609\n",
      "Step: 1579, Loss: 0.003007513005286455\n",
      "Data loss: 0.0007235876400955021, Function loss: 0.0022579613141715527\n",
      "Step: 1580, Loss: 0.002981548896059394\n",
      "Data loss: 0.00071029580431059, Function loss: 0.0023053898476064205\n",
      "Step: 1581, Loss: 0.0030156855937093496\n",
      "Data loss: 0.0007453618454746902, Function loss: 0.002333000535145402\n",
      "Step: 1582, Loss: 0.003078362438827753\n",
      "Data loss: 0.000693708541803062, Function loss: 0.0024360993411391973\n",
      "Step: 1583, Loss: 0.0031298077665269375\n",
      "Data loss: 0.0007565601845271885, Function loss: 0.002416795352473855\n",
      "Step: 1584, Loss: 0.0031733554787933826\n",
      "Data loss: 0.0006812203791923821, Function loss: 0.002522651804611087\n",
      "Step: 1585, Loss: 0.003203872125595808\n",
      "Data loss: 0.0007631385233253241, Function loss: 0.0024680981878191233\n",
      "Step: 1586, Loss: 0.0032312367111444473\n",
      "Data loss: 0.0006723484839312732, Function loss: 0.0025809835642576218\n",
      "Step: 1587, Loss: 0.003253332106396556\n",
      "Data loss: 0.0007653103675693274, Function loss: 0.002499652560800314\n",
      "Step: 1588, Loss: 0.0032649629283696413\n",
      "Data loss: 0.0006657555932179093, Function loss: 0.0025955243036150932\n",
      "Step: 1589, Loss: 0.0032612797804176807\n",
      "Data loss: 0.0007649837643839419, Function loss: 0.00249919923953712\n",
      "Step: 1590, Loss: 0.003264182945713401\n",
      "Data loss: 0.0006564095965586603, Function loss: 0.002618536353111267\n",
      "Step: 1591, Loss: 0.0032749460078775883\n",
      "Data loss: 0.0007703740266151726, Function loss: 0.0025328113697469234\n",
      "Step: 1592, Loss: 0.003303185338154435\n",
      "Data loss: 0.0006435951218008995, Function loss: 0.0027022946160286665\n",
      "Step: 1593, Loss: 0.003345889737829566\n",
      "Data loss: 0.0007804805063642561, Function loss: 0.0026289455126971006\n",
      "Step: 1594, Loss: 0.003409425960853696\n",
      "Data loss: 0.0006319363601505756, Function loss: 0.00284390477463603\n",
      "Step: 1595, Loss: 0.003475841134786606\n",
      "Data loss: 0.0007904422236606479, Function loss: 0.002773628104478121\n",
      "Step: 1596, Loss: 0.0035640704445540905\n",
      "Data loss: 0.0006246700068004429, Function loss: 0.00302046537399292\n",
      "Step: 1597, Loss: 0.003645135322585702\n",
      "Data loss: 0.0007985932170413435, Function loss: 0.002948622452095151\n",
      "Step: 1598, Loss: 0.0037472157273441553\n",
      "Data loss: 0.0006219607894308865, Function loss: 0.003178930375725031\n",
      "Step: 1599, Loss: 0.0038008911069482565\n",
      "Data loss: 0.0008007248397916555, Function loss: 0.0030575350392609835\n",
      "Step: 1600, Loss: 0.003858259879052639\n",
      "Data loss: 0.0006214714958332479, Function loss: 0.0032376947347074747\n",
      "Step: 1601, Loss: 0.0038591662887483835\n",
      "Data loss: 0.0007973333122208714, Function loss: 0.003065930912271142\n",
      "Step: 1602, Loss: 0.0038632643409073353\n",
      "Data loss: 0.0006210578721947968, Function loss: 0.0031807762570679188\n",
      "Step: 1603, Loss: 0.0038018340710550547\n",
      "Data loss: 0.0007882168865762651, Function loss: 0.0029609003104269505\n",
      "Step: 1604, Loss: 0.0037491172552108765\n",
      "Data loss: 0.000618951627984643, Function loss: 0.0030458979308605194\n",
      "Step: 1605, Loss: 0.0036648495588451624\n",
      "Data loss: 0.0007777250139042735, Function loss: 0.0028146281838417053\n",
      "Step: 1606, Loss: 0.0035923533141613007\n",
      "Data loss: 0.0006172261200845242, Function loss: 0.002876556245610118\n",
      "Step: 1607, Loss: 0.003493782365694642\n",
      "Data loss: 0.0007637252565473318, Function loss: 0.0026370242703706026\n",
      "Step: 1608, Loss: 0.0034007495269179344\n",
      "Data loss: 0.0006222744705155492, Function loss: 0.002662807237356901\n",
      "Step: 1609, Loss: 0.0032850815914571285\n",
      "Data loss: 0.0007421890622936189, Function loss: 0.002435817616060376\n",
      "Step: 1610, Loss: 0.003178006736561656\n",
      "Data loss: 0.000633670249953866, Function loss: 0.0024463613517582417\n",
      "Step: 1611, Loss: 0.0030800316017121077\n",
      "Data loss: 0.0007161095272749662, Function loss: 0.002278559608384967\n",
      "Step: 1612, Loss: 0.002994669135659933\n",
      "Data loss: 0.0006510644452646375, Function loss: 0.0022830970119684935\n",
      "Step: 1613, Loss: 0.0029341615736484528\n",
      "Data loss: 0.0006883516325615346, Function loss: 0.002205410273745656\n",
      "Step: 1614, Loss: 0.0028937619645148516\n",
      "Data loss: 0.000670348817948252, Function loss: 0.002204607240855694\n",
      "Step: 1615, Loss: 0.002874956000596285\n",
      "Data loss: 0.0006651896401308477, Function loss: 0.0022095167078077793\n",
      "Step: 1616, Loss: 0.002874706406146288\n",
      "Data loss: 0.0006856654654257, Function loss: 0.00219648121856153\n",
      "Step: 1617, Loss: 0.002882146742194891\n",
      "Data loss: 0.00065011897822842, Function loss: 0.002241203561425209\n",
      "Step: 1618, Loss: 0.00289132259786129\n",
      "Data loss: 0.0006922766915522516, Function loss: 0.0022123742382973433\n",
      "Step: 1619, Loss: 0.0029046509880572557\n",
      "Data loss: 0.0006411947542801499, Function loss: 0.0022809032816439867\n",
      "Step: 1620, Loss: 0.0029220981523394585\n",
      "Data loss: 0.0006952937110327184, Function loss: 0.0022527442779392004\n",
      "Step: 1621, Loss: 0.0029480380471795797\n",
      "Data loss: 0.0006353445933200419, Function loss: 0.0023480067029595375\n",
      "Step: 1622, Loss: 0.0029833512380719185\n",
      "Data loss: 0.0007018790347501636, Function loss: 0.0023399426136165857\n",
      "Step: 1623, Loss: 0.0030418215319514275\n",
      "Data loss: 0.0006283199181780219, Function loss: 0.0024982441682368517\n",
      "Step: 1624, Loss: 0.0031265639699995518\n",
      "Data loss: 0.0007169342134147882, Function loss: 0.0025377273559570312\n",
      "Step: 1625, Loss: 0.0032546615693718195\n",
      "Data loss: 0.0006219585193321109, Function loss: 0.002802185947075486\n",
      "Step: 1626, Loss: 0.0034241443499922752\n",
      "Data loss: 0.0007444861112162471, Function loss: 0.0029315673746168613\n",
      "Step: 1627, Loss: 0.0036760536022484303\n",
      "Data loss: 0.0006133695133030415, Function loss: 0.003429646836593747\n",
      "Step: 1628, Loss: 0.004043016582727432\n",
      "Data loss: 0.0008052599732764065, Function loss: 0.0038867960684001446\n",
      "Step: 1629, Loss: 0.004692056216299534\n",
      "Data loss: 0.0005996867548674345, Function loss: 0.005075366701930761\n",
      "Step: 1630, Loss: 0.005675053223967552\n",
      "Data loss: 0.0009416327811777592, Function loss: 0.006584373768419027\n",
      "Step: 1631, Loss: 0.0075260065495967865\n",
      "Data loss: 0.0006075878045521677, Function loss: 0.00957022700458765\n",
      "Step: 1632, Loss: 0.010177814401686192\n",
      "Data loss: 0.0012259968789294362, Function loss: 0.013729806058108807\n",
      "Step: 1633, Loss: 0.014955802820622921\n",
      "Data loss: 0.0006947981310077012, Function loss: 0.019130609929561615\n",
      "Step: 1634, Loss: 0.0198254082351923\n",
      "Data loss: 0.0016504984814673662, Function loss: 0.026169829070568085\n",
      "Step: 1635, Loss: 0.027820328250527382\n",
      "Data loss: 0.0008320550550706685, Function loss: 0.029003264382481575\n",
      "Step: 1636, Loss: 0.02983531914651394\n",
      "Data loss: 0.0017566431779414415, Function loss: 0.029257386922836304\n",
      "Step: 1637, Loss: 0.031014030799269676\n",
      "Data loss: 0.0007378400187008083, Function loss: 0.01914343237876892\n",
      "Step: 1638, Loss: 0.019881272688508034\n",
      "Data loss: 0.0010590491583570838, Function loss: 0.00785275362432003\n",
      "Step: 1639, Loss: 0.008911802433431149\n",
      "Data loss: 0.0007383782649412751, Function loss: 0.002256943378597498\n",
      "Step: 1640, Loss: 0.0029953215271234512\n",
      "Data loss: 0.0007226827437989414, Function loss: 0.005841911304742098\n",
      "Step: 1641, Loss: 0.0065645938739180565\n",
      "Data loss: 0.0013013449497520924, Function loss: 0.01193505059927702\n",
      "Step: 1642, Loss: 0.0132363960146904\n",
      "Data loss: 0.0008083261782303452, Function loss: 0.011651886627078056\n",
      "Step: 1643, Loss: 0.012460213154554367\n",
      "Data loss: 0.0011027068831026554, Function loss: 0.005789414048194885\n",
      "Step: 1644, Loss: 0.006892120931297541\n",
      "Data loss: 0.0008650266681797802, Function loss: 0.002177283400669694\n",
      "Step: 1645, Loss: 0.0030423100106418133\n",
      "Data loss: 0.0008226333884522319, Function loss: 0.004655994009226561\n",
      "Step: 1646, Loss: 0.005478627514094114\n",
      "Data loss: 0.0012495983392000198, Function loss: 0.007921773009002209\n",
      "Step: 1647, Loss: 0.009171371348202229\n",
      "Data loss: 0.0008541513816453516, Function loss: 0.006628484930843115\n",
      "Step: 1648, Loss: 0.007482636254280806\n",
      "Data loss: 0.001020970637910068, Function loss: 0.002836809493601322\n",
      "Step: 1649, Loss: 0.003857780247926712\n",
      "Data loss: 0.0009924498153850436, Function loss: 0.002420558826997876\n",
      "Step: 1650, Loss: 0.003413008525967598\n",
      "Data loss: 0.00086009013466537, Function loss: 0.004868207033723593\n",
      "Step: 1651, Loss: 0.005728296935558319\n",
      "Data loss: 0.001172004733234644, Function loss: 0.005266960710287094\n",
      "Step: 1652, Loss: 0.006438965443521738\n",
      "Data loss: 0.0008844037656672299, Function loss: 0.0033821112010627985\n",
      "Step: 1653, Loss: 0.0042665149085223675\n",
      "Data loss: 0.000934104376938194, Function loss: 0.0021126174833625555\n",
      "Step: 1654, Loss: 0.0030467219185084105\n",
      "Data loss: 0.0010575695196166635, Function loss: 0.003098514396697283\n",
      "Step: 1655, Loss: 0.004156083799898624\n",
      "Data loss: 0.0008640829473733902, Function loss: 0.004184842109680176\n",
      "Step: 1656, Loss: 0.005048925057053566\n",
      "Data loss: 0.0010526124387979507, Function loss: 0.003228740068152547\n",
      "Step: 1657, Loss: 0.004281352274119854\n",
      "Data loss: 0.000919987098313868, Function loss: 0.002225039293989539\n",
      "Step: 1658, Loss: 0.003145026508718729\n",
      "Data loss: 0.000868928269483149, Function loss: 0.0024499609135091305\n",
      "Step: 1659, Loss: 0.0033188890665769577\n",
      "Data loss: 0.0010378846200183034, Function loss: 0.003115483559668064\n",
      "Step: 1660, Loss: 0.004153368063271046\n",
      "Data loss: 0.0008322626817971468, Function loss: 0.0033082321751862764\n",
      "Step: 1661, Loss: 0.004140494856983423\n",
      "Data loss: 0.0009570955298841, Function loss: 0.0024337288923561573\n",
      "Step: 1662, Loss: 0.0033908244222402573\n",
      "Data loss: 0.0009064590558409691, Function loss: 0.002063197083771229\n",
      "Step: 1663, Loss: 0.002969656139612198\n",
      "Data loss: 0.0008158324053511024, Function loss: 0.0025047443341463804\n",
      "Step: 1664, Loss: 0.003320576623082161\n",
      "Data loss: 0.0009824744192883372, Function loss: 0.0027386934962123632\n",
      "Step: 1665, Loss: 0.0037211677990853786\n",
      "Data loss: 0.0007908083498477936, Function loss: 0.0027035519015043974\n",
      "Step: 1666, Loss: 0.003494360251352191\n",
      "Data loss: 0.0008863647817634046, Function loss: 0.00212620641104877\n",
      "Step: 1667, Loss: 0.0030125712510198355\n",
      "Data loss: 0.0008658483275212348, Function loss: 0.00202351831831038\n",
      "Step: 1668, Loss: 0.002889366587623954\n",
      "Data loss: 0.0007743435562588274, Function loss: 0.0023777049500495195\n",
      "Step: 1669, Loss: 0.003152048448100686\n",
      "Data loss: 0.0009176491876132786, Function loss: 0.0023877795320004225\n",
      "Step: 1670, Loss: 0.00330542866140604\n",
      "Data loss: 0.0007650262559764087, Function loss: 0.002342663239687681\n",
      "Step: 1671, Loss: 0.003107689553871751\n",
      "Data loss: 0.0008331162389367819, Function loss: 0.0020246747881174088\n",
      "Step: 1672, Loss: 0.0028577910270541906\n",
      "Data loss: 0.0008342306828126311, Function loss: 0.00202037300914526\n",
      "Step: 1673, Loss: 0.002854603808373213\n",
      "Data loss: 0.0007483256049454212, Function loss: 0.0022710778284817934\n",
      "Step: 1674, Loss: 0.0030194034334272146\n",
      "Data loss: 0.000870315358042717, Function loss: 0.0022208895534276962\n",
      "Step: 1675, Loss: 0.003091204911470413\n",
      "Data loss: 0.0007408176315948367, Function loss: 0.0022249389439821243\n",
      "Step: 1676, Loss: 0.002965756691992283\n",
      "Data loss: 0.0008077627862803638, Function loss: 0.0020108625758439302\n",
      "Step: 1677, Loss: 0.002818625420331955\n",
      "Data loss: 0.0007919850177131593, Function loss: 0.0020158670376986265\n",
      "Step: 1678, Loss: 0.0028078521136194468\n",
      "Data loss: 0.0007371391984634101, Function loss: 0.002165040699765086\n",
      "Step: 1679, Loss: 0.0029021799564361572\n",
      "Data loss: 0.000824972172267735, Function loss: 0.0021348490845412016\n",
      "Step: 1680, Loss: 0.0029598213732242584\n",
      "Data loss: 0.0007214733050204813, Function loss: 0.0021831851918250322\n",
      "Step: 1681, Loss: 0.0029046584386378527\n",
      "Data loss: 0.0007866732194088399, Function loss: 0.0020205804612487555\n",
      "Step: 1682, Loss: 0.0028072537388652563\n",
      "Data loss: 0.0007559799123555422, Function loss: 0.0020222088787704706\n",
      "Step: 1683, Loss: 0.002778188791126013\n",
      "Data loss: 0.0007280487334355712, Function loss: 0.002105379942804575\n",
      "Step: 1684, Loss: 0.002833428792655468\n",
      "Data loss: 0.0007852021954022348, Function loss: 0.002096661599352956\n",
      "Step: 1685, Loss: 0.0028818638529628515\n",
      "Data loss: 0.0007081181975081563, Function loss: 0.0021447327453643084\n",
      "Step: 1686, Loss: 0.0028528510592877865\n",
      "Data loss: 0.0007634229841642082, Function loss: 0.0020229651127010584\n",
      "Step: 1687, Loss: 0.0027863881550729275\n",
      "Data loss: 0.0007237413665279746, Function loss: 0.00202777492813766\n",
      "Step: 1688, Loss: 0.002751516178250313\n",
      "Data loss: 0.000726094120182097, Function loss: 0.0020450151059776545\n",
      "Step: 1689, Loss: 0.0027711093425750732\n",
      "Data loss: 0.000741205585654825, Function loss: 0.002065467881038785\n",
      "Step: 1690, Loss: 0.002806673524901271\n",
      "Data loss: 0.0007077943882904947, Function loss: 0.0020945274736732244\n",
      "Step: 1691, Loss: 0.00280232192017138\n",
      "Data loss: 0.000728656305000186, Function loss: 0.002035955898463726\n",
      "Step: 1692, Loss: 0.002764612203463912\n",
      "Data loss: 0.0007119013462215662, Function loss: 0.0020174451638013124\n",
      "Step: 1693, Loss: 0.0027293465100228786\n",
      "Data loss: 0.0007016854942776263, Function loss: 0.002018190920352936\n",
      "Step: 1694, Loss: 0.002719876356422901\n",
      "Data loss: 0.0007176854414865375, Function loss: 0.002015266101807356\n",
      "Step: 1695, Loss: 0.002732951659709215\n",
      "Data loss: 0.0006880972068756819, Function loss: 0.0020560186821967363\n",
      "Step: 1696, Loss: 0.002744115889072418\n",
      "Data loss: 0.0007095559267327189, Function loss: 0.00203076028265059\n",
      "Step: 1697, Loss: 0.002740316092967987\n",
      "Data loss: 0.0006891807424835861, Function loss: 0.002029956318438053\n",
      "Step: 1698, Loss: 0.0027191371191293\n",
      "Data loss: 0.0006856406689621508, Function loss: 0.0020186572801321745\n",
      "Step: 1699, Loss: 0.0027042978908866644\n",
      "Data loss: 0.0006981024635024369, Function loss: 0.0020064401905983686\n",
      "Step: 1700, Loss: 0.0027045425958931446\n",
      "Data loss: 0.0006667702691629529, Function loss: 0.0020449559669941664\n",
      "Step: 1701, Loss: 0.002711726352572441\n",
      "Data loss: 0.000696284812875092, Function loss: 0.002019214443862438\n",
      "Step: 1702, Loss: 0.0027154991403222084\n",
      "Data loss: 0.0006610704585909843, Function loss: 0.002047318499535322\n",
      "Step: 1703, Loss: 0.0027083889581263065\n",
      "Data loss: 0.0006820497219450772, Function loss: 0.002011174103245139\n",
      "Step: 1704, Loss: 0.0026932237669825554\n",
      "Data loss: 0.0006672884919680655, Function loss: 0.002012980403378606\n",
      "Step: 1705, Loss: 0.0026802688371390104\n",
      "Data loss: 0.0006615862366743386, Function loss: 0.002016919432207942\n",
      "Step: 1706, Loss: 0.0026785056106746197\n",
      "Data loss: 0.0006747106090188026, Function loss: 0.0020093584898859262\n",
      "Step: 1707, Loss: 0.002684069098904729\n",
      "Data loss: 0.0006466401973739266, Function loss: 0.0020433946046978235\n",
      "Step: 1708, Loss: 0.002690034918487072\n",
      "Data loss: 0.0006736114155501127, Function loss: 0.002012755488976836\n",
      "Step: 1709, Loss: 0.002686366904526949\n",
      "Data loss: 0.0006429054774343967, Function loss: 0.002034446457400918\n",
      "Step: 1710, Loss: 0.0026773519348353148\n",
      "Data loss: 0.0006621259963139892, Function loss: 0.0020062222611159086\n",
      "Step: 1711, Loss: 0.002668348141014576\n",
      "Data loss: 0.0006472295499406755, Function loss: 0.0020136015955358744\n",
      "Step: 1712, Loss: 0.0026608312036842108\n",
      "Data loss: 0.0006455536931753159, Function loss: 0.00201206817291677\n",
      "Step: 1713, Loss: 0.002657621866092086\n",
      "Data loss: 0.0006534324493259192, Function loss: 0.0020065626595169306\n",
      "Step: 1714, Loss: 0.0026599951088428497\n",
      "Data loss: 0.0006323117413558066, Function loss: 0.0020284862257540226\n",
      "Step: 1715, Loss: 0.0026607979089021683\n",
      "Data loss: 0.0006548975361511111, Function loss: 0.0020064718555659056\n",
      "Step: 1716, Loss: 0.002661369275301695\n",
      "Data loss: 0.0006257465574890375, Function loss: 0.0020322157070040703\n",
      "Step: 1717, Loss: 0.002657962264493108\n",
      "Data loss: 0.0006493381224572659, Function loss: 0.0020037046633660793\n",
      "Step: 1718, Loss: 0.002653042785823345\n",
      "Data loss: 0.0006269490113481879, Function loss: 0.0020156551618129015\n",
      "Step: 1719, Loss: 0.0026426040567457676\n",
      "Data loss: 0.0006346217123791575, Function loss: 0.0020014317706227303\n",
      "Step: 1720, Loss: 0.0026360535994172096\n",
      "Data loss: 0.0006351483752951026, Function loss: 0.00200043898075819\n",
      "Step: 1721, Loss: 0.0026355874724686146\n",
      "Data loss: 0.000619456812273711, Function loss: 0.0020192384254187346\n",
      "Step: 1722, Loss: 0.0026386952959001064\n",
      "Data loss: 0.000641720078419894, Function loss: 0.0020034846384078264\n",
      "Step: 1723, Loss: 0.0026452047750353813\n",
      "Data loss: 0.0006081067840568721, Function loss: 0.0020424993708729744\n",
      "Step: 1724, Loss: 0.0026506062131375074\n",
      "Data loss: 0.0006439000717364252, Function loss: 0.0020096292719244957\n",
      "Step: 1725, Loss: 0.0026535294018685818\n",
      "Data loss: 0.0006035465630702674, Function loss: 0.002050448674708605\n",
      "Step: 1726, Loss: 0.002653995295986533\n",
      "Data loss: 0.0006405445165000856, Function loss: 0.0020107331220060587\n",
      "Step: 1727, Loss: 0.002651277696713805\n",
      "Data loss: 0.00060499511891976, Function loss: 0.0020428765565156937\n",
      "Step: 1728, Loss: 0.0026478716172277927\n",
      "Data loss: 0.0006331437034532428, Function loss: 0.0020178030245006084\n",
      "Step: 1729, Loss: 0.002650946844369173\n",
      "Data loss: 0.0006075639394111931, Function loss: 0.0020513967610895634\n",
      "Step: 1730, Loss: 0.0026589606422930956\n",
      "Data loss: 0.0006297951913438737, Function loss: 0.0020417545456439257\n",
      "Step: 1731, Loss: 0.0026715497951954603\n",
      "Data loss: 0.000607399910222739, Function loss: 0.002080621663480997\n",
      "Step: 1732, Loss: 0.002688021631911397\n",
      "Data loss: 0.0006325097056105733, Function loss: 0.002072748262435198\n",
      "Step: 1733, Loss: 0.0027052578516304493\n",
      "Data loss: 0.0006022590096108615, Function loss: 0.002121191006153822\n",
      "Step: 1734, Loss: 0.0027234500739723444\n",
      "Data loss: 0.0006406078464351594, Function loss: 0.0021011277567595243\n",
      "Step: 1735, Loss: 0.0027417356614023447\n",
      "Data loss: 0.0005950129125267267, Function loss: 0.0021717583294957876\n",
      "Step: 1736, Loss: 0.0027667712420225143\n",
      "Data loss: 0.0006522819167003036, Function loss: 0.0021495367400348186\n",
      "Step: 1737, Loss: 0.002801818773150444\n",
      "Data loss: 0.0005882277619093657, Function loss: 0.0022592509631067514\n",
      "Step: 1738, Loss: 0.002847478725016117\n",
      "Data loss: 0.0006685726693831384, Function loss: 0.0022379946894943714\n",
      "Step: 1739, Loss: 0.0029065674170851707\n",
      "Data loss: 0.0005841953679919243, Function loss: 0.0024015617091208696\n",
      "Step: 1740, Loss: 0.002985757077112794\n",
      "Data loss: 0.0006923074251972139, Function loss: 0.002403368940576911\n",
      "Step: 1741, Loss: 0.0030956764239817858\n",
      "Data loss: 0.0005901175318285823, Function loss: 0.0026461868546903133\n",
      "Step: 1742, Loss: 0.0032363045029342175\n",
      "Data loss: 0.0007268893532454967, Function loss: 0.002695988165214658\n",
      "Step: 1743, Loss: 0.0034228775184601545\n",
      "Data loss: 0.0006101674516685307, Function loss: 0.003071674844250083\n",
      "Step: 1744, Loss: 0.0036818422377109528\n",
      "Data loss: 0.0007865673978812993, Function loss: 0.003231819486245513\n",
      "Step: 1745, Loss: 0.004018386825919151\n",
      "Data loss: 0.0006529010715894401, Function loss: 0.003785274690017104\n",
      "Step: 1746, Loss: 0.004438175819814205\n",
      "Data loss: 0.0008726919186301529, Function loss: 0.004078516270965338\n",
      "Step: 1747, Loss: 0.0049512083642184734\n",
      "Data loss: 0.0007241427665576339, Function loss: 0.004827762488275766\n",
      "Step: 1748, Loss: 0.005551905371248722\n",
      "Data loss: 0.0009770344477146864, Function loss: 0.005144489463418722\n",
      "Step: 1749, Loss: 0.006121523678302765\n",
      "Data loss: 0.0007872904534451663, Function loss: 0.005728298332542181\n",
      "Step: 1750, Loss: 0.006515588611364365\n",
      "Data loss: 0.0010121644008904696, Function loss: 0.005515439435839653\n",
      "Step: 1751, Loss: 0.006527603603899479\n",
      "Data loss: 0.000760450551752001, Function loss: 0.005295674782246351\n",
      "Step: 1752, Loss: 0.006056125275790691\n",
      "Data loss: 0.0008827194105833769, Function loss: 0.004235990345478058\n",
      "Step: 1753, Loss: 0.005118709988892078\n",
      "Data loss: 0.000641588878352195, Function loss: 0.003378943307325244\n",
      "Step: 1754, Loss: 0.004020532127469778\n",
      "Data loss: 0.0006803406868129969, Function loss: 0.002411002991721034\n",
      "Step: 1755, Loss: 0.003091343678534031\n",
      "Data loss: 0.0005892658955417573, Function loss: 0.002011390868574381\n",
      "Step: 1756, Loss: 0.002600656822323799\n",
      "Data loss: 0.0005904396530240774, Function loss: 0.0020293116103857756\n",
      "Step: 1757, Loss: 0.002619751263409853\n",
      "Data loss: 0.0006691986927762628, Function loss: 0.002320514526218176\n",
      "Step: 1758, Loss: 0.002989713102579117\n",
      "Data loss: 0.0006179802003316581, Function loss: 0.002821606118232012\n",
      "Step: 1759, Loss: 0.003439586376771331\n",
      "Data loss: 0.0007495806203223765, Function loss: 0.0029649424832314253\n",
      "Step: 1760, Loss: 0.003714523045346141\n",
      "Data loss: 0.0006267051794566214, Function loss: 0.003073576372116804\n",
      "Step: 1761, Loss: 0.0037002814933657646\n",
      "Data loss: 0.0007244807784445584, Function loss: 0.002692239359021187\n",
      "Step: 1762, Loss: 0.0034167200792580843\n",
      "Data loss: 0.0005862789694219828, Function loss: 0.00243962649255991\n",
      "Step: 1763, Loss: 0.0030259054619818926\n",
      "Data loss: 0.000646954751573503, Function loss: 0.0020643260795623064\n",
      "Step: 1764, Loss: 0.0027112807147204876\n",
      "Data loss: 0.0005810369038954377, Function loss: 0.002009584568440914\n",
      "Step: 1765, Loss: 0.00259062135592103\n",
      "Data loss: 0.0006137184100225568, Function loss: 0.002049088478088379\n",
      "Step: 1766, Loss: 0.002662806771695614\n",
      "Data loss: 0.0006279387744143605, Function loss: 0.002203551819548011\n",
      "Step: 1767, Loss: 0.002831490710377693\n",
      "Data loss: 0.0006173624424263835, Function loss: 0.0023599243722856045\n",
      "Step: 1768, Loss: 0.00297728693112731\n",
      "Data loss: 0.0006546066724695265, Function loss: 0.0023492409382015467\n",
      "Step: 1769, Loss: 0.0030038475524634123\n",
      "Data loss: 0.0006076379795558751, Function loss: 0.002298399806022644\n",
      "Step: 1770, Loss: 0.002906037727370858\n",
      "Data loss: 0.0006315343780443072, Function loss: 0.0021128542721271515\n",
      "Step: 1771, Loss: 0.0027443887665867805\n",
      "Data loss: 0.000592063763178885, Function loss: 0.002014908706769347\n",
      "Step: 1772, Loss: 0.002606972586363554\n",
      "Data loss: 0.0006038775318302214, Function loss: 0.0019251607591286302\n",
      "Step: 1773, Loss: 0.0025290383491665125\n",
      "Data loss: 0.0005946533055976033, Function loss: 0.0019196576904505491\n",
      "Step: 1774, Loss: 0.0025143111124634743\n",
      "Data loss: 0.0005941448034718633, Function loss: 0.0019507467513903975\n",
      "Step: 1775, Loss: 0.002544891554862261\n",
      "Data loss: 0.00060717185260728, Function loss: 0.0019874004647135735\n",
      "Step: 1776, Loss: 0.0025945722591131926\n",
      "Data loss: 0.000593941193073988, Function loss: 0.002042945008724928\n",
      "Step: 1777, Loss: 0.002636886201798916\n",
      "Data loss: 0.0006137861637398601, Function loss: 0.002042742446064949\n",
      "Step: 1778, Loss: 0.002656528726220131\n",
      "Data loss: 0.0005946402088738978, Function loss: 0.0020581253338605165\n",
      "Step: 1779, Loss: 0.0026527654845267534\n",
      "Data loss: 0.0006060689338482916, Function loss: 0.002025533700361848\n",
      "Step: 1780, Loss: 0.0026316025760024786\n",
      "Data loss: 0.0005959743284620345, Function loss: 0.0020100465044379234\n",
      "Step: 1781, Loss: 0.002606020774692297\n",
      "Data loss: 0.0005918506649322808, Function loss: 0.0019913632422685623\n",
      "Step: 1782, Loss: 0.002583213848993182\n",
      "Data loss: 0.0006006400799378753, Function loss: 0.0019610291346907616\n",
      "Step: 1783, Loss: 0.0025616693310439587\n",
      "Data loss: 0.0005746431997977197, Function loss: 0.0019858398009091616\n",
      "Step: 1784, Loss: 0.002560483058914542\n",
      "Data loss: 0.0006132338312454522, Function loss: 0.001966730458661914\n",
      "Step: 1785, Loss: 0.002579964231699705\n",
      "Data loss: 0.000554903584998101, Function loss: 0.002075123367831111\n",
      "Step: 1786, Loss: 0.002630027011036873\n",
      "Data loss: 0.000634381256531924, Function loss: 0.0020794288720935583\n",
      "Step: 1787, Loss: 0.0027138101868331432\n",
      "Data loss: 0.0005377113120630383, Function loss: 0.002275700680911541\n",
      "Step: 1788, Loss: 0.0028134118765592575\n",
      "Data loss: 0.0006562361377291381, Function loss: 0.0022990284487605095\n",
      "Step: 1789, Loss: 0.0029552646446973085\n",
      "Data loss: 0.000523464463185519, Function loss: 0.002622807864099741\n",
      "Step: 1790, Loss: 0.003146272385492921\n",
      "Data loss: 0.0006871342193335295, Function loss: 0.002781625371426344\n",
      "Step: 1791, Loss: 0.0034687595907598734\n",
      "Data loss: 0.0005116596585139632, Function loss: 0.003418440232053399\n",
      "Step: 1792, Loss: 0.0039300997741520405\n",
      "Data loss: 0.0007449568947777152, Function loss: 0.003933302126824856\n",
      "Step: 1793, Loss: 0.004678259138017893\n",
      "Data loss: 0.000510233745444566, Function loss: 0.0050727915950119495\n",
      "Step: 1794, Loss: 0.005583025515079498\n",
      "Data loss: 0.0008387042325921357, Function loss: 0.006098989397287369\n",
      "Step: 1795, Loss: 0.006937693804502487\n",
      "Data loss: 0.0005265675717964768, Function loss: 0.007703704759478569\n",
      "Step: 1796, Loss: 0.008230272680521011\n",
      "Data loss: 0.0009602975915186107, Function loss: 0.009228017181158066\n",
      "Step: 1797, Loss: 0.010188315063714981\n",
      "Data loss: 0.0005564694292843342, Function loss: 0.010892619378864765\n",
      "Step: 1798, Loss: 0.011449089273810387\n",
      "Data loss: 0.001067990786395967, Function loss: 0.012201924808323383\n",
      "Step: 1799, Loss: 0.013269915245473385\n",
      "Data loss: 0.000578111968934536, Function loss: 0.012503881007432938\n",
      "Step: 1800, Loss: 0.013081992976367474\n",
      "Data loss: 0.0010547639103606343, Function loss: 0.011655923910439014\n",
      "Step: 1801, Loss: 0.012710687704384327\n",
      "Data loss: 0.0005584789905697107, Function loss: 0.009096156805753708\n",
      "Step: 1802, Loss: 0.009654635563492775\n",
      "Data loss: 0.0008419136865995824, Function loss: 0.005648166406899691\n",
      "Step: 1803, Loss: 0.006490080151706934\n",
      "Data loss: 0.0005620046867989004, Function loss: 0.003029571147635579\n",
      "Step: 1804, Loss: 0.0035915758926421404\n",
      "Data loss: 0.000630558526609093, Function loss: 0.0018578850431367755\n",
      "Step: 1805, Loss: 0.0024884436279535294\n",
      "Data loss: 0.0007043264922685921, Function loss: 0.0024577691219747066\n",
      "Step: 1806, Loss: 0.003162095556035638\n",
      "Data loss: 0.0005815916229039431, Function loss: 0.004022790584713221\n",
      "Step: 1807, Loss: 0.004604382440447807\n",
      "Data loss: 0.0008427011198364198, Function loss: 0.00483563682064414\n",
      "Step: 1808, Loss: 0.005678337998688221\n",
      "Data loss: 0.0005899023381061852, Function loss: 0.00475042499601841\n",
      "Step: 1809, Loss: 0.005340327508747578\n",
      "Data loss: 0.000785761687438935, Function loss: 0.0034380590077489614\n",
      "Step: 1810, Loss: 0.004223820753395557\n",
      "Data loss: 0.0006161949713714421, Function loss: 0.002368398243561387\n",
      "Step: 1811, Loss: 0.0029845931567251682\n",
      "Data loss: 0.0006589521653950214, Function loss: 0.0018330775201320648\n",
      "Step: 1812, Loss: 0.0024920296855270863\n",
      "Data loss: 0.0007110167643986642, Function loss: 0.002115881070494652\n",
      "Step: 1813, Loss: 0.002826897893100977\n",
      "Data loss: 0.0006079607992433012, Function loss: 0.002890805946663022\n",
      "Step: 1814, Loss: 0.003498766804113984\n",
      "Data loss: 0.0007815472781658173, Function loss: 0.003142434870824218\n",
      "Step: 1815, Loss: 0.003923982381820679\n",
      "Data loss: 0.0006073852418921888, Function loss: 0.0031141680665314198\n",
      "Step: 1816, Loss: 0.0037215533666312695\n",
      "Data loss: 0.0007378780865110457, Function loss: 0.002444403013214469\n",
      "Step: 1817, Loss: 0.0031822810415178537\n",
      "Data loss: 0.0006369379698298872, Function loss: 0.0020105259027332067\n",
      "Step: 1818, Loss: 0.002647463930770755\n",
      "Data loss: 0.0006531754625029862, Function loss: 0.0018208521651104093\n",
      "Step: 1819, Loss: 0.0024740276858210564\n",
      "Data loss: 0.0007013703580014408, Function loss: 0.00199231063015759\n",
      "Step: 1820, Loss: 0.0026936810463666916\n",
      "Data loss: 0.0006093549891375005, Function loss: 0.002444356679916382\n",
      "Step: 1821, Loss: 0.0030537117272615433\n",
      "Data loss: 0.0007404612842947245, Function loss: 0.002550069009885192\n",
      "Step: 1822, Loss: 0.0032905302941799164\n",
      "Data loss: 0.0006061970489099622, Function loss: 0.002621859312057495\n",
      "Step: 1823, Loss: 0.0032280562445521355\n",
      "Data loss: 0.0007119666552171111, Function loss: 0.0022650486789643764\n",
      "Step: 1824, Loss: 0.0029770154505968094\n",
      "Data loss: 0.0006251278682611883, Function loss: 0.0020532698836177588\n",
      "Step: 1825, Loss: 0.002678397810086608\n",
      "Data loss: 0.0006543403724208474, Function loss: 0.0018301171949133277\n",
      "Step: 1826, Loss: 0.002484457567334175\n",
      "Data loss: 0.0006577203166671097, Function loss: 0.0017957189120352268\n",
      "Step: 1827, Loss: 0.0024534391704946756\n",
      "Data loss: 0.0006092589464969933, Function loss: 0.001961363945156336\n",
      "Step: 1828, Loss: 0.0025706228334456682\n",
      "Data loss: 0.0006928431103006005, Function loss: 0.002067365450784564\n",
      "Step: 1829, Loss: 0.0027602086775004864\n",
      "Data loss: 0.0005893100751563907, Function loss: 0.0023189682979136705\n",
      "Step: 1830, Loss: 0.0029082782566547394\n",
      "Data loss: 0.0007022826466709375, Function loss: 0.0022530751302838326\n",
      "Step: 1831, Loss: 0.00295535777695477\n",
      "Data loss: 0.0005897462251596153, Function loss: 0.0022748196497559547\n",
      "Step: 1832, Loss: 0.002864565933123231\n",
      "Data loss: 0.0006766545120626688, Function loss: 0.0020441152155399323\n",
      "Step: 1833, Loss: 0.002720769727602601\n",
      "Data loss: 0.0006053493707440794, Function loss: 0.001965533010661602\n",
      "Step: 1834, Loss: 0.0025708824396133423\n",
      "Data loss: 0.0006367873284034431, Function loss: 0.0018281004158779979\n",
      "Step: 1835, Loss: 0.00246488768607378\n",
      "Data loss: 0.0006291365716606379, Function loss: 0.0017879598308354616\n",
      "Step: 1836, Loss: 0.0024170964024960995\n",
      "Data loss: 0.0006028750212863088, Function loss: 0.0018262347439303994\n",
      "Step: 1837, Loss: 0.002429109765216708\n",
      "Data loss: 0.0006484686164185405, Function loss: 0.001830157358199358\n",
      "Step: 1838, Loss: 0.0024786260910332203\n",
      "Data loss: 0.0005844292463734746, Function loss: 0.0019475369481369853\n",
      "Step: 1839, Loss: 0.00253196619451046\n",
      "Data loss: 0.000652676448225975, Function loss: 0.0019035015720874071\n",
      "Step: 1840, Loss: 0.002556178020313382\n",
      "Data loss: 0.0005828618304803967, Function loss: 0.0019522355869412422\n",
      "Step: 1841, Loss: 0.002535097301006317\n",
      "Data loss: 0.0006377724348567426, Function loss: 0.0018533370457589626\n",
      "Step: 1842, Loss: 0.002491109538823366\n",
      "Data loss: 0.0005923064891248941, Function loss: 0.0018512773094698787\n",
      "Step: 1843, Loss: 0.002443583682179451\n",
      "Data loss: 0.0006144979270175099, Function loss: 0.001792372204363346\n",
      "Step: 1844, Loss: 0.002406870014965534\n",
      "Data loss: 0.0006066726637072861, Function loss: 0.0017813611775636673\n",
      "Step: 1845, Loss: 0.0023880337830632925\n",
      "Data loss: 0.0005921314004808664, Function loss: 0.0017966958694159985\n",
      "Step: 1846, Loss: 0.002388827269896865\n",
      "Data loss: 0.0006200578063726425, Function loss: 0.0017820183420553803\n",
      "Step: 1847, Loss: 0.002402076032012701\n",
      "Data loss: 0.0005746359820477664, Function loss: 0.0018531003734096885\n",
      "Step: 1848, Loss: 0.002427736297249794\n",
      "Data loss: 0.0006301211542449892, Function loss: 0.0018301823874935508\n",
      "Step: 1849, Loss: 0.002460303483530879\n",
      "Data loss: 0.0005621983436867595, Function loss: 0.0019312090007588267\n",
      "Step: 1850, Loss: 0.002493407344445586\n",
      "Data loss: 0.0006335435318760574, Function loss: 0.0018875047098845243\n",
      "Step: 1851, Loss: 0.0025210482999682426\n",
      "Data loss: 0.0005576899857260287, Function loss: 0.001974567538127303\n",
      "Step: 1852, Loss: 0.002532257465645671\n",
      "Data loss: 0.0006271885940805078, Function loss: 0.0019028017995879054\n",
      "Step: 1853, Loss: 0.002529990393668413\n",
      "Data loss: 0.0005612200475297868, Function loss: 0.001951209967955947\n",
      "Step: 1854, Loss: 0.0025124300736933947\n",
      "Data loss: 0.0006130034453235567, Function loss: 0.001869505736976862\n",
      "Step: 1855, Loss: 0.0024825092405080795\n",
      "Data loss: 0.0005694261053577065, Function loss: 0.0018808487802743912\n",
      "Step: 1856, Loss: 0.002450274769216776\n",
      "Data loss: 0.0005980373825877905, Function loss: 0.0018319530645385385\n",
      "Step: 1857, Loss: 0.0024299905635416508\n",
      "Data loss: 0.0005738012841902673, Function loss: 0.0018390779150649905\n",
      "Step: 1858, Loss: 0.002412879141047597\n",
      "Data loss: 0.0005897771916352212, Function loss: 0.0018115572165697813\n",
      "Step: 1859, Loss: 0.0024013344664126635\n",
      "Data loss: 0.0005706003867089748, Function loss: 0.001821898273192346\n",
      "Step: 1860, Loss: 0.0023924987763166428\n",
      "Data loss: 0.0005874402122572064, Function loss: 0.0017960008699446917\n",
      "Step: 1861, Loss: 0.00238344119861722\n",
      "Data loss: 0.0005623719771392643, Function loss: 0.0018179417820647359\n",
      "Step: 1862, Loss: 0.002380313817411661\n",
      "Data loss: 0.0005901121185161173, Function loss: 0.001791207818314433\n",
      "Step: 1863, Loss: 0.0023813198786228895\n",
      "Data loss: 0.0005517426761798561, Function loss: 0.0018365071155130863\n",
      "Step: 1864, Loss: 0.0023882498499006033\n",
      "Data loss: 0.0005948864272795618, Function loss: 0.0018001730786636472\n",
      "Step: 1865, Loss: 0.002395059447735548\n",
      "Data loss: 0.0005433636833913624, Function loss: 0.0018576402217149734\n",
      "Step: 1866, Loss: 0.002401003846898675\n",
      "Data loss: 0.0005977613036520779, Function loss: 0.0018110343953594565\n",
      "Step: 1867, Loss: 0.0024087957572191954\n",
      "Data loss: 0.0005372622981667519, Function loss: 0.0018829901237040758\n",
      "Step: 1868, Loss: 0.0024202524218708277\n",
      "Data loss: 0.00060160958673805, Function loss: 0.0018346717115491629\n",
      "Step: 1869, Loss: 0.002436281181871891\n",
      "Data loss: 0.0005306830862537026, Function loss: 0.0019273271318525076\n",
      "Step: 1870, Loss: 0.002458010334521532\n",
      "Data loss: 0.0006065380293875933, Function loss: 0.0018717152997851372\n",
      "Step: 1871, Loss: 0.0024782533291727304\n",
      "Data loss: 0.000525644572917372, Function loss: 0.00196841056458652\n",
      "Step: 1872, Loss: 0.0024940550792962313\n",
      "Data loss: 0.0006097088917158544, Function loss: 0.0019003426423296332\n",
      "Step: 1873, Loss: 0.0025100514758378267\n",
      "Data loss: 0.0005217338912189007, Function loss: 0.002002554479986429\n",
      "Step: 1874, Loss: 0.00252428837120533\n",
      "Data loss: 0.0006118468008935452, Function loss: 0.0019267394673079252\n",
      "Step: 1875, Loss: 0.0025385862682014704\n",
      "Data loss: 0.0005184093024581671, Function loss: 0.0020248417276889086\n",
      "Step: 1876, Loss: 0.0025432510301470757\n",
      "Data loss: 0.000611117051448673, Function loss: 0.0019347011111676693\n",
      "Step: 1877, Loss: 0.0025458182208240032\n",
      "Data loss: 0.0005156741244718432, Function loss: 0.0020322464406490326\n",
      "Step: 1878, Loss: 0.0025479206815361977\n",
      "Data loss: 0.0006102250190451741, Function loss: 0.0019490529084578156\n",
      "Step: 1879, Loss: 0.0025592779275029898\n",
      "Data loss: 0.0005116237443871796, Function loss: 0.0020616790279746056\n",
      "Step: 1880, Loss: 0.0025733027141541243\n",
      "Data loss: 0.0006108460365794599, Function loss: 0.0019899681210517883\n",
      "Step: 1881, Loss: 0.002600814215838909\n",
      "Data loss: 0.0005058451206423342, Function loss: 0.0021326597779989243\n",
      "Step: 1882, Loss: 0.0026385048404335976\n",
      "Data loss: 0.0006149923428893089, Function loss: 0.0020940257236361504\n",
      "Step: 1883, Loss: 0.0027090180665254593\n",
      "Data loss: 0.0004994570626877248, Function loss: 0.0023070499300956726\n",
      "Step: 1884, Loss: 0.0028065070509910583\n",
      "Data loss: 0.0006268264842219651, Function loss: 0.0023284987546503544\n",
      "Step: 1885, Loss: 0.0029553251806646585\n",
      "Data loss: 0.0004945053951814771, Function loss: 0.0026474790647625923\n",
      "Step: 1886, Loss: 0.0031419843435287476\n",
      "Data loss: 0.0006500965100713074, Function loss: 0.0028087077662348747\n",
      "Step: 1887, Loss: 0.0034588042180985212\n",
      "Data loss: 0.0004945716937072575, Function loss: 0.0034085307270288467\n",
      "Step: 1888, Loss: 0.0039031023625284433\n",
      "Data loss: 0.0007046482642181218, Function loss: 0.004009599797427654\n",
      "Step: 1889, Loss: 0.004714248236268759\n",
      "Data loss: 0.0005152407684363425, Function loss: 0.005324352066963911\n",
      "Step: 1890, Loss: 0.005839592777192593\n",
      "Data loss: 0.0008305615629069507, Function loss: 0.007010878529399633\n",
      "Step: 1891, Loss: 0.007841439917683601\n",
      "Data loss: 0.0005990415229462087, Function loss: 0.009688736870884895\n",
      "Step: 1892, Loss: 0.010287778452038765\n",
      "Data loss: 0.0010803994955495, Function loss: 0.01331205852329731\n",
      "Step: 1893, Loss: 0.014392457902431488\n",
      "Data loss: 0.0008025721763260663, Function loss: 0.017339210957288742\n",
      "Step: 1894, Loss: 0.01814178377389908\n",
      "Data loss: 0.0014523685676977038, Function loss: 0.02234298177063465\n",
      "Step: 1895, Loss: 0.023795349523425102\n",
      "Data loss: 0.0010461218189448118, Function loss: 0.02375897392630577\n",
      "Step: 1896, Loss: 0.02480509504675865\n",
      "Data loss: 0.001543430844321847, Function loss: 0.022875824943184853\n",
      "Step: 1897, Loss: 0.024419255554676056\n",
      "Data loss: 0.0008980769198387861, Function loss: 0.015228940173983574\n",
      "Step: 1898, Loss: 0.01612701639533043\n",
      "Data loss: 0.0009244218235835433, Function loss: 0.006623648572713137\n",
      "Step: 1899, Loss: 0.007548070512712002\n",
      "Data loss: 0.0006389205227605999, Function loss: 0.001973204081878066\n",
      "Step: 1900, Loss: 0.002612124662846327\n",
      "Data loss: 0.0006138826138339937, Function loss: 0.003712190082296729\n",
      "Step: 1901, Loss: 0.00432607252150774\n",
      "Data loss: 0.0010294824605807662, Function loss: 0.00812110397964716\n",
      "Step: 1902, Loss: 0.00915058609098196\n",
      "Data loss: 0.0007756186532787979, Function loss: 0.009643727913498878\n",
      "Step: 1903, Loss: 0.010419346392154694\n",
      "Data loss: 0.0010142927058041096, Function loss: 0.006474575959146023\n",
      "Step: 1904, Loss: 0.007488868664950132\n",
      "Data loss: 0.0007092963205650449, Function loss: 0.0025470799300819635\n",
      "Step: 1905, Loss: 0.0032563763670623302\n",
      "Data loss: 0.0006923821056261659, Function loss: 0.002270853379741311\n",
      "Step: 1906, Loss: 0.0029632356017827988\n",
      "Data loss: 0.0009460467263124883, Function loss: 0.004872840363532305\n",
      "Step: 1907, Loss: 0.005818887148052454\n",
      "Data loss: 0.0007402513874694705, Function loss: 0.006177443545311689\n",
      "Step: 1908, Loss: 0.006917695049196482\n",
      "Data loss: 0.0009338576928712428, Function loss: 0.004169035237282515\n",
      "Step: 1909, Loss: 0.005102892871946096\n",
      "Data loss: 0.0007393172709271312, Function loss: 0.0019920961931347847\n",
      "Step: 1910, Loss: 0.0027314135804772377\n",
      "Data loss: 0.000719645933713764, Function loss: 0.002206322504207492\n",
      "Step: 1911, Loss: 0.0029259684961289167\n",
      "Data loss: 0.0009141159243881702, Function loss: 0.0038401426281780005\n",
      "Step: 1912, Loss: 0.004754258319735527\n",
      "Data loss: 0.0007285176543518901, Function loss: 0.0043923514895141125\n",
      "Step: 1913, Loss: 0.005120869260281324\n",
      "Data loss: 0.0008602484012953937, Function loss: 0.002843614434823394\n",
      "Step: 1914, Loss: 0.0037038628943264484\n",
      "Data loss: 0.0007478204788640141, Function loss: 0.0016834840644150972\n",
      "Step: 1915, Loss: 0.002431304659694433\n",
      "Data loss: 0.0007230413029901683, Function loss: 0.0022610106971114874\n",
      "Step: 1916, Loss: 0.0029840520583093166\n",
      "Data loss: 0.0008730917470529675, Function loss: 0.0032693720422685146\n",
      "Step: 1917, Loss: 0.00414246367290616\n",
      "Data loss: 0.0007108067511580884, Function loss: 0.0032484272960573435\n",
      "Step: 1918, Loss: 0.003959233872592449\n",
      "Data loss: 0.0007975427433848381, Function loss: 0.0020910927560180426\n",
      "Step: 1919, Loss: 0.0028886354994028807\n",
      "Data loss: 0.0007330413209274411, Function loss: 0.001633517094887793\n",
      "Step: 1920, Loss: 0.002366558415815234\n",
      "Data loss: 0.000710957043338567, Function loss: 0.002225610427558422\n",
      "Step: 1921, Loss: 0.002936567412689328\n",
      "Data loss: 0.000819288135971874, Function loss: 0.0027725757099688053\n",
      "Step: 1922, Loss: 0.0035918639041483402\n",
      "Data loss: 0.0006832448998466134, Function loss: 0.0026479456573724747\n",
      "Step: 1923, Loss: 0.0033311904408037663\n",
      "Data loss: 0.0007574704941362143, Function loss: 0.001869548810645938\n",
      "Step: 1924, Loss: 0.002627019304782152\n",
      "Data loss: 0.0006956838187761605, Function loss: 0.0016519156051799655\n",
      "Step: 1925, Loss: 0.002347599482163787\n",
      "Data loss: 0.0006976319127716124, Function loss: 0.0019954140298068523\n",
      "Step: 1926, Loss: 0.002693045884370804\n",
      "Data loss: 0.0007596303476020694, Function loss: 0.002358678262680769\n",
      "Step: 1927, Loss: 0.00311830872669816\n",
      "Data loss: 0.0006715367780998349, Function loss: 0.0023901036474853754\n",
      "Step: 1928, Loss: 0.003061640542000532\n",
      "Data loss: 0.0007249830523505807, Function loss: 0.0019190552411600947\n",
      "Step: 1929, Loss: 0.0026440382935106754\n",
      "Data loss: 0.0006620881031267345, Function loss: 0.001649182289838791\n",
      "Step: 1930, Loss: 0.0023112704511731863\n",
      "Data loss: 0.0006704583647660911, Function loss: 0.0016770651564002037\n",
      "Step: 1931, Loss: 0.0023475235793739557\n",
      "Data loss: 0.0007016875897534192, Function loss: 0.0019150066655129194\n",
      "Step: 1932, Loss: 0.0026166941970586777\n",
      "Data loss: 0.0006547935190610588, Function loss: 0.002126227132976055\n",
      "Step: 1933, Loss: 0.002781020710244775\n",
      "Data loss: 0.0007013414870016277, Function loss: 0.0019819866865873337\n",
      "Step: 1934, Loss: 0.0026833282317966223\n",
      "Data loss: 0.0006427429034374654, Function loss: 0.001778945210389793\n",
      "Step: 1935, Loss: 0.0024216880556195974\n",
      "Data loss: 0.0006523950723931193, Function loss: 0.0016022487543523312\n",
      "Step: 1936, Loss: 0.0022546439431607723\n",
      "Data loss: 0.0006592671852558851, Function loss: 0.0016567291459068656\n",
      "Step: 1937, Loss: 0.002315996214747429\n",
      "Data loss: 0.0006325404974631965, Function loss: 0.001856851507909596\n",
      "Step: 1938, Loss: 0.0024893919471651316\n",
      "Data loss: 0.0006710208836011589, Function loss: 0.0018858322873711586\n",
      "Step: 1939, Loss: 0.0025568532291799784\n",
      "Data loss: 0.0006321272812783718, Function loss: 0.0018139625899493694\n",
      "Step: 1940, Loss: 0.0024460898712277412\n",
      "Data loss: 0.0006352955242618918, Function loss: 0.0016587077407166362\n",
      "Step: 1941, Loss: 0.002294003264978528\n",
      "Data loss: 0.0006377151003107429, Function loss: 0.001600049901753664\n",
      "Step: 1942, Loss: 0.0022377651184797287\n",
      "Data loss: 0.0006082137115299702, Function loss: 0.0016797285061329603\n",
      "Step: 1943, Loss: 0.0022879422176629305\n",
      "Data loss: 0.0006453890237025917, Function loss: 0.0017211775993928313\n",
      "Step: 1944, Loss: 0.002366566564887762\n",
      "Data loss: 0.0006076586432754993, Function loss: 0.0017767397221177816\n",
      "Step: 1945, Loss: 0.002384398365393281\n",
      "Data loss: 0.0006274481420405209, Function loss: 0.0017058135708793998\n",
      "Step: 1946, Loss: 0.0023332617711275816\n",
      "Data loss: 0.0006188501720316708, Function loss: 0.0016433040145784616\n",
      "Step: 1947, Loss: 0.0022621541284024715\n",
      "Data loss: 0.0005930225015617907, Function loss: 0.0016431182157248259\n",
      "Step: 1948, Loss: 0.0022361406590789557\n",
      "Data loss: 0.0006302184774540365, Function loss: 0.0016250039916485548\n",
      "Step: 1949, Loss: 0.002255222527310252\n",
      "Data loss: 0.0005767571274191141, Function loss: 0.0017011656891554594\n",
      "Step: 1950, Loss: 0.0022779228165745735\n",
      "Data loss: 0.000623710046056658, Function loss: 0.0016519061755388975\n",
      "Step: 1951, Loss: 0.0022756161633878946\n",
      "Data loss: 0.0005786785623058677, Function loss: 0.0016638326924294233\n",
      "Step: 1952, Loss: 0.002242511138319969\n",
      "Data loss: 0.0006015111575834453, Function loss: 0.0016060327179729939\n",
      "Step: 1953, Loss: 0.0022075439337641\n",
      "Data loss: 0.0005896227667108178, Function loss: 0.0016018458409234881\n",
      "Step: 1954, Loss: 0.002191468607634306\n",
      "Data loss: 0.0005786242545582354, Function loss: 0.0016158082289621234\n",
      "Step: 1955, Loss: 0.0021944325417280197\n",
      "Data loss: 0.0005986011819913983, Function loss: 0.0016120950458571315\n",
      "Step: 1956, Loss: 0.00221069622784853\n",
      "Data loss: 0.0005626651109196246, Function loss: 0.0016628419980406761\n",
      "Step: 1957, Loss: 0.0022255070507526398\n",
      "Data loss: 0.0006011962541379035, Function loss: 0.0016314443200826645\n",
      "Step: 1958, Loss: 0.002232640516012907\n",
      "Data loss: 0.0005545833264477551, Function loss: 0.0016733568627387285\n",
      "Step: 1959, Loss: 0.0022279401309788227\n",
      "Data loss: 0.000595921534113586, Function loss: 0.0016213767230510712\n",
      "Step: 1960, Loss: 0.002217298373579979\n",
      "Data loss: 0.0005513927899301052, Function loss: 0.0016610526945441961\n",
      "Step: 1961, Loss: 0.0022124454844743013\n",
      "Data loss: 0.0005903429118916392, Function loss: 0.0016189760062843561\n",
      "Step: 1962, Loss: 0.0022093188017606735\n",
      "Data loss: 0.000551144010387361, Function loss: 0.0016534775495529175\n",
      "Step: 1963, Loss: 0.0022046216763556004\n",
      "Data loss: 0.0005817089113406837, Function loss: 0.0016217828961089253\n",
      "Step: 1964, Loss: 0.002203491749241948\n",
      "Data loss: 0.0005531866918317974, Function loss: 0.0016494686715304852\n",
      "Step: 1965, Loss: 0.0022026554215699434\n",
      "Data loss: 0.0005724377115257084, Function loss: 0.0016376571729779243\n",
      "Step: 1966, Loss: 0.002210094826295972\n",
      "Data loss: 0.0005548303015530109, Function loss: 0.001660674810409546\n",
      "Step: 1967, Loss: 0.002215505111962557\n",
      "Data loss: 0.0005667263176292181, Function loss: 0.0016589273000136018\n",
      "Step: 1968, Loss: 0.0022256537340581417\n",
      "Data loss: 0.0005527890753000975, Function loss: 0.001676039770245552\n",
      "Step: 1969, Loss: 0.0022288288455456495\n",
      "Data loss: 0.0005648582009598613, Function loss: 0.0016634744824841619\n",
      "Step: 1970, Loss: 0.002228332683444023\n",
      "Data loss: 0.0005453602061606944, Function loss: 0.0016811563400551677\n",
      "Step: 1971, Loss: 0.002226516604423523\n",
      "Data loss: 0.0005669588572345674, Function loss: 0.0016552071319893003\n",
      "Step: 1972, Loss: 0.0022221659310162067\n",
      "Data loss: 0.000534102029632777, Function loss: 0.001682378351688385\n",
      "Step: 1973, Loss: 0.002216480439528823\n",
      "Data loss: 0.000571727636270225, Function loss: 0.0016440945910289884\n",
      "Step: 1974, Loss: 0.0022158222272992134\n",
      "Data loss: 0.0005209154915064573, Function loss: 0.0017055525677278638\n",
      "Step: 1975, Loss: 0.002226468175649643\n",
      "Data loss: 0.0005785421817563474, Function loss: 0.0016740275314077735\n",
      "Step: 1976, Loss: 0.00225256965495646\n",
      "Data loss: 0.0005103607545606792, Function loss: 0.001770889270119369\n",
      "Step: 1977, Loss: 0.0022812499664723873\n",
      "Data loss: 0.0005857215728610754, Function loss: 0.0017261839238926768\n",
      "Step: 1978, Loss: 0.0023119053803384304\n",
      "Data loss: 0.0005042899283580482, Function loss: 0.001840044278651476\n",
      "Step: 1979, Loss: 0.002344334265217185\n",
      "Data loss: 0.0005957430694252253, Function loss: 0.0017978919204324484\n",
      "Step: 1980, Loss: 0.0023936349898576736\n",
      "Data loss: 0.000502805516589433, Function loss: 0.001959770219400525\n",
      "Step: 1981, Loss: 0.002462575677782297\n",
      "Data loss: 0.000615881581325084, Function loss: 0.001949434750713408\n",
      "Step: 1982, Loss: 0.002565316390246153\n",
      "Data loss: 0.0005091670318506658, Function loss: 0.0022157023195177317\n",
      "Step: 1983, Loss: 0.0027248694095760584\n",
      "Data loss: 0.0006605993839912117, Function loss: 0.0023201098665595055\n",
      "Step: 1984, Loss: 0.002980709308758378\n",
      "Data loss: 0.000531430181581527, Function loss: 0.0028586077969521284\n",
      "Step: 1985, Loss: 0.0033900379203259945\n",
      "Data loss: 0.0007635739748366177, Function loss: 0.003259120276197791\n",
      "Step: 1986, Loss: 0.004022694192826748\n",
      "Data loss: 0.0005850255838595331, Function loss: 0.004337585996836424\n",
      "Step: 1987, Loss: 0.004922611638903618\n",
      "Data loss: 0.0009426819160580635, Function loss: 0.005166617687791586\n",
      "Step: 1988, Loss: 0.006109299603849649\n",
      "Data loss: 0.0006778492825105786, Function loss: 0.006821612827479839\n",
      "Step: 1989, Loss: 0.007499461993575096\n",
      "Data loss: 0.0011442606337368488, Function loss: 0.0077497283928096294\n",
      "Step: 1990, Loss: 0.008893989026546478\n",
      "Data loss: 0.0007344504119828343, Function loss: 0.008970447815954685\n",
      "Step: 1991, Loss: 0.009704898111522198\n",
      "Data loss: 0.0011482266709208488, Function loss: 0.008473843336105347\n",
      "Step: 1992, Loss: 0.009622070007026196\n",
      "Data loss: 0.0006019986467435956, Function loss: 0.007604369428008795\n",
      "Step: 1993, Loss: 0.008206368423998356\n",
      "Data loss: 0.0008484967402182519, Function loss: 0.0054165394976735115\n",
      "Step: 1994, Loss: 0.006265036296099424\n",
      "Data loss: 0.0004711605142802, Function loss: 0.003854871727526188\n",
      "Step: 1995, Loss: 0.0043260324746370316\n",
      "Data loss: 0.0006258853827603161, Function loss: 0.0027057945262640715\n",
      "Step: 1996, Loss: 0.0033316798508167267\n",
      "Data loss: 0.000619799189735204, Function loss: 0.002590194344520569\n",
      "Step: 1997, Loss: 0.003209993476048112\n",
      "Data loss: 0.0006033534300513566, Function loss: 0.003006999846547842\n",
      "Step: 1998, Loss: 0.0036103532183915377\n",
      "Data loss: 0.0007571775931864977, Function loss: 0.0032193127553910017\n",
      "Step: 1999, Loss: 0.003976490348577499\n",
      "Data loss: 0.0005393113824538887, Function loss: 0.0034382324665784836\n",
      "Step: 2000, Loss: 0.0039775436744093895\n",
      "Data loss: 0.0006961994804441929, Function loss: 0.0030040396377444267\n",
      "Step: 2001, Loss: 0.0037002391181886196\n",
      "Data loss: 0.0005002795369364321, Function loss: 0.002817814238369465\n",
      "Step: 2002, Loss: 0.003318093717098236\n",
      "Data loss: 0.000628722133114934, Function loss: 0.002446438418701291\n",
      "Step: 2003, Loss: 0.003075160551816225\n",
      "Data loss: 0.0005887098959647119, Function loss: 0.0023090457543730736\n",
      "Step: 2004, Loss: 0.0028977557085454464\n",
      "Data loss: 0.000585476285777986, Function loss: 0.00217742333188653\n",
      "Step: 2005, Loss: 0.002762899734079838\n",
      "Data loss: 0.0006350522744469345, Function loss: 0.0020134220831096172\n",
      "Step: 2006, Loss: 0.0026484744157642126\n",
      "Data loss: 0.0005201590829528868, Function loss: 0.00212972448207438\n",
      "Step: 2007, Loss: 0.002649883506819606\n",
      "Data loss: 0.0006304937996901572, Function loss: 0.0021788496524095535\n",
      "Step: 2008, Loss: 0.00280934339389205\n",
      "Data loss: 0.0005234299460425973, Function loss: 0.0024270382709801197\n",
      "Step: 2009, Loss: 0.002950468100607395\n",
      "Data loss: 0.0006250265869311988, Function loss: 0.0023147829342633486\n",
      "Step: 2010, Loss: 0.0029398095794022083\n",
      "Data loss: 0.000558603962417692, Function loss: 0.0021187218371778727\n",
      "Step: 2011, Loss: 0.0026773258578032255\n",
      "Data loss: 0.0005744829541072249, Function loss: 0.001773631083779037\n",
      "Step: 2012, Loss: 0.002348114037886262\n",
      "Data loss: 0.0005705183139070868, Function loss: 0.0015825992450118065\n",
      "Step: 2013, Loss: 0.002153117675334215\n",
      "Data loss: 0.0005298396572470665, Function loss: 0.001675449893809855\n",
      "Step: 2014, Loss: 0.0022052894346415997\n",
      "Data loss: 0.0005964225856587291, Function loss: 0.0018576242728158832\n",
      "Step: 2015, Loss: 0.0024540468584746122\n",
      "Data loss: 0.0005259728641249239, Function loss: 0.0021931000519543886\n",
      "Step: 2016, Loss: 0.0027190728578716516\n",
      "Data loss: 0.0006174432928673923, Function loss: 0.0022416345309466124\n",
      "Step: 2017, Loss: 0.0028590778820216656\n",
      "Data loss: 0.0005215476849116385, Function loss: 0.002267907140776515\n",
      "Step: 2018, Loss: 0.0027894547674804926\n",
      "Data loss: 0.0006042213644832373, Function loss: 0.002002193359658122\n",
      "Step: 2019, Loss: 0.0026064147241413593\n",
      "Data loss: 0.000511827296577394, Function loss: 0.001870996318757534\n",
      "Step: 2020, Loss: 0.00238282373175025\n",
      "Data loss: 0.0005830506561324, Function loss: 0.001646472024731338\n",
      "Step: 2021, Loss: 0.002229522680863738\n",
      "Data loss: 0.0005262300255708396, Function loss: 0.0016463851789012551\n",
      "Step: 2022, Loss: 0.002172615146264434\n",
      "Data loss: 0.0005631032981909811, Function loss: 0.0016074128216132522\n",
      "Step: 2023, Loss: 0.0021705161780118942\n",
      "Data loss: 0.0005494809010997415, Function loss: 0.001629460952244699\n",
      "Step: 2024, Loss: 0.0021789418533444405\n",
      "Data loss: 0.0005317263421602547, Function loss: 0.0016478918259963393\n",
      "Step: 2025, Loss: 0.002179618226364255\n",
      "Data loss: 0.0005667580408044159, Function loss: 0.0016241465928032994\n",
      "Step: 2026, Loss: 0.0021909046918153763\n",
      "Data loss: 0.0005079965339973569, Function loss: 0.0017095176735892892\n",
      "Step: 2027, Loss: 0.002217514207586646\n",
      "Data loss: 0.0005851552123203874, Function loss: 0.00168395577929914\n",
      "Step: 2028, Loss: 0.002269111108034849\n",
      "Data loss: 0.0005028807208873332, Function loss: 0.001804513856768608\n",
      "Step: 2029, Loss: 0.0023073945194482803\n",
      "Data loss: 0.0005901192198507488, Function loss: 0.0017178213456645608\n",
      "Step: 2030, Loss: 0.0023079405073076487\n",
      "Data loss: 0.0005028191953897476, Function loss: 0.0017576960381120443\n",
      "Step: 2031, Loss: 0.002260515233501792\n",
      "Data loss: 0.0005715790903195739, Function loss: 0.0016214745119214058\n",
      "Step: 2032, Loss: 0.0021930537186563015\n",
      "Data loss: 0.0005069742328487337, Function loss: 0.0016272157663479447\n",
      "Step: 2033, Loss: 0.0021341899409890175\n",
      "Data loss: 0.0005497320089489222, Function loss: 0.0015625597443431616\n",
      "Step: 2034, Loss: 0.0021122917532920837\n",
      "Data loss: 0.0005184884648770094, Function loss: 0.001601127558387816\n",
      "Step: 2035, Loss: 0.002119616139680147\n",
      "Data loss: 0.0005404995172284544, Function loss: 0.00160393922124058\n",
      "Step: 2036, Loss: 0.0021444386802613735\n",
      "Data loss: 0.0005253661656752229, Function loss: 0.0016385726630687714\n",
      "Step: 2037, Loss: 0.0021639387123286724\n",
      "Data loss: 0.0005397443892434239, Function loss: 0.0016346544725820422\n",
      "Step: 2038, Loss: 0.002174398861825466\n",
      "Data loss: 0.0005180885782465339, Function loss: 0.0016516756732016802\n",
      "Step: 2039, Loss: 0.0021697641350328922\n",
      "Data loss: 0.000544618465937674, Function loss: 0.0016243573045358062\n",
      "Step: 2040, Loss: 0.0021689757704734802\n",
      "Data loss: 0.0005016876966692507, Function loss: 0.0016777390846982598\n",
      "Step: 2041, Loss: 0.0021794268395751715\n",
      "Data loss: 0.0005542818107642233, Function loss: 0.0016477544559165835\n",
      "Step: 2042, Loss: 0.002202036324888468\n",
      "Data loss: 0.0004895353340543807, Function loss: 0.0017336859600618482\n",
      "Step: 2043, Loss: 0.0022232213523238897\n",
      "Data loss: 0.0005615251720882952, Function loss: 0.0016894667642191052\n",
      "Step: 2044, Loss: 0.0022509919945150614\n",
      "Data loss: 0.00048243513447232544, Function loss: 0.0017923491541296244\n",
      "Step: 2045, Loss: 0.0022747842594981194\n",
      "Data loss: 0.0005665004136972129, Function loss: 0.0017412527231499553\n",
      "Step: 2046, Loss: 0.0023077530786395073\n",
      "Data loss: 0.00047750980593264103, Function loss: 0.0018598646856844425\n",
      "Step: 2047, Loss: 0.0023373744916170835\n",
      "Data loss: 0.0005682178889401257, Function loss: 0.0017985592130571604\n",
      "Step: 2048, Loss: 0.002366777043789625\n",
      "Data loss: 0.0004757967253681272, Function loss: 0.001908959704451263\n",
      "Step: 2049, Loss: 0.0023847564589232206\n",
      "Data loss: 0.0005656280554831028, Function loss: 0.0018357180524617434\n",
      "Step: 2050, Loss: 0.002401346107944846\n",
      "Data loss: 0.00047755977720953524, Function loss: 0.001915696426294744\n",
      "Step: 2051, Loss: 0.002393256174400449\n",
      "Data loss: 0.0005581862642429769, Function loss: 0.0018174670403823256\n",
      "Step: 2052, Loss: 0.0023756532464176416\n",
      "Data loss: 0.00048468762543052435, Function loss: 0.0018471446819603443\n",
      "Step: 2053, Loss: 0.0023318324238061905\n",
      "Data loss: 0.000545110204257071, Function loss: 0.0017392777372151613\n",
      "Step: 2054, Loss: 0.002284388057887554\n",
      "Data loss: 0.0004946127301082015, Function loss: 0.0017295898869633675\n",
      "Step: 2055, Loss: 0.002224202733486891\n",
      "Data loss: 0.0005297644529491663, Function loss: 0.0016352698439732194\n",
      "Step: 2056, Loss: 0.0021650344133377075\n",
      "Data loss: 0.0005010960157960653, Function loss: 0.0016126370755955577\n",
      "Step: 2057, Loss: 0.002113732974976301\n",
      "Data loss: 0.0005186456837691367, Function loss: 0.0015595612348988652\n",
      "Step: 2058, Loss: 0.002078206976875663\n",
      "Data loss: 0.0005018335650674999, Function loss: 0.0015498806023970246\n",
      "Step: 2059, Loss: 0.0020517141092568636\n",
      "Data loss: 0.0005138338310644031, Function loss: 0.0015233184676617384\n",
      "Step: 2060, Loss: 0.0020371521823108196\n",
      "Data loss: 0.0004989159060642123, Function loss: 0.0015262248925864697\n",
      "Step: 2061, Loss: 0.002025140915066004\n",
      "Data loss: 0.000512561178766191, Function loss: 0.0015058960998430848\n",
      "Step: 2062, Loss: 0.002018457278609276\n",
      "Data loss: 0.0004958402714692056, Function loss: 0.0015146482037380338\n",
      "Step: 2063, Loss: 0.0020104884169995785\n",
      "Data loss: 0.0005119295092299581, Function loss: 0.0014932156773284078\n",
      "Step: 2064, Loss: 0.002005145186558366\n",
      "Data loss: 0.0004934243625029922, Function loss: 0.0015091997338458896\n",
      "Step: 2065, Loss: 0.0020026240963488817\n",
      "Data loss: 0.0005112172802910209, Function loss: 0.0014864675467833877\n",
      "Step: 2066, Loss: 0.0019976848270744085\n",
      "Data loss: 0.0004930342547595501, Function loss: 0.0015040780417621136\n",
      "Step: 2067, Loss: 0.0019971122965216637\n",
      "Data loss: 0.0005080971750430763, Function loss: 0.001487275119870901\n",
      "Step: 2068, Loss: 0.0019953723531216383\n",
      "Data loss: 0.0004947216366417706, Function loss: 0.0015003456501290202\n",
      "Step: 2069, Loss: 0.0019950673449784517\n",
      "Data loss: 0.0005043513374403119, Function loss: 0.001491105416789651\n",
      "Step: 2070, Loss: 0.0019954568706452847\n",
      "Data loss: 0.0004961129743605852, Function loss: 0.0014965059235692024\n",
      "Step: 2071, Loss: 0.0019926188979297876\n",
      "Data loss: 0.0005000160890631378, Function loss: 0.0014902225229889154\n",
      "Step: 2072, Loss: 0.001990238670259714\n",
      "Data loss: 0.0004966774140484631, Function loss: 0.0014909220626577735\n",
      "Step: 2073, Loss: 0.0019875995349138975\n",
      "Data loss: 0.0004961497616022825, Function loss: 0.0014874296030029655\n",
      "Step: 2074, Loss: 0.001983579248189926\n",
      "Data loss: 0.0004968515131622553, Function loss: 0.0014844752149656415\n",
      "Step: 2075, Loss: 0.0019813268445432186\n",
      "Data loss: 0.0004930237191729248, Function loss: 0.0014860990922898054\n",
      "Step: 2076, Loss: 0.001979122869670391\n",
      "Data loss: 0.0004980385419912636, Function loss: 0.0014819243224337697\n",
      "Step: 2077, Loss: 0.0019799629226326942\n",
      "Data loss: 0.0004884342197328806, Function loss: 0.001491769915446639\n",
      "Step: 2078, Loss: 0.0019802041351795197\n",
      "Data loss: 0.0005014599300920963, Function loss: 0.0014851982705295086\n",
      "Step: 2079, Loss: 0.001986658200621605\n",
      "Data loss: 0.00048223105841316283, Function loss: 0.0015135103603824973\n",
      "Step: 2080, Loss: 0.0019957413896918297\n",
      "Data loss: 0.0005078293615952134, Function loss: 0.0015066667692735791\n",
      "Step: 2081, Loss: 0.0020144961308687925\n",
      "Data loss: 0.000472368294140324, Function loss: 0.0015861832071095705\n",
      "Step: 2082, Loss: 0.002058551413938403\n",
      "Data loss: 0.0005223334883339703, Function loss: 0.0016186563298106194\n",
      "Step: 2083, Loss: 0.0021409897599369287\n",
      "Data loss: 0.00045847659930586815, Function loss: 0.001830221270211041\n",
      "Step: 2084, Loss: 0.0022886977531015873\n",
      "Data loss: 0.0005523903528228402, Function loss: 0.002002747729420662\n",
      "Step: 2085, Loss: 0.0025551379658281803\n",
      "Data loss: 0.000442869815742597, Function loss: 0.0025792778469622135\n",
      "Step: 2086, Loss: 0.003022147575393319\n",
      "Data loss: 0.0006193295121192932, Function loss: 0.0033168087247759104\n",
      "Step: 2087, Loss: 0.00393613800406456\n",
      "Data loss: 0.00044015669845975935, Function loss: 0.005082670133560896\n",
      "Step: 2088, Loss: 0.005522826686501503\n",
      "Data loss: 0.000787975499406457, Function loss: 0.007878548465669155\n",
      "Step: 2089, Loss: 0.008666523732244968\n",
      "Data loss: 0.0005096585373394191, Function loss: 0.012889813631772995\n",
      "Step: 2090, Loss: 0.013399472460150719\n",
      "Data loss: 0.001191269257105887, Function loss: 0.02118740603327751\n",
      "Step: 2091, Loss: 0.022378675639629364\n",
      "Data loss: 0.000749792845454067, Function loss: 0.029954228550195694\n",
      "Step: 2092, Loss: 0.030704021453857422\n",
      "Data loss: 0.0017634873511269689, Function loss: 0.04215606674551964\n",
      "Step: 2093, Loss: 0.043919555842876434\n",
      "Data loss: 0.000945298932492733, Function loss: 0.03978575021028519\n",
      "Step: 2094, Loss: 0.040731050074100494\n",
      "Data loss: 0.0014834897592663765, Function loss: 0.031002068892121315\n",
      "Step: 2095, Loss: 0.032485559582710266\n",
      "Data loss: 0.0006106216460466385, Function loss: 0.010328229516744614\n",
      "Step: 2096, Loss: 0.010938851162791252\n",
      "Data loss: 0.0006155365263111889, Function loss: 0.0015408386243507266\n",
      "Step: 2097, Loss: 0.0021563752088695765\n",
      "Data loss: 0.0009626484825275838, Function loss: 0.009045270271599293\n",
      "Step: 2098, Loss: 0.010007918812334538\n",
      "Data loss: 0.0008363228407688439, Function loss: 0.017106689512729645\n",
      "Step: 2099, Loss: 0.017943011596798897\n",
      "Data loss: 0.0011806642869487405, Function loss: 0.013700387440621853\n",
      "Step: 2100, Loss: 0.014881052076816559\n",
      "Data loss: 0.0007535215117968619, Function loss: 0.0032013433519750834\n",
      "Step: 2101, Loss: 0.003954865038394928\n",
      "Data loss: 0.0007923991070128977, Function loss: 0.003336177906021476\n",
      "Step: 2102, Loss: 0.004128577187657356\n",
      "Data loss: 0.0011901080142706633, Function loss: 0.010123590007424355\n",
      "Step: 2103, Loss: 0.011313698254525661\n",
      "Data loss: 0.0008865449926815927, Function loss: 0.008221447467803955\n",
      "Step: 2104, Loss: 0.009107992053031921\n",
      "Data loss: 0.0009544708300381899, Function loss: 0.0020604208111763\n",
      "Step: 2105, Loss: 0.00301489164121449\n",
      "Data loss: 0.001006521750241518, Function loss: 0.0029983872082084417\n",
      "Step: 2106, Loss: 0.004004908725619316\n",
      "Data loss: 0.0009129475220106542, Function loss: 0.007013233844190836\n",
      "Step: 2107, Loss: 0.007926180958747864\n",
      "Data loss: 0.0011155800893902779, Function loss: 0.005042752716690302\n",
      "Step: 2108, Loss: 0.00615833280608058\n",
      "Data loss: 0.0009207134135067463, Function loss: 0.0015609489055350423\n",
      "Step: 2109, Loss: 0.0024816622026264668\n",
      "Data loss: 0.0009136921144090593, Function loss: 0.003343450604006648\n",
      "Step: 2110, Loss: 0.004257142543792725\n",
      "Data loss: 0.0011348037514835596, Function loss: 0.005074758548289537\n",
      "Step: 2111, Loss: 0.006209562532603741\n",
      "Data loss: 0.0009124118951149285, Function loss: 0.0026787540409713984\n",
      "Step: 2112, Loss: 0.003591165877878666\n",
      "Data loss: 0.0009247365524061024, Function loss: 0.001569562591612339\n",
      "Step: 2113, Loss: 0.0024942990858107805\n",
      "Data loss: 0.0010651281336322427, Function loss: 0.0034226211719214916\n",
      "Step: 2114, Loss: 0.0044877491891384125\n",
      "Data loss: 0.0008834268664941192, Function loss: 0.003466612659394741\n",
      "Step: 2115, Loss: 0.004350039642304182\n",
      "Data loss: 0.0009430819191038609, Function loss: 0.0015659915516152978\n",
      "Step: 2116, Loss: 0.002509073354303837\n",
      "Data loss: 0.0009504158515483141, Function loss: 0.0017753092106431723\n",
      "Step: 2117, Loss: 0.0027257250621914864\n",
      "Data loss: 0.000836642284411937, Function loss: 0.0030583597254008055\n",
      "Step: 2118, Loss: 0.0038950019516050816\n",
      "Data loss: 0.0009617739706300199, Function loss: 0.002289977390319109\n",
      "Step: 2119, Loss: 0.0032517514191567898\n",
      "Data loss: 0.0008510769112035632, Function loss: 0.001352691208012402\n",
      "Step: 2120, Loss: 0.0022037681192159653\n",
      "Data loss: 0.0007962323143146932, Function loss: 0.0020122407004237175\n",
      "Step: 2121, Loss: 0.0028084730729460716\n",
      "Data loss: 0.0009398098918609321, Function loss: 0.002512734616175294\n",
      "Step: 2122, Loss: 0.003452544566243887\n",
      "Data loss: 0.0007577010546810925, Function loss: 0.0019439649768173695\n",
      "Step: 2123, Loss: 0.002701665973290801\n",
      "Data loss: 0.0007888228865340352, Function loss: 0.0013280417770147324\n",
      "Step: 2124, Loss: 0.0021168645471334457\n",
      "Data loss: 0.0008449084707535803, Function loss: 0.0017647980712354183\n",
      "Step: 2125, Loss: 0.0026097064837813377\n",
      "Data loss: 0.0006994259310886264, Function loss: 0.0022548846900463104\n",
      "Step: 2126, Loss: 0.0029543107375502586\n",
      "Data loss: 0.0008135945536196232, Function loss: 0.0016344202449545264\n",
      "Step: 2127, Loss: 0.0024480149149894714\n",
      "Data loss: 0.0007402266492135823, Function loss: 0.0013283162843436003\n",
      "Step: 2128, Loss: 0.0020685428753495216\n",
      "Data loss: 0.0006827761535532773, Function loss: 0.001733517972752452\n",
      "Step: 2129, Loss: 0.0024162940680980682\n",
      "Data loss: 0.0008071116171777248, Function loss: 0.0018753176555037498\n",
      "Step: 2130, Loss: 0.0026824292726814747\n",
      "Data loss: 0.0006627533584833145, Function loss: 0.0016770103247836232\n",
      "Step: 2131, Loss: 0.002339763566851616\n",
      "Data loss: 0.0007055627065710723, Function loss: 0.0013268032344058156\n",
      "Step: 2132, Loss: 0.002032365882769227\n",
      "Data loss: 0.0007374626002274454, Function loss: 0.0014849469298496842\n",
      "Step: 2133, Loss: 0.0022224094718694687\n",
      "Data loss: 0.0006301620742306113, Function loss: 0.0018017570255324244\n",
      "Step: 2134, Loss: 0.0024319190997630358\n",
      "Data loss: 0.0007307728519663215, Function loss: 0.0015160281909629703\n",
      "Step: 2135, Loss: 0.0022468010429292917\n",
      "Data loss: 0.0006583190406672657, Function loss: 0.0013526598922908306\n",
      "Step: 2136, Loss: 0.002010978991165757\n",
      "Data loss: 0.0006374318618327379, Function loss: 0.001466057961806655\n",
      "Step: 2137, Loss: 0.002103489823639393\n",
      "Data loss: 0.0007099458598531783, Function loss: 0.0015430301427841187\n",
      "Step: 2138, Loss: 0.002252975944429636\n",
      "Data loss: 0.0006133307470008731, Function loss: 0.0015348895685747266\n",
      "Step: 2139, Loss: 0.0021482203155755997\n",
      "Data loss: 0.0006575271836481988, Function loss: 0.0013324308674782515\n",
      "Step: 2140, Loss: 0.001989958109334111\n",
      "Data loss: 0.0006584524526260793, Function loss: 0.0013734399108216166\n",
      "Step: 2141, Loss: 0.002031892305240035\n",
      "Data loss: 0.0005996378022246063, Function loss: 0.0015489981742575765\n",
      "Step: 2142, Loss: 0.002148635918274522\n",
      "Data loss: 0.0006673229509033263, Function loss: 0.001457361038774252\n",
      "Step: 2143, Loss: 0.0021246839314699173\n",
      "Data loss: 0.0006057493737898767, Function loss: 0.0013916664756834507\n",
      "Step: 2144, Loss: 0.0019974159076809883\n",
      "Data loss: 0.0006102924235165119, Function loss: 0.0013447314267978072\n",
      "Step: 2145, Loss: 0.001955023966729641\n",
      "Data loss: 0.0006411317153833807, Function loss: 0.0013839075108990073\n",
      "Step: 2146, Loss: 0.002025039168074727\n",
      "Data loss: 0.0005753687000833452, Function loss: 0.0015087418723851442\n",
      "Step: 2147, Loss: 0.0020841106306761503\n",
      "Data loss: 0.0006339786923490465, Function loss: 0.0014089306350797415\n",
      "Step: 2148, Loss: 0.002042909385636449\n",
      "Data loss: 0.0005860251258127391, Function loss: 0.0013675516238436103\n",
      "Step: 2149, Loss: 0.0019535766914486885\n",
      "Data loss: 0.0005857525975443423, Function loss: 0.0013512480072677135\n",
      "Step: 2150, Loss: 0.001937000546604395\n",
      "Data loss: 0.0006118880701251328, Function loss: 0.0013699629344046116\n",
      "Step: 2151, Loss: 0.0019818509463220835\n",
      "Data loss: 0.0005610016523860395, Function loss: 0.001439196988940239\n",
      "Step: 2152, Loss: 0.0020001986995339394\n",
      "Data loss: 0.0005987425101920962, Function loss: 0.0013621330726891756\n",
      "Step: 2153, Loss: 0.00196087546646595\n",
      "Data loss: 0.000570174481254071, Function loss: 0.001347687328234315\n",
      "Step: 2154, Loss: 0.0019178618676960468\n",
      "Data loss: 0.0005649308441206813, Function loss: 0.001349091064184904\n",
      "Step: 2155, Loss: 0.0019140219083055854\n",
      "Data loss: 0.0005827746936120093, Function loss: 0.0013548682909458876\n",
      "Step: 2156, Loss: 0.001937642926350236\n",
      "Data loss: 0.0005483212880790234, Function loss: 0.0013957768678665161\n",
      "Step: 2157, Loss: 0.0019440981559455395\n",
      "Data loss: 0.0005715671577490866, Function loss: 0.0013515707105398178\n",
      "Step: 2158, Loss: 0.0019231378100812435\n",
      "Data loss: 0.00055424973834306, Function loss: 0.001347634824924171\n",
      "Step: 2159, Loss: 0.001901884563267231\n",
      "Data loss: 0.0005464469431899488, Function loss: 0.0013513662852346897\n",
      "Step: 2160, Loss: 0.0018978132866322994\n",
      "Data loss: 0.0005630142404697835, Function loss: 0.0013438506284728646\n",
      "Step: 2161, Loss: 0.0019068648107349873\n",
      "Data loss: 0.0005333629087544978, Function loss: 0.0013745618052780628\n",
      "Step: 2162, Loss: 0.0019079246558248997\n",
      "Data loss: 0.0005526752211153507, Function loss: 0.001341072958894074\n",
      "Step: 2163, Loss: 0.0018937481800094247\n",
      "Data loss: 0.0005390519509091973, Function loss: 0.0013427158119156957\n",
      "Step: 2164, Loss: 0.001881767762824893\n",
      "Data loss: 0.0005314340232871473, Function loss: 0.0013541484950110316\n",
      "Step: 2165, Loss: 0.001885582460090518\n",
      "Data loss: 0.0005457974039018154, Function loss: 0.0013459897600114346\n",
      "Step: 2166, Loss: 0.00189178716391325\n",
      "Data loss: 0.0005204838234931231, Function loss: 0.0013705429155379534\n",
      "Step: 2167, Loss: 0.0018910267390310764\n",
      "Data loss: 0.0005395314656198025, Function loss: 0.0013442662311717868\n",
      "Step: 2168, Loss: 0.0018837976967915893\n",
      "Data loss: 0.0005205092602409422, Function loss: 0.0013572911266237497\n",
      "Step: 2169, Loss: 0.001877800328657031\n",
      "Data loss: 0.0005275378352962434, Function loss: 0.001347101991996169\n",
      "Step: 2170, Loss: 0.0018746398855000734\n",
      "Data loss: 0.0005222699837759137, Function loss: 0.0013514888705685735\n",
      "Step: 2171, Loss: 0.0018737588543444872\n",
      "Data loss: 0.0005179886939004064, Function loss: 0.0013538956409320235\n",
      "Step: 2172, Loss: 0.0018718843348324299\n",
      "Data loss: 0.0005194790428504348, Function loss: 0.0013466650852933526\n",
      "Step: 2173, Loss: 0.0018661441281437874\n",
      "Data loss: 0.0005131304496899247, Function loss: 0.0013482565991580486\n",
      "Step: 2174, Loss: 0.0018613870488479733\n",
      "Data loss: 0.0005142504232935607, Function loss: 0.0013458174653351307\n",
      "Step: 2175, Loss: 0.0018600679468363523\n",
      "Data loss: 0.0005101619753986597, Function loss: 0.0013512296136468649\n",
      "Step: 2176, Loss: 0.0018613915890455246\n",
      "Data loss: 0.0005111614591442049, Function loss: 0.0013526901602745056\n",
      "Step: 2177, Loss: 0.0018638516776263714\n",
      "Data loss: 0.0005052420310676098, Function loss: 0.0013582607498392463\n",
      "Step: 2178, Loss: 0.001863502780906856\n",
      "Data loss: 0.000509797828271985, Function loss: 0.001351678860373795\n",
      "Step: 2179, Loss: 0.00186147668864578\n",
      "Data loss: 0.0004994720802642405, Function loss: 0.0013598583173006773\n",
      "Step: 2180, Loss: 0.0018593303393572569\n",
      "Data loss: 0.0005079220281913877, Function loss: 0.0013484257506206632\n",
      "Step: 2181, Loss: 0.0018563477788120508\n",
      "Data loss: 0.0004943067324347794, Function loss: 0.001357273431494832\n",
      "Step: 2182, Loss: 0.0018515801057219505\n",
      "Data loss: 0.0005053862696513534, Function loss: 0.0013428047532215714\n",
      "Step: 2183, Loss: 0.0018481910228729248\n",
      "Data loss: 0.000491517421323806, Function loss: 0.0013535073958337307\n",
      "Step: 2184, Loss: 0.0018450247589498758\n",
      "Data loss: 0.000500128255225718, Function loss: 0.0013414488639682531\n",
      "Step: 2185, Loss: 0.0018415771191939712\n",
      "Data loss: 0.000491845712531358, Function loss: 0.001347126322798431\n",
      "Step: 2186, Loss: 0.0018389720935374498\n",
      "Data loss: 0.0004934280877932906, Function loss: 0.0013430193066596985\n",
      "Step: 2187, Loss: 0.001836447394452989\n",
      "Data loss: 0.0004928437410853803, Function loss: 0.0013425175566226244\n",
      "Step: 2188, Loss: 0.0018353613559156656\n",
      "Data loss: 0.0004863948852289468, Function loss: 0.0013484443770721555\n",
      "Step: 2189, Loss: 0.0018348392331972718\n",
      "Data loss: 0.0004945890977978706, Function loss: 0.0013403001939877868\n",
      "Step: 2190, Loss: 0.0018348892917856574\n",
      "Data loss: 0.0004814291896764189, Function loss: 0.001355102052912116\n",
      "Step: 2191, Loss: 0.0018365312134847045\n",
      "Data loss: 0.0004935279139317572, Function loss: 0.001341907656751573\n",
      "Step: 2192, Loss: 0.0018354356288909912\n",
      "Data loss: 0.0004788359219674021, Function loss: 0.001354750944301486\n",
      "Step: 2193, Loss: 0.0018335868371650577\n",
      "Data loss: 0.000490334932692349, Function loss: 0.0013404891360551119\n",
      "Step: 2194, Loss: 0.0018308240687474608\n",
      "Data loss: 0.00047799362801015377, Function loss: 0.0013489931588992476\n",
      "Step: 2195, Loss: 0.0018269867869094014\n",
      "Data loss: 0.0004852282290812582, Function loss: 0.0013376877177506685\n",
      "Step: 2196, Loss: 0.0018229159759357572\n",
      "Data loss: 0.0004787738434970379, Function loss: 0.0013401552569121122\n",
      "Step: 2197, Loss: 0.0018189291004091501\n",
      "Data loss: 0.0004794510023202747, Function loss: 0.0013374912086874247\n",
      "Step: 2198, Loss: 0.0018169422401115298\n",
      "Data loss: 0.00048083014553412795, Function loss: 0.0013352667447179556\n",
      "Step: 2199, Loss: 0.0018160969484597445\n",
      "Data loss: 0.00047372590051963925, Function loss: 0.0013424326898530126\n",
      "Step: 2200, Loss: 0.0018161586485803127\n",
      "Data loss: 0.0004819376626983285, Function loss: 0.0013333236565813422\n",
      "Step: 2201, Loss: 0.0018152613192796707\n",
      "Data loss: 0.00047110108425840735, Function loss: 0.0013435319997370243\n",
      "Step: 2202, Loss: 0.0018146331422030926\n",
      "Data loss: 0.0004805077041964978, Function loss: 0.0013342673191800714\n",
      "Step: 2203, Loss: 0.0018147750524803996\n",
      "Data loss: 0.0004699233395513147, Function loss: 0.0013445154763758183\n",
      "Step: 2204, Loss: 0.0018144388450309634\n",
      "Data loss: 0.000478545407531783, Function loss: 0.001336999936029315\n",
      "Step: 2205, Loss: 0.0018155453726649284\n",
      "Data loss: 0.0004688254848588258, Function loss: 0.0013494044542312622\n",
      "Step: 2206, Loss: 0.0018182299099862576\n",
      "Data loss: 0.000478237634524703, Function loss: 0.0013417020672932267\n",
      "Step: 2207, Loss: 0.0018199397018179297\n",
      "Data loss: 0.00046644790563732386, Function loss: 0.0013572359457612038\n",
      "Step: 2208, Loss: 0.0018236838513985276\n",
      "Data loss: 0.00047929564607329667, Function loss: 0.0013455147854983807\n",
      "Step: 2209, Loss: 0.0018248104024678469\n",
      "Data loss: 0.0004636644443962723, Function loss: 0.0013661927077919245\n",
      "Step: 2210, Loss: 0.0018298571230843663\n",
      "Data loss: 0.00047977195936255157, Function loss: 0.0013532586162909865\n",
      "Step: 2211, Loss: 0.0018330306047573686\n",
      "Data loss: 0.000462490163045004, Function loss: 0.001377715147100389\n",
      "Step: 2212, Loss: 0.0018402052810415626\n",
      "Data loss: 0.00047891371650621295, Function loss: 0.0013622051337733865\n",
      "Step: 2213, Loss: 0.0018411187920719385\n",
      "Data loss: 0.0004617373051587492, Function loss: 0.001379199791699648\n",
      "Step: 2214, Loss: 0.0018409370677545667\n",
      "Data loss: 0.0004782473261002451, Function loss: 0.0013589446898549795\n",
      "Step: 2215, Loss: 0.0018371919868513942\n",
      "Data loss: 0.00045843576663173735, Function loss: 0.0013756088446825743\n",
      "Step: 2216, Loss: 0.0018340445822104812\n",
      "Data loss: 0.00047826653462834656, Function loss: 0.0013515999307855964\n",
      "Step: 2217, Loss: 0.0018298664363101125\n",
      "Data loss: 0.00045630725799128413, Function loss: 0.0013705454766750336\n",
      "Step: 2218, Loss: 0.0018268527928739786\n",
      "Data loss: 0.00047635677037760615, Function loss: 0.0013457208406180143\n",
      "Step: 2219, Loss: 0.0018220776692032814\n",
      "Data loss: 0.00045585649786517024, Function loss: 0.001366276410408318\n",
      "Step: 2220, Loss: 0.0018221328500658274\n",
      "Data loss: 0.00047545740380883217, Function loss: 0.0013496195897459984\n",
      "Step: 2221, Loss: 0.0018250769935548306\n",
      "Data loss: 0.0004557811189442873, Function loss: 0.0013724530581384897\n",
      "Step: 2222, Loss: 0.001828234177082777\n",
      "Data loss: 0.000474659405881539, Function loss: 0.0013592108152806759\n",
      "Step: 2223, Loss: 0.0018338701920583844\n",
      "Data loss: 0.0004566456482280046, Function loss: 0.0013850826071575284\n",
      "Step: 2224, Loss: 0.0018417282262817025\n",
      "Data loss: 0.0004747749480884522, Function loss: 0.0013790681259706616\n",
      "Step: 2225, Loss: 0.0018538431031629443\n",
      "Data loss: 0.0004587278817780316, Function loss: 0.001411703764460981\n",
      "Step: 2226, Loss: 0.0018704317044466734\n",
      "Data loss: 0.0004763575561810285, Function loss: 0.0014186517801135778\n",
      "Step: 2227, Loss: 0.0018950093071907759\n",
      "Data loss: 0.0004600976244546473, Function loss: 0.001467162393964827\n",
      "Step: 2228, Loss: 0.0019272600766271353\n",
      "Data loss: 0.0004863882786594331, Function loss: 0.0014883115654811263\n",
      "Step: 2229, Loss: 0.0019746997859328985\n",
      "Data loss: 0.00046133986325003207, Function loss: 0.0015843661967664957\n",
      "Step: 2230, Loss: 0.002045706147328019\n",
      "Data loss: 0.0005075078224763274, Function loss: 0.0016342246672138572\n",
      "Step: 2231, Loss: 0.0021417324896901846\n",
      "Data loss: 0.000468072626972571, Function loss: 0.0018006409518420696\n",
      "Step: 2232, Loss: 0.002268713666126132\n",
      "Data loss: 0.000540437875315547, Function loss: 0.0019011760596185923\n",
      "Step: 2233, Loss: 0.0024416139349341393\n",
      "Data loss: 0.00048820720985531807, Function loss: 0.0022088992409408092\n",
      "Step: 2234, Loss: 0.0026971064507961273\n",
      "Data loss: 0.0005998095730319619, Function loss: 0.0024506081826984882\n",
      "Step: 2235, Loss: 0.0030504176393151283\n",
      "Data loss: 0.0005349690327420831, Function loss: 0.002988925902172923\n",
      "Step: 2236, Loss: 0.003523895051330328\n",
      "Data loss: 0.0006948079098947346, Function loss: 0.0034184337127953768\n",
      "Step: 2237, Loss: 0.0041132415644824505\n",
      "Data loss: 0.0006170769338496029, Function loss: 0.0041749258525669575\n",
      "Step: 2238, Loss: 0.004792002961039543\n",
      "Data loss: 0.0008096641977317631, Function loss: 0.00467646773904562\n",
      "Step: 2239, Loss: 0.005486132111400366\n",
      "Data loss: 0.0007093935273587704, Function loss: 0.005368546582758427\n",
      "Step: 2240, Loss: 0.006077940110117197\n",
      "Data loss: 0.0008695542346686125, Function loss: 0.005437255837023258\n",
      "Step: 2241, Loss: 0.006306810304522514\n",
      "Data loss: 0.0007139415829442441, Function loss: 0.005273260176181793\n",
      "Step: 2242, Loss: 0.005987201817333698\n",
      "Data loss: 0.0007464890368282795, Function loss: 0.004345310851931572\n",
      "Step: 2243, Loss: 0.0050917998887598515\n",
      "Data loss: 0.0005891529726795852, Function loss: 0.0033385073766112328\n",
      "Step: 2244, Loss: 0.003927660174667835\n",
      "Data loss: 0.0005268839886412024, Function loss: 0.002333590527996421\n",
      "Step: 2245, Loss: 0.0028604744002223015\n",
      "Data loss: 0.0004999588709324598, Function loss: 0.001734196557663381\n",
      "Step: 2246, Loss: 0.002234155312180519\n",
      "Data loss: 0.0004335286212153733, Function loss: 0.001669437624514103\n",
      "Step: 2247, Loss: 0.0021029661875218153\n",
      "Data loss: 0.0005395415937528014, Function loss: 0.001782421488314867\n",
      "Step: 2248, Loss: 0.0023219631984829903\n",
      "Data loss: 0.00047418512986041605, Function loss: 0.002175562782213092\n",
      "Step: 2249, Loss: 0.0026497479993849993\n",
      "Data loss: 0.00059296510880813, Function loss: 0.002265321323648095\n",
      "Step: 2250, Loss: 0.002858286490663886\n",
      "Data loss: 0.0005139143322594464, Function loss: 0.002321020234376192\n",
      "Step: 2251, Loss: 0.0028349345084279776\n",
      "Data loss: 0.0005497468519024551, Function loss: 0.002061427803710103\n",
      "Step: 2252, Loss: 0.002611174713820219\n",
      "Data loss: 0.0005037891678512096, Function loss: 0.001823742059059441\n",
      "Step: 2253, Loss: 0.002327531110495329\n",
      "Data loss: 0.0004647929745260626, Function loss: 0.0016839236486703157\n",
      "Step: 2254, Loss: 0.0021487167105078697\n",
      "Data loss: 0.0005101854912936687, Function loss: 0.0016335122054442763\n",
      "Step: 2255, Loss: 0.002143697813153267\n",
      "Data loss: 0.0004358682199381292, Function loss: 0.0017890926683321595\n",
      "Step: 2256, Loss: 0.002224960830062628\n",
      "Data loss: 0.0005369705031625926, Function loss: 0.0017608844209462404\n",
      "Step: 2257, Loss: 0.002297854982316494\n",
      "Data loss: 0.00044761758181266487, Function loss: 0.0018429869087412953\n",
      "Step: 2258, Loss: 0.002290604403242469\n",
      "Data loss: 0.0005361667135730386, Function loss: 0.0016701689455658197\n",
      "Step: 2259, Loss: 0.00220633577555418\n",
      "Data loss: 0.00046098095481283963, Function loss: 0.0016089924611151218\n",
      "Step: 2260, Loss: 0.002069973386824131\n",
      "Data loss: 0.0005003989790566266, Function loss: 0.0014313163701444864\n",
      "Step: 2261, Loss: 0.001931715290993452\n",
      "Data loss: 0.00047258095582947135, Function loss: 0.0013731512008234859\n",
      "Step: 2262, Loss: 0.0018457320984452963\n",
      "Data loss: 0.00046014940016902983, Function loss: 0.00137435644865036\n",
      "Step: 2263, Loss: 0.0018345058197155595\n",
      "Data loss: 0.0004937212215736508, Function loss: 0.001395312836393714\n",
      "Step: 2264, Loss: 0.0018890340579673648\n",
      "Data loss: 0.00044044575770385563, Function loss: 0.001529964618384838\n",
      "Step: 2265, Loss: 0.0019704103469848633\n",
      "Data loss: 0.0005152146331965923, Function loss: 0.001535999123007059\n",
      "Step: 2266, Loss: 0.0020512137562036514\n",
      "Data loss: 0.00043708219891414046, Function loss: 0.001655851025134325\n",
      "Step: 2267, Loss: 0.0020929332822561264\n",
      "Data loss: 0.0005211564130149782, Function loss: 0.00156548956874758\n",
      "Step: 2268, Loss: 0.0020866459235548973\n",
      "Data loss: 0.0004393544513732195, Function loss: 0.001586661208420992\n",
      "Step: 2269, Loss: 0.0020260156597942114\n",
      "Data loss: 0.0005064909928478301, Function loss: 0.001431301119737327\n",
      "Step: 2270, Loss: 0.001937792170792818\n",
      "Data loss: 0.0004461578209884465, Function loss: 0.0013910771813243628\n",
      "Step: 2271, Loss: 0.0018372349441051483\n",
      "Data loss: 0.00047984335105866194, Function loss: 0.001276942901313305\n",
      "Step: 2272, Loss: 0.0017567862523719668\n",
      "Data loss: 0.0004607088922057301, Function loss: 0.0012577468296512961\n",
      "Step: 2273, Loss: 0.0017184556927531958\n",
      "Data loss: 0.00045622774632647634, Function loss: 0.0012670655269175768\n",
      "Step: 2274, Loss: 0.0017232932150363922\n",
      "Data loss: 0.00047886298852972686, Function loss: 0.0012788122985512018\n",
      "Step: 2275, Loss: 0.0017576753161847591\n",
      "Data loss: 0.00044360413448885083, Function loss: 0.001352593768388033\n",
      "Step: 2276, Loss: 0.0017961978446692228\n",
      "Data loss: 0.000489796744659543, Function loss: 0.0013370286906138062\n",
      "Step: 2277, Loss: 0.0018268254352733493\n",
      "Data loss: 0.0004390134708955884, Function loss: 0.0014014100888743997\n",
      "Step: 2278, Loss: 0.001840423559769988\n",
      "Data loss: 0.0004909179406240582, Function loss: 0.0013469676487147808\n",
      "Step: 2279, Loss: 0.001837885589338839\n",
      "Data loss: 0.0004388333181850612, Function loss: 0.001382859773002565\n",
      "Step: 2280, Loss: 0.0018216930329799652\n",
      "Data loss: 0.0004842990601900965, Function loss: 0.0013148161815479398\n",
      "Step: 2281, Loss: 0.0017991152126342058\n",
      "Data loss: 0.0004412246053107083, Function loss: 0.001333139487542212\n",
      "Step: 2282, Loss: 0.0017743641510605812\n",
      "Data loss: 0.00047513662138953805, Function loss: 0.0012763353297486901\n",
      "Step: 2283, Loss: 0.001751472009345889\n",
      "Data loss: 0.00044597923988476396, Function loss: 0.001286230399273336\n",
      "Step: 2284, Loss: 0.0017322096973657608\n",
      "Data loss: 0.00046554964501410723, Function loss: 0.001252966234460473\n",
      "Step: 2285, Loss: 0.0017185158794745803\n",
      "Data loss: 0.00045186170609667897, Function loss: 0.0012622936628758907\n",
      "Step: 2286, Loss: 0.0017141553107649088\n",
      "Data loss: 0.0004580528475344181, Function loss: 0.001254257746040821\n",
      "Step: 2287, Loss: 0.0017123105935752392\n",
      "Data loss: 0.00045838532969355583, Function loss: 0.0012572651030495763\n",
      "Step: 2288, Loss: 0.0017156504327431321\n",
      "Data loss: 0.0004532967577688396, Function loss: 0.0012668714625760913\n",
      "Step: 2289, Loss: 0.00172016816213727\n",
      "Data loss: 0.0004604504501912743, Function loss: 0.0012620252091437578\n",
      "Step: 2290, Loss: 0.0017224756302312016\n",
      "Data loss: 0.0004534974868874997, Function loss: 0.0012733670882880688\n",
      "Step: 2291, Loss: 0.001726864604279399\n",
      "Data loss: 0.00045607457286678255, Function loss: 0.001275041257031262\n",
      "Step: 2292, Loss: 0.001731115859001875\n",
      "Data loss: 0.0004580851527862251, Function loss: 0.0012851967476308346\n",
      "Step: 2293, Loss: 0.0017432819586247206\n",
      "Data loss: 0.00044889363925904036, Function loss: 0.0013070995919406414\n",
      "Step: 2294, Loss: 0.0017559932311996818\n",
      "Data loss: 0.0004642033309210092, Function loss: 0.0013099504867568612\n",
      "Step: 2295, Loss: 0.00177415378857404\n",
      "Data loss: 0.00044080737279728055, Function loss: 0.00134855427313596\n",
      "Step: 2296, Loss: 0.0017893617041409016\n",
      "Data loss: 0.0004700666177086532, Function loss: 0.001340051181614399\n",
      "Step: 2297, Loss: 0.001810117857530713\n",
      "Data loss: 0.0004327175847720355, Function loss: 0.0014055103529244661\n",
      "Step: 2298, Loss: 0.001838227966800332\n",
      "Data loss: 0.00047764438204467297, Function loss: 0.001401220099069178\n",
      "Step: 2299, Loss: 0.001878864481113851\n",
      "Data loss: 0.0004260732384864241, Function loss: 0.001503767678514123\n",
      "Step: 2300, Loss: 0.0019298408878967166\n",
      "Data loss: 0.00048733357107266784, Function loss: 0.0015309121226891875\n",
      "Step: 2301, Loss: 0.0020182456355541945\n",
      "Data loss: 0.0004214516666252166, Function loss: 0.001729421317577362\n",
      "Step: 2302, Loss: 0.002150872955098748\n",
      "Data loss: 0.0005048028542660177, Function loss: 0.0018679588101804256\n",
      "Step: 2303, Loss: 0.0023727617226541042\n",
      "Data loss: 0.0004222012939862907, Function loss: 0.002272359561175108\n",
      "Step: 2304, Loss: 0.0026945609133690596\n",
      "Data loss: 0.0005393650499172509, Function loss: 0.002652062103152275\n",
      "Step: 2305, Loss: 0.003191427094861865\n",
      "Data loss: 0.0004361684841569513, Function loss: 0.0033453484065830708\n",
      "Step: 2306, Loss: 0.0037815168034285307\n",
      "Data loss: 0.0005965435993857682, Function loss: 0.004069264512509108\n",
      "Step: 2307, Loss: 0.0046658082865178585\n",
      "Data loss: 0.00046619705972261727, Function loss: 0.005206923931837082\n",
      "Step: 2308, Loss: 0.005673121195286512\n",
      "Data loss: 0.0006889778305776417, Function loss: 0.006522709503769875\n",
      "Step: 2309, Loss: 0.007211687508970499\n",
      "Data loss: 0.0005108249024488032, Function loss: 0.008006298914551735\n",
      "Step: 2310, Loss: 0.008517123758792877\n",
      "Data loss: 0.0007935843896120787, Function loss: 0.009555689059197903\n",
      "Step: 2311, Loss: 0.010349273681640625\n",
      "Data loss: 0.0005467618466354907, Function loss: 0.010525165125727654\n",
      "Step: 2312, Loss: 0.011071926914155483\n",
      "Data loss: 0.0008427194552496076, Function loss: 0.011013208888471127\n",
      "Step: 2313, Loss: 0.011855928227305412\n",
      "Data loss: 0.000531431520357728, Function loss: 0.009756895713508129\n",
      "Step: 2314, Loss: 0.010288327001035213\n",
      "Data loss: 0.0007364281918853521, Function loss: 0.007508846465498209\n",
      "Step: 2315, Loss: 0.008245274424552917\n",
      "Data loss: 0.00046811794163659215, Function loss: 0.004554367624223232\n",
      "Step: 2316, Loss: 0.005022485740482807\n",
      "Data loss: 0.0005437209620140493, Function loss: 0.002070624614134431\n",
      "Step: 2317, Loss: 0.002614345634356141\n",
      "Data loss: 0.0004836170410271734, Function loss: 0.0012094432022422552\n",
      "Step: 2318, Loss: 0.001693060272373259\n",
      "Data loss: 0.0004649972834158689, Function loss: 0.0018580402247607708\n",
      "Step: 2319, Loss: 0.0023230374790728092\n",
      "Data loss: 0.0006063109030947089, Function loss: 0.0030819480307400227\n",
      "Step: 2320, Loss: 0.0036882590502500534\n",
      "Data loss: 0.0004826992517337203, Function loss: 0.004098048433661461\n",
      "Step: 2321, Loss: 0.004580747801810503\n",
      "Data loss: 0.0006497233989648521, Function loss: 0.003946371842175722\n",
      "Step: 2322, Loss: 0.0045960950665175915\n",
      "Data loss: 0.00048358080675825477, Function loss: 0.0030339511577039957\n",
      "Step: 2323, Loss: 0.0035175320226699114\n",
      "Data loss: 0.0005600849981419742, Function loss: 0.0017720474861562252\n",
      "Step: 2324, Loss: 0.0023321325425058603\n",
      "Data loss: 0.0005035711801610887, Function loss: 0.0012111421674489975\n",
      "Step: 2325, Loss: 0.0017147134058177471\n",
      "Data loss: 0.0004941293736919761, Function loss: 0.0014341792557388544\n",
      "Step: 2326, Loss: 0.0019283086294308305\n",
      "Data loss: 0.0005768206319771707, Function loss: 0.0020049146842211485\n",
      "Step: 2327, Loss: 0.00258173537440598\n",
      "Data loss: 0.0004901560605503619, Function loss: 0.002545154420658946\n",
      "Step: 2328, Loss: 0.003035310423001647\n",
      "Data loss: 0.0005975987296551466, Function loss: 0.0024467052426189184\n",
      "Step: 2329, Loss: 0.003044303972274065\n",
      "Data loss: 0.0004909029812552035, Function loss: 0.0020724462810903788\n",
      "Step: 2330, Loss: 0.0025633492041379213\n",
      "Data loss: 0.0005461294203996658, Function loss: 0.0014661188470199704\n",
      "Step: 2331, Loss: 0.0020122481510043144\n",
      "Data loss: 0.0005068344762548804, Function loss: 0.0011951205087825656\n",
      "Step: 2332, Loss: 0.001701954985037446\n",
      "Data loss: 0.0005001627723686397, Function loss: 0.0012493685353547335\n",
      "Step: 2333, Loss: 0.001749531365931034\n",
      "Data loss: 0.00054527772590518, Function loss: 0.0014910344034433365\n",
      "Step: 2334, Loss: 0.0020363121293485165\n",
      "Data loss: 0.00048498690011911094, Function loss: 0.0018069843063130975\n",
      "Step: 2335, Loss: 0.002291971119120717\n",
      "Data loss: 0.0005599466967396438, Function loss: 0.0017839918145909905\n",
      "Step: 2336, Loss: 0.0023439384531229734\n",
      "Data loss: 0.0004802200710400939, Function loss: 0.0016801971942186356\n",
      "Step: 2337, Loss: 0.0021604173816740513\n",
      "Data loss: 0.0005339864874258637, Function loss: 0.0013803262263536453\n",
      "Step: 2338, Loss: 0.001914312713779509\n",
      "Data loss: 0.00048734841402620077, Function loss: 0.0012333530467003584\n",
      "Step: 2339, Loss: 0.0017207014607265592\n",
      "Data loss: 0.0005016765790060163, Function loss: 0.0011616465635597706\n",
      "Step: 2340, Loss: 0.0016633231425657868\n",
      "Data loss: 0.0005074068321846426, Function loss: 0.0012105659116059542\n",
      "Step: 2341, Loss: 0.0017179728019982576\n",
      "Data loss: 0.0004809842794202268, Function loss: 0.0013460306217893958\n",
      "Step: 2342, Loss: 0.0018270148430019617\n",
      "Data loss: 0.0005245321081019938, Function loss: 0.0014035203494131565\n",
      "Step: 2343, Loss: 0.0019280523993074894\n",
      "Data loss: 0.000469885126221925, Function loss: 0.00150119059253484\n",
      "Step: 2344, Loss: 0.001971075776964426\n",
      "Data loss: 0.0005250477697700262, Function loss: 0.0014282192569226027\n",
      "Step: 2345, Loss: 0.001953267026692629\n",
      "Data loss: 0.00046596871106885374, Function loss: 0.0014155606040731072\n",
      "Step: 2346, Loss: 0.0018815293442457914\n",
      "Data loss: 0.0005111447535455227, Function loss: 0.001278506126254797\n",
      "Step: 2347, Loss: 0.0017896508798003197\n",
      "Data loss: 0.00046981655759736896, Function loss: 0.0012323515256866813\n",
      "Step: 2348, Loss: 0.0017021680250763893\n",
      "Data loss: 0.0004905536770820618, Function loss: 0.001156470039859414\n",
      "Step: 2349, Loss: 0.0016470237169414759\n",
      "Data loss: 0.0004819764581043273, Function loss: 0.0011598897399380803\n",
      "Step: 2350, Loss: 0.0016418661689385772\n",
      "Data loss: 0.00047262359294109046, Function loss: 0.0012038697022944689\n",
      "Step: 2351, Loss: 0.0016764933243393898\n",
      "Data loss: 0.0004943121457472444, Function loss: 0.0012313680490478873\n",
      "Step: 2352, Loss: 0.0017256801947951317\n",
      "Data loss: 0.0004621997068170458, Function loss: 0.0013026684755459428\n",
      "Step: 2353, Loss: 0.0017648681532591581\n",
      "Data loss: 0.0004978355136699975, Function loss: 0.0012768649030476809\n",
      "Step: 2354, Loss: 0.0017747003585100174\n",
      "Data loss: 0.0004558911023195833, Function loss: 0.0013052086578682065\n",
      "Step: 2355, Loss: 0.0017610997892916203\n",
      "Data loss: 0.0004943861276842654, Function loss: 0.00124042434617877\n",
      "Step: 2356, Loss: 0.0017348104156553745\n",
      "Data loss: 0.00045280600897967815, Function loss: 0.0012531698448583484\n",
      "Step: 2357, Loss: 0.0017059758538380265\n",
      "Data loss: 0.0004891795106232166, Function loss: 0.0011931764893233776\n",
      "Step: 2358, Loss: 0.0016823559999465942\n",
      "Data loss: 0.0004523276293184608, Function loss: 0.0012171308044344187\n",
      "Step: 2359, Loss: 0.00166945846285671\n",
      "Data loss: 0.0004860293120145798, Function loss: 0.001181884203106165\n",
      "Step: 2360, Loss: 0.0016679135151207447\n",
      "Data loss: 0.0004528854915406555, Function loss: 0.0012219713535159826\n",
      "Step: 2361, Loss: 0.0016748568741604686\n",
      "Data loss: 0.0004840175388380885, Function loss: 0.0011987450998276472\n",
      "Step: 2362, Loss: 0.0016827626386657357\n",
      "Data loss: 0.0004532974271569401, Function loss: 0.0012327837757766247\n",
      "Step: 2363, Loss: 0.0016860811738297343\n",
      "Data loss: 0.0004807783698197454, Function loss: 0.0012012483784928918\n",
      "Step: 2364, Loss: 0.0016820267774164677\n",
      "Data loss: 0.00045108984340913594, Function loss: 0.0012207657564431429\n",
      "Step: 2365, Loss: 0.0016718555707484484\n",
      "Data loss: 0.0004750367079395801, Function loss: 0.0011800186475738883\n",
      "Step: 2366, Loss: 0.001655055326409638\n",
      "Data loss: 0.00044843732030130923, Function loss: 0.0011897585354745388\n",
      "Step: 2367, Loss: 0.0016381958266720176\n",
      "Data loss: 0.0004678418335970491, Function loss: 0.0011552580399438739\n",
      "Step: 2368, Loss: 0.0016230999026447535\n",
      "Data loss: 0.0004470270068850368, Function loss: 0.0011684977216646075\n",
      "Step: 2369, Loss: 0.0016155247576534748\n",
      "Data loss: 0.00046152991126291454, Function loss: 0.0011508315801620483\n",
      "Step: 2370, Loss: 0.0016123615205287933\n",
      "Data loss: 0.0004474674933589995, Function loss: 0.0011714225402101874\n",
      "Step: 2371, Loss: 0.0016188900917768478\n",
      "Data loss: 0.0004582195251714438, Function loss: 0.001171655603684485\n",
      "Step: 2372, Loss: 0.0016298751579597592\n",
      "Data loss: 0.0004491847357712686, Function loss: 0.0011944264406338334\n",
      "Step: 2373, Loss: 0.001643611118197441\n",
      "Data loss: 0.0004559496301226318, Function loss: 0.0012009420897811651\n",
      "Step: 2374, Loss: 0.0016568917781114578\n",
      "Data loss: 0.00045159252476878464, Function loss: 0.0012130462564527988\n",
      "Step: 2375, Loss: 0.001664638752117753\n",
      "Data loss: 0.00045241208863444626, Function loss: 0.0012160427868366241\n",
      "Step: 2376, Loss: 0.00166845484636724\n",
      "Data loss: 0.0004545249976217747, Function loss: 0.0012102666078135371\n",
      "Step: 2377, Loss: 0.0016647916054353118\n",
      "Data loss: 0.00044684356544166803, Function loss: 0.001212532166391611\n",
      "Step: 2378, Loss: 0.0016593757318332791\n",
      "Data loss: 0.0004576666688080877, Function loss: 0.001197183271870017\n",
      "Step: 2379, Loss: 0.0016548499697819352\n",
      "Data loss: 0.00044104925473220646, Function loss: 0.0012147189117968082\n",
      "Step: 2380, Loss: 0.0016557681374251842\n",
      "Data loss: 0.000461099436506629, Function loss: 0.0011982574360445142\n",
      "Step: 2381, Loss: 0.0016593568725511432\n",
      "Data loss: 0.00043579438352026045, Function loss: 0.0012317043729126453\n",
      "Step: 2382, Loss: 0.0016674987273290753\n",
      "Data loss: 0.00046635454054921865, Function loss: 0.0012159724719822407\n",
      "Step: 2383, Loss: 0.0016823270125314593\n",
      "Data loss: 0.00043058086885139346, Function loss: 0.0012774340575560927\n",
      "Step: 2384, Loss: 0.0017080148681998253\n",
      "Data loss: 0.0004758691356983036, Function loss: 0.0012706028064712882\n",
      "Step: 2385, Loss: 0.0017464719712734222\n",
      "Data loss: 0.00042643636697903275, Function loss: 0.0013784629991278052\n",
      "Step: 2386, Loss: 0.001804899424314499\n",
      "Data loss: 0.0004941761144436896, Function loss: 0.0014012778410688043\n",
      "Step: 2387, Loss: 0.0018954540137201548\n",
      "Data loss: 0.00042506400495767593, Function loss: 0.0016092807054519653\n",
      "Step: 2388, Loss: 0.0020343447104096413\n",
      "Data loss: 0.000530004792381078, Function loss: 0.0017149002524092793\n",
      "Step: 2389, Loss: 0.0022449051029980183\n",
      "Data loss: 0.00043380845454521477, Function loss: 0.002104201354086399\n",
      "Step: 2390, Loss: 0.0025380097795277834\n",
      "Data loss: 0.0005910329055041075, Function loss: 0.0023531601764261723\n",
      "Step: 2391, Loss: 0.0029441930819302797\n",
      "Data loss: 0.0004601425025612116, Function loss: 0.003018914256244898\n",
      "Step: 2392, Loss: 0.0034790567588061094\n",
      "Data loss: 0.0006853959057480097, Function loss: 0.003489768598228693\n",
      "Step: 2393, Loss: 0.004175164736807346\n",
      "Data loss: 0.000508733734022826, Function loss: 0.004484198987483978\n",
      "Step: 2394, Loss: 0.0049929325468838215\n",
      "Data loss: 0.0008022334659472108, Function loss: 0.005137769505381584\n",
      "Step: 2395, Loss: 0.005940002854913473\n",
      "Data loss: 0.0005637936410494149, Function loss: 0.006239159032702446\n",
      "Step: 2396, Loss: 0.006802952848374844\n",
      "Data loss: 0.0008787717088125646, Function loss: 0.006610088516026735\n",
      "Step: 2397, Loss: 0.007488860283046961\n",
      "Data loss: 0.0005529329064302146, Function loss: 0.006962948478758335\n",
      "Step: 2398, Loss: 0.007515881210565567\n",
      "Data loss: 0.00079724210081622, Function loss: 0.006278558634221554\n",
      "Step: 2399, Loss: 0.007075800560414791\n",
      "Data loss: 0.0004472999426070601, Function loss: 0.005598339717835188\n",
      "Step: 2400, Loss: 0.006045639514923096\n",
      "Data loss: 0.0006295987986959517, Function loss: 0.004697073251008987\n",
      "Step: 2401, Loss: 0.005326671991497278\n",
      "Data loss: 0.00044193395297043025, Function loss: 0.004474876914173365\n",
      "Step: 2402, Loss: 0.004916810896247625\n",
      "Data loss: 0.0006294779013842344, Function loss: 0.00472050067037344\n",
      "Step: 2403, Loss: 0.005349978804588318\n",
      "Data loss: 0.000624308071564883, Function loss: 0.005332092754542828\n",
      "Step: 2404, Loss: 0.005956400651484728\n",
      "Data loss: 0.0007518916972912848, Function loss: 0.0058235954493284225\n",
      "Step: 2405, Loss: 0.006575487088412046\n",
      "Data loss: 0.0007376470603048801, Function loss: 0.005628159735351801\n",
      "Step: 2406, Loss: 0.006365806795656681\n",
      "Data loss: 0.000709524261765182, Function loss: 0.00466584600508213\n",
      "Step: 2407, Loss: 0.005375370383262634\n",
      "Data loss: 0.0005881114047951996, Function loss: 0.0032331636175513268\n",
      "Step: 2408, Loss: 0.0038212749641388655\n",
      "Data loss: 0.0005328780971467495, Function loss: 0.0019285728922113776\n",
      "Step: 2409, Loss: 0.002461451105773449\n",
      "Data loss: 0.00045661607873626053, Function loss: 0.001335537526756525\n",
      "Step: 2410, Loss: 0.0017921535763889551\n",
      "Data loss: 0.0005159804131835699, Function loss: 0.0013753141975030303\n",
      "Step: 2411, Loss: 0.0018912946106866002\n",
      "Data loss: 0.0005195236881263554, Function loss: 0.001846877159550786\n",
      "Step: 2412, Loss: 0.0023664007894694805\n",
      "Data loss: 0.0005747208488173783, Function loss: 0.002167673781514168\n",
      "Step: 2413, Loss: 0.002742394572123885\n",
      "Data loss: 0.0005654508713632822, Function loss: 0.0022307259496301413\n",
      "Step: 2414, Loss: 0.0027961768209934235\n",
      "Data loss: 0.0005215099081397057, Function loss: 0.0020721496548503637\n",
      "Step: 2415, Loss: 0.0025936595629900694\n",
      "Data loss: 0.0005390543374232948, Function loss: 0.0018547577783465385\n",
      "Step: 2416, Loss: 0.0023938121739774942\n",
      "Data loss: 0.00045398392830975354, Function loss: 0.0018853001529350877\n",
      "Step: 2417, Loss: 0.0023392841685563326\n",
      "Data loss: 0.0005522783030755818, Function loss: 0.001913027255795896\n",
      "Step: 2418, Loss: 0.0024653056170791388\n",
      "Data loss: 0.00045734213199466467, Function loss: 0.002074884483590722\n",
      "Step: 2419, Loss: 0.002532226499170065\n",
      "Data loss: 0.0005628662183880806, Function loss: 0.0018611593404784799\n",
      "Step: 2420, Loss: 0.0024240254424512386\n",
      "Data loss: 0.00045887130545452237, Function loss: 0.001662293914705515\n",
      "Step: 2421, Loss: 0.0021211651619523764\n",
      "Data loss: 0.0005139262066222727, Function loss: 0.0012862657895311713\n",
      "Step: 2422, Loss: 0.001800192054361105\n",
      "Data loss: 0.00046534810098819435, Function loss: 0.0011437806533649564\n",
      "Step: 2423, Loss: 0.0016091287834569812\n",
      "Data loss: 0.0004741939774248749, Function loss: 0.0011328003602102399\n",
      "Step: 2424, Loss: 0.0016069943085312843\n",
      "Data loss: 0.0005038163508288562, Function loss: 0.001211655675433576\n",
      "Step: 2425, Loss: 0.0017154719680547714\n",
      "Data loss: 0.00046034425031393766, Function loss: 0.0013599095400422812\n",
      "Step: 2426, Loss: 0.0018202537903562188\n",
      "Data loss: 0.0005191406235098839, Function loss: 0.0013213478960096836\n",
      "Step: 2427, Loss: 0.0018404885195195675\n",
      "Data loss: 0.0004473768058232963, Function loss: 0.0013440016191452742\n",
      "Step: 2428, Loss: 0.0017913784831762314\n",
      "Data loss: 0.0005006855353713036, Function loss: 0.0012406801106408238\n",
      "Step: 2429, Loss: 0.0017413656460121274\n",
      "Data loss: 0.00045037511154077947, Function loss: 0.0012889726785942912\n",
      "Step: 2430, Loss: 0.0017393478192389011\n",
      "Data loss: 0.0004893246223218739, Function loss: 0.0013077827170491219\n",
      "Step: 2431, Loss: 0.0017971072811633348\n",
      "Data loss: 0.0004699834680650383, Function loss: 0.0014056345680728555\n",
      "Step: 2432, Loss: 0.0018756180070340633\n",
      "Data loss: 0.0004908940172754228, Function loss: 0.0014528915053233504\n",
      "Step: 2433, Loss: 0.0019437854643911123\n",
      "Data loss: 0.000473519554361701, Function loss: 0.0014970839256420732\n",
      "Step: 2434, Loss: 0.001970603596419096\n",
      "Data loss: 0.0004921823856420815, Function loss: 0.0014695213176310062\n",
      "Step: 2435, Loss: 0.001961703645065427\n",
      "Data loss: 0.00045334870810620487, Function loss: 0.0014740462647750974\n",
      "Step: 2436, Loss: 0.0019273950019851327\n",
      "Data loss: 0.0004944102838635445, Function loss: 0.001413607271388173\n",
      "Step: 2437, Loss: 0.0019080175552517176\n",
      "Data loss: 0.0004318291612435132, Function loss: 0.0014749137917533517\n",
      "Step: 2438, Loss: 0.0019067429238930345\n",
      "Data loss: 0.0005041162949055433, Function loss: 0.001437914907000959\n",
      "Step: 2439, Loss: 0.0019420312019065022\n",
      "Data loss: 0.00042160580051131546, Function loss: 0.0015560946194455028\n",
      "Step: 2440, Loss: 0.0019777005072683096\n",
      "Data loss: 0.000511269667185843, Function loss: 0.001494469353929162\n",
      "Step: 2441, Loss: 0.002005739137530327\n",
      "Data loss: 0.00041863095248118043, Function loss: 0.0015869714552536607\n",
      "Step: 2442, Loss: 0.002005602465942502\n",
      "Data loss: 0.0005068670143373311, Function loss: 0.0015077192801982164\n",
      "Step: 2443, Loss: 0.0020145862363278866\n",
      "Data loss: 0.0004194539214950055, Function loss: 0.0016219689277932048\n",
      "Step: 2444, Loss: 0.002041422761976719\n",
      "Data loss: 0.0005051444750279188, Function loss: 0.0016357991844415665\n",
      "Step: 2445, Loss: 0.0021409436594694853\n",
      "Data loss: 0.000427456310717389, Function loss: 0.001888778991997242\n",
      "Step: 2446, Loss: 0.0023162353318184614\n",
      "Data loss: 0.0005224721971899271, Function loss: 0.0020993209909647703\n",
      "Step: 2447, Loss: 0.0026217931881546974\n",
      "Data loss: 0.00044114148477092385, Function loss: 0.0025908618699759245\n",
      "Step: 2448, Loss: 0.0030320032965391874\n",
      "Data loss: 0.0005662105395458639, Function loss: 0.0031100246123969555\n",
      "Step: 2449, Loss: 0.0036762352101504803\n",
      "Data loss: 0.00046208439744077623, Function loss: 0.004006050992757082\n",
      "Step: 2450, Loss: 0.0044681355357170105\n",
      "Data loss: 0.0006406640750356019, Function loss: 0.004999025724828243\n",
      "Step: 2451, Loss: 0.005639689974486828\n",
      "Data loss: 0.00048740595229901373, Function loss: 0.006206131540238857\n",
      "Step: 2452, Loss: 0.006693537347018719\n",
      "Data loss: 0.0007241573184728622, Function loss: 0.007420032285153866\n",
      "Step: 2453, Loss: 0.008144189603626728\n",
      "Data loss: 0.0005029651219956577, Function loss: 0.008434293791651726\n",
      "Step: 2454, Loss: 0.008937259204685688\n",
      "Data loss: 0.000778019311837852, Function loss: 0.009165653958916664\n",
      "Step: 2455, Loss: 0.009943673387169838\n",
      "Data loss: 0.0004963753744959831, Function loss: 0.008931648917496204\n",
      "Step: 2456, Loss: 0.009428024291992188\n",
      "Data loss: 0.0007429313263855875, Function loss: 0.008011478930711746\n",
      "Step: 2457, Loss: 0.008754409849643707\n",
      "Data loss: 0.00046008374192751944, Function loss: 0.006146744359284639\n",
      "Step: 2458, Loss: 0.0066068279556930065\n",
      "Data loss: 0.0006147134699858725, Function loss: 0.003944126423448324\n",
      "Step: 2459, Loss: 0.004558839835226536\n",
      "Data loss: 0.00044170598266646266, Function loss: 0.002234561601653695\n",
      "Step: 2460, Loss: 0.0026762676425278187\n",
      "Data loss: 0.0004993390757590532, Function loss: 0.0011777966283261776\n",
      "Step: 2461, Loss: 0.0016771357040852308\n",
      "Data loss: 0.0004960351507179439, Function loss: 0.0011363431112840772\n",
      "Step: 2462, Loss: 0.0016323782037943602\n",
      "Data loss: 0.00045871769543737173, Function loss: 0.001830375986173749\n",
      "Step: 2463, Loss: 0.002289093565195799\n",
      "Data loss: 0.0005836723139509559, Function loss: 0.002615229692310095\n",
      "Step: 2464, Loss: 0.0031989021226763725\n",
      "Data loss: 0.0004598096711561084, Function loss: 0.0032636399846524\n",
      "Step: 2465, Loss: 0.0037234495393931866\n",
      "Data loss: 0.0006070929230190814, Function loss: 0.003092074068263173\n",
      "Step: 2466, Loss: 0.0036991669330745935\n",
      "Data loss: 0.00046096480218693614, Function loss: 0.002516181441023946\n",
      "Step: 2467, Loss: 0.002977146301418543\n",
      "Data loss: 0.0005429640877991915, Function loss: 0.0016016970621421933\n",
      "Step: 2468, Loss: 0.0021446612663567066\n",
      "Data loss: 0.00048225311911664903, Function loss: 0.0011198471765965223\n",
      "Step: 2469, Loss: 0.0016021003248170018\n",
      "Data loss: 0.00048087575123645365, Function loss: 0.0011226764181628823\n",
      "Step: 2470, Loss: 0.0016035521402955055\n",
      "Data loss: 0.0005366207915358245, Function loss: 0.0014729604590684175\n",
      "Step: 2471, Loss: 0.002009581308811903\n",
      "Data loss: 0.0004641441919375211, Function loss: 0.002009539632126689\n",
      "Step: 2472, Loss: 0.0024736837949603796\n",
      "Data loss: 0.0005692326230928302, Function loss: 0.002153480425477028\n",
      "Step: 2473, Loss: 0.00272271316498518\n",
      "Data loss: 0.0004642961430363357, Function loss: 0.002093782415613532\n",
      "Step: 2474, Loss: 0.0025580786168575287\n",
      "Data loss: 0.0005418460350483656, Function loss: 0.0016439527971670032\n",
      "Step: 2475, Loss: 0.002185798715800047\n",
      "Data loss: 0.00047305243788287044, Function loss: 0.0013086359249427915\n",
      "Step: 2476, Loss: 0.001781688304618001\n",
      "Data loss: 0.0004940375220030546, Function loss: 0.0010612504556775093\n",
      "Step: 2477, Loss: 0.001555287977680564\n",
      "Data loss: 0.0004969614092260599, Function loss: 0.0010451418347656727\n",
      "Step: 2478, Loss: 0.0015421032439917326\n",
      "Data loss: 0.0004645836306735873, Function loss: 0.001213071751408279\n",
      "Step: 2479, Loss: 0.0016776553820818663\n",
      "Data loss: 0.0005195909179747105, Function loss: 0.0013295855605974793\n",
      "Step: 2480, Loss: 0.0018491764785721898\n",
      "Data loss: 0.00045576540287584066, Function loss: 0.0015090947272256017\n",
      "Step: 2481, Loss: 0.0019648601301014423\n",
      "Data loss: 0.0005209523951634765, Function loss: 0.0014671458629891276\n",
      "Step: 2482, Loss: 0.001988098258152604\n",
      "Data loss: 0.0004608634626492858, Function loss: 0.0014345294330269098\n",
      "Step: 2483, Loss: 0.0018953928956761956\n",
      "Data loss: 0.0004988574655726552, Function loss: 0.0012586815282702446\n",
      "Step: 2484, Loss: 0.0017575389938428998\n",
      "Data loss: 0.0004729983920697123, Function loss: 0.0011473321355879307\n",
      "Step: 2485, Loss: 0.0016203304985538125\n",
      "Data loss: 0.0004690190253313631, Function loss: 0.001075033680535853\n",
      "Step: 2486, Loss: 0.0015440527349710464\n",
      "Data loss: 0.00048567616613581777, Function loss: 0.0010523328091949224\n",
      "Step: 2487, Loss: 0.0015380089171230793\n",
      "Data loss: 0.0004493399173952639, Function loss: 0.001131333177909255\n",
      "Step: 2488, Loss: 0.001580673037096858\n",
      "Data loss: 0.0004923200467601418, Function loss: 0.0011465694988146424\n",
      "Step: 2489, Loss: 0.0016388895455747843\n",
      "Data loss: 0.0004446111270226538, Function loss: 0.0012288126163184643\n",
      "Step: 2490, Loss: 0.0016734236851334572\n",
      "Data loss: 0.00048606362543068826, Function loss: 0.0011867403518408537\n",
      "Step: 2491, Loss: 0.0016728040063753724\n",
      "Data loss: 0.0004522568779066205, Function loss: 0.001186846406199038\n",
      "Step: 2492, Loss: 0.0016391032841056585\n",
      "Data loss: 0.0004706698236986995, Function loss: 0.0011231605894863605\n",
      "Step: 2493, Loss: 0.00159383041318506\n",
      "Data loss: 0.0004592861805576831, Function loss: 0.0010861658956855536\n",
      "Step: 2494, Loss: 0.0015454520471394062\n",
      "Data loss: 0.00045512616634368896, Function loss: 0.0010539984796196222\n",
      "Step: 2495, Loss: 0.0015091246459633112\n",
      "Data loss: 0.0004615318030118942, Function loss: 0.0010278940899297595\n",
      "Step: 2496, Loss: 0.0014894258929416537\n",
      "Data loss: 0.0004460639029275626, Function loss: 0.001038771471939981\n",
      "Step: 2497, Loss: 0.001484835403971374\n",
      "Data loss: 0.00045961496653035283, Function loss: 0.0010303858434781432\n",
      "Step: 2498, Loss: 0.001490000868216157\n",
      "Data loss: 0.0004441097844392061, Function loss: 0.0010557579807937145\n",
      "Step: 2499, Loss: 0.0014998677652329206\n",
      "Data loss: 0.00045663854689337313, Function loss: 0.0010516317561268806\n",
      "Step: 2500, Loss: 0.0015082702739164233\n",
      "Data loss: 0.0004432380374055356, Function loss: 0.001070763566531241\n",
      "Step: 2501, Loss: 0.001514001633040607\n",
      "Data loss: 0.00045356995542533696, Function loss: 0.0010555441258475184\n",
      "Step: 2502, Loss: 0.001509114052169025\n",
      "Data loss: 0.00044135755160823464, Function loss: 0.0010538545902818441\n",
      "Step: 2503, Loss: 0.0014952120836824179\n",
      "Data loss: 0.00044772110413759947, Function loss: 0.001032204949297011\n",
      "Step: 2504, Loss: 0.0014799260534346104\n",
      "Data loss: 0.0004409939865581691, Function loss: 0.0010253412183374166\n",
      "Step: 2505, Loss: 0.0014663352631032467\n",
      "Data loss: 0.0004406222724355757, Function loss: 0.00102061009965837\n",
      "Step: 2506, Loss: 0.0014612323138862848\n",
      "Data loss: 0.00044245325261726975, Function loss: 0.0010212532943114638\n",
      "Step: 2507, Loss: 0.0014637066051363945\n",
      "Data loss: 0.0004356174904387444, Function loss: 0.0010317642008885741\n",
      "Step: 2508, Loss: 0.001467381720431149\n",
      "Data loss: 0.00044190906919538975, Function loss: 0.0010295857209712267\n",
      "Step: 2509, Loss: 0.0014714947901666164\n",
      "Data loss: 0.00043384302989579737, Function loss: 0.0010417078156024218\n",
      "Step: 2510, Loss: 0.0014755508163943887\n",
      "Data loss: 0.0004394391435198486, Function loss: 0.0010380572639405727\n",
      "Step: 2511, Loss: 0.0014774964656680822\n",
      "Data loss: 0.0004327077476773411, Function loss: 0.0010448594111949205\n",
      "Step: 2512, Loss: 0.0014775671297684312\n",
      "Data loss: 0.00043612689478322864, Function loss: 0.0010377614526078105\n",
      "Step: 2513, Loss: 0.0014738882891833782\n",
      "Data loss: 0.000431983673479408, Function loss: 0.0010347523493692279\n",
      "Step: 2514, Loss: 0.001466735964640975\n",
      "Data loss: 0.0004319155996199697, Function loss: 0.0010273426305502653\n",
      "Step: 2515, Loss: 0.0014592582592740655\n",
      "Data loss: 0.00043111873674206436, Function loss: 0.0010217943927273154\n",
      "Step: 2516, Loss: 0.0014529131585732102\n",
      "Data loss: 0.00042816478526219726, Function loss: 0.0010199520038440824\n",
      "Step: 2517, Loss: 0.0014481167308986187\n",
      "Data loss: 0.00043055860442109406, Function loss: 0.0010148194851353765\n",
      "Step: 2518, Loss: 0.00144537806045264\n",
      "Data loss: 0.0004247028555255383, Function loss: 0.0010203582933172584\n",
      "Step: 2519, Loss: 0.0014450611779466271\n",
      "Data loss: 0.0004306146001908928, Function loss: 0.00101609923876822\n",
      "Step: 2520, Loss: 0.0014467138098552823\n",
      "Data loss: 0.0004209583858028054, Function loss: 0.0010343140456825495\n",
      "Step: 2521, Loss: 0.001455272431485355\n",
      "Data loss: 0.0004326842608861625, Function loss: 0.0010353696998208761\n",
      "Step: 2522, Loss: 0.0014680540189146996\n",
      "Data loss: 0.0004169229359831661, Function loss: 0.0010667952010408044\n",
      "Step: 2523, Loss: 0.001483718166127801\n",
      "Data loss: 0.00043590099085122347, Function loss: 0.0010685714660212398\n",
      "Step: 2524, Loss: 0.0015044724568724632\n",
      "Data loss: 0.0004119224613532424, Function loss: 0.0011199191212654114\n",
      "Step: 2525, Loss: 0.0015318415826186538\n",
      "Data loss: 0.0004401725600473583, Function loss: 0.0011225950438529253\n",
      "Step: 2526, Loss: 0.0015627676621079445\n",
      "Data loss: 0.0004076508921571076, Function loss: 0.0011933597270399332\n",
      "Step: 2527, Loss: 0.0016010105609893799\n",
      "Data loss: 0.00044587263255380094, Function loss: 0.0012169437250122428\n",
      "Step: 2528, Loss: 0.0016628163866698742\n",
      "Data loss: 0.00040538515895605087, Function loss: 0.0013516646577045321\n",
      "Step: 2529, Loss: 0.001757049816660583\n",
      "Data loss: 0.00045745630632154644, Function loss: 0.001456677564419806\n",
      "Step: 2530, Loss: 0.001914133899845183\n",
      "Data loss: 0.0004073884920217097, Function loss: 0.0017365515232086182\n",
      "Step: 2531, Loss: 0.002143939957022667\n",
      "Data loss: 0.00048385577974841, Function loss: 0.002032039687037468\n",
      "Step: 2532, Loss: 0.002515895524993539\n",
      "Data loss: 0.00041517167119309306, Function loss: 0.0025787553749978542\n",
      "Step: 2533, Loss: 0.002993927104398608\n",
      "Data loss: 0.000531150377355516, Function loss: 0.0031895420979708433\n",
      "Step: 2534, Loss: 0.0037206923589110374\n",
      "Data loss: 0.00043386584729887545, Function loss: 0.0042008208110928535\n",
      "Step: 2535, Loss: 0.00463468674570322\n",
      "Data loss: 0.0006134845898486674, Function loss: 0.005475120153278112\n",
      "Step: 2536, Loss: 0.006088604684919119\n",
      "Data loss: 0.00047642161371186376, Function loss: 0.0071796029806137085\n",
      "Step: 2537, Loss: 0.007656024768948555\n",
      "Data loss: 0.0007402928313240409, Function loss: 0.009384006261825562\n",
      "Step: 2538, Loss: 0.010124298743903637\n",
      "Data loss: 0.000547108065802604, Function loss: 0.011477024294435978\n",
      "Step: 2539, Loss: 0.012024132534861565\n",
      "Data loss: 0.0008799438364803791, Function loss: 0.013922058045864105\n",
      "Step: 2540, Loss: 0.014802001416683197\n",
      "Data loss: 0.0006035391706973314, Function loss: 0.014348539523780346\n",
      "Step: 2541, Loss: 0.014952078461647034\n",
      "Data loss: 0.0008870933088473976, Function loss: 0.01374007761478424\n",
      "Step: 2542, Loss: 0.014627170749008656\n",
      "Data loss: 0.0005447060102596879, Function loss: 0.009925595484673977\n",
      "Step: 2543, Loss: 0.01047030184417963\n",
      "Data loss: 0.0006511801620945334, Function loss: 0.005504578351974487\n",
      "Step: 2544, Loss: 0.006155758630484343\n",
      "Data loss: 0.0004528512363322079, Function loss: 0.0020715247374027967\n",
      "Step: 2545, Loss: 0.0025243759155273438\n",
      "Data loss: 0.0004659581754822284, Function loss: 0.001042335294187069\n",
      "Step: 2546, Loss: 0.0015082934405654669\n",
      "Data loss: 0.0005606712074950337, Function loss: 0.002338636899366975\n",
      "Step: 2547, Loss: 0.0028993082232773304\n",
      "Data loss: 0.0004920870414935052, Function loss: 0.004639292135834694\n",
      "Step: 2548, Loss: 0.00513137923553586\n",
      "Data loss: 0.0007080839714035392, Function loss: 0.00586735038086772\n",
      "Step: 2549, Loss: 0.006575434468686581\n",
      "Data loss: 0.0005174498073756695, Function loss: 0.005069853737950325\n",
      "Step: 2550, Loss: 0.0055873035453259945\n",
      "Data loss: 0.0006196594331413507, Function loss: 0.002966585336253047\n",
      "Step: 2551, Loss: 0.0035862447693943977\n",
      "Data loss: 0.0005029092426411808, Function loss: 0.001342599163763225\n",
      "Step: 2552, Loss: 0.001845508348196745\n",
      "Data loss: 0.0005017748335376382, Function loss: 0.0011667292565107346\n",
      "Step: 2553, Loss: 0.0016685040900483727\n",
      "Data loss: 0.0005918605602346361, Function loss: 0.0021774061024188995\n",
      "Step: 2554, Loss: 0.0027692667208611965\n",
      "Data loss: 0.0005050027975812554, Function loss: 0.003269944339990616\n",
      "Step: 2555, Loss: 0.0037749470211565495\n",
      "Data loss: 0.0006345955189317465, Function loss: 0.003175358986482024\n",
      "Step: 2556, Loss: 0.0038099545054137707\n",
      "Data loss: 0.0005083869909867644, Function loss: 0.0022231265902519226\n",
      "Step: 2557, Loss: 0.002731513697654009\n",
      "Data loss: 0.0005463686538860202, Function loss: 0.00118087453301996\n",
      "Step: 2558, Loss: 0.0017272431869059801\n",
      "Data loss: 0.000533485144842416, Function loss: 0.0009992796694859862\n",
      "Step: 2559, Loss: 0.0015327648725360632\n",
      "Data loss: 0.000503995455801487, Function loss: 0.0015463628806173801\n",
      "Step: 2560, Loss: 0.002050358336418867\n",
      "Data loss: 0.0005839975201524794, Function loss: 0.0019889152608811855\n",
      "Step: 2561, Loss: 0.002572912722826004\n",
      "Data loss: 0.0005037222290411592, Function loss: 0.0019999626092612743\n",
      "Step: 2562, Loss: 0.0025036847218871117\n",
      "Data loss: 0.0005536967655643821, Function loss: 0.001456689671613276\n",
      "Step: 2563, Loss: 0.002010386437177658\n",
      "Data loss: 0.0005041490076109767, Function loss: 0.0010452189017087221\n",
      "Step: 2564, Loss: 0.0015493679093196988\n",
      "Data loss: 0.0005017808871343732, Function loss: 0.0009948473889380693\n",
      "Step: 2565, Loss: 0.0014966282760724425\n",
      "Data loss: 0.0005362191586755216, Function loss: 0.0012609862023964524\n",
      "Step: 2566, Loss: 0.0017972053028643131\n",
      "Data loss: 0.0004842814523726702, Function loss: 0.0016129142604768276\n",
      "Step: 2567, Loss: 0.002097195712849498\n",
      "Data loss: 0.0005490746698342264, Function loss: 0.0015756807988509536\n",
      "Step: 2568, Loss: 0.002124755410477519\n",
      "Data loss: 0.00047236084355972707, Function loss: 0.0013906724052503705\n",
      "Step: 2569, Loss: 0.001863033277913928\n",
      "Data loss: 0.0005153987440280616, Function loss: 0.001063253148458898\n",
      "Step: 2570, Loss: 0.0015786518342792988\n",
      "Data loss: 0.00048150026123039424, Function loss: 0.0009765865979716182\n",
      "Step: 2571, Loss: 0.0014580868883058429\n",
      "Data loss: 0.0004859768087044358, Function loss: 0.0010379577288404107\n",
      "Step: 2572, Loss: 0.0015239345375448465\n",
      "Data loss: 0.0005046866135671735, Function loss: 0.0011675350833684206\n",
      "Step: 2573, Loss: 0.001672221696935594\n",
      "Data loss: 0.00047024100786074996, Function loss: 0.001288056024350226\n",
      "Step: 2574, Loss: 0.001758296974003315\n",
      "Data loss: 0.0005066179437562823, Function loss: 0.001209618872962892\n",
      "Step: 2575, Loss: 0.0017162368167191744\n",
      "Data loss: 0.0004585020069498569, Function loss: 0.001132219797000289\n",
      "Step: 2576, Loss: 0.0015907217748463154\n",
      "Data loss: 0.0004901528009213507, Function loss: 0.000996058457531035\n",
      "Step: 2577, Loss: 0.0014862113166600466\n",
      "Data loss: 0.00045943260192871094, Function loss: 0.0009911473607644439\n",
      "Step: 2578, Loss: 0.0014505799626931548\n",
      "Data loss: 0.00047993662883527577, Function loss: 0.001003266079351306\n",
      "Step: 2579, Loss: 0.0014832026790827513\n",
      "Data loss: 0.00046934300917200744, Function loss: 0.0010693343356251717\n",
      "Step: 2580, Loss: 0.0015386773739010096\n",
      "Data loss: 0.0004727353516500443, Function loss: 0.0010990769369527698\n",
      "Step: 2581, Loss: 0.0015718123177066445\n",
      "Data loss: 0.0004703194717876613, Function loss: 0.0010912440484389663\n",
      "Step: 2582, Loss: 0.0015615634620189667\n",
      "Data loss: 0.00046159635530784726, Function loss: 0.0010553660104051232\n",
      "Step: 2583, Loss: 0.0015169624239206314\n",
      "Data loss: 0.0004621134139597416, Function loss: 0.0009961342439055443\n",
      "Step: 2584, Loss: 0.0014582476578652859\n",
      "Data loss: 0.00045409394078888, Function loss: 0.0009585745283402503\n",
      "Step: 2585, Loss: 0.0014126684982329607\n",
      "Data loss: 0.00045289917034097016, Function loss: 0.0009466582559980452\n",
      "Step: 2586, Loss: 0.0013995574554428458\n",
      "Data loss: 0.00045316823525354266, Function loss: 0.0009587741806171834\n",
      "Step: 2587, Loss: 0.0014119424158707261\n",
      "Data loss: 0.00044960592640563846, Function loss: 0.0009863384766504169\n",
      "Step: 2588, Loss: 0.0014359443448483944\n",
      "Data loss: 0.0004520510265138, Function loss: 0.0009987527737393975\n",
      "Step: 2589, Loss: 0.001450803829357028\n",
      "Data loss: 0.0004488046106416732, Function loss: 0.0010039625922217965\n",
      "Step: 2590, Loss: 0.0014527671737596393\n",
      "Data loss: 0.00044554847409017384, Function loss: 0.0009936593705788255\n",
      "Step: 2591, Loss: 0.0014392078155651689\n",
      "Data loss: 0.00044875789899379015, Function loss: 0.0009743468835949898\n",
      "Step: 2592, Loss: 0.00142310478258878\n",
      "Data loss: 0.0004359460435807705, Function loss: 0.0009739964734762907\n",
      "Step: 2593, Loss: 0.0014099425170570612\n",
      "Data loss: 0.00044998599332757294, Function loss: 0.000956163858063519\n",
      "Step: 2594, Loss: 0.0014061498222872615\n",
      "Data loss: 0.0004274414968676865, Function loss: 0.0009810900082811713\n",
      "Step: 2595, Loss: 0.0014085315633565187\n",
      "Data loss: 0.00045021355617791414, Function loss: 0.0009621369536034763\n",
      "Step: 2596, Loss: 0.0014123504515737295\n",
      "Data loss: 0.0004218042304273695, Function loss: 0.0009963767370209098\n",
      "Step: 2597, Loss: 0.0014181809965521097\n",
      "Data loss: 0.00044936189078725874, Function loss: 0.0009736577630974352\n",
      "Step: 2598, Loss: 0.0014230196829885244\n",
      "Data loss: 0.0004178984963800758, Function loss: 0.0010063900845125318\n",
      "Step: 2599, Loss: 0.001424288609996438\n",
      "Data loss: 0.00044746408821083605, Function loss: 0.0009787575108930469\n",
      "Step: 2600, Loss: 0.0014262215700000525\n",
      "Data loss: 0.00041530380258336663, Function loss: 0.0010071174474433064\n",
      "Step: 2601, Loss: 0.001422421308234334\n",
      "Data loss: 0.0004435683658812195, Function loss: 0.0009709069272503257\n",
      "Step: 2602, Loss: 0.0014144752640277147\n",
      "Data loss: 0.0004153113404754549, Function loss: 0.0009880468714982271\n",
      "Step: 2603, Loss: 0.0014033581828698516\n",
      "Data loss: 0.000436783186160028, Function loss: 0.0009552370756864548\n",
      "Step: 2604, Loss: 0.0013920202618464828\n",
      "Data loss: 0.00041772460099309683, Function loss: 0.000963405764196068\n",
      "Step: 2605, Loss: 0.0013811304233968258\n",
      "Data loss: 0.0004274581151548773, Function loss: 0.0009452425292693079\n",
      "Step: 2606, Loss: 0.0013727006735280156\n",
      "Data loss: 0.00042240432230755687, Function loss: 0.0009482521563768387\n",
      "Step: 2607, Loss: 0.0013706565368920565\n",
      "Data loss: 0.00041826048982329667, Function loss: 0.0009542569750919938\n",
      "Step: 2608, Loss: 0.00137251743581146\n",
      "Data loss: 0.00042548973578959703, Function loss: 0.0009477399289608002\n",
      "Step: 2609, Loss: 0.0013732296647503972\n",
      "Data loss: 0.00041299720760434866, Function loss: 0.0009621262433938682\n",
      "Step: 2610, Loss: 0.001375123392790556\n",
      "Data loss: 0.000425399950472638, Function loss: 0.0009497053688392043\n",
      "Step: 2611, Loss: 0.0013751053484156728\n",
      "Data loss: 0.0004109450674150139, Function loss: 0.0009601411293260753\n",
      "Step: 2612, Loss: 0.0013710862258449197\n",
      "Data loss: 0.00042258569737896323, Function loss: 0.0009448120254091918\n",
      "Step: 2613, Loss: 0.001367397722788155\n",
      "Data loss: 0.0004108528373762965, Function loss: 0.0009526785579510033\n",
      "Step: 2614, Loss: 0.0013635314535349607\n",
      "Data loss: 0.00041913322638720274, Function loss: 0.0009425003081560135\n",
      "Step: 2615, Loss: 0.0013616335345432162\n",
      "Data loss: 0.0004114168114028871, Function loss: 0.0009489221265539527\n",
      "Step: 2616, Loss: 0.0013603388797491789\n",
      "Data loss: 0.0004156322102062404, Function loss: 0.0009432960650883615\n",
      "Step: 2617, Loss: 0.001358928275294602\n",
      "Data loss: 0.0004123487451579422, Function loss: 0.000947585329413414\n",
      "Step: 2618, Loss: 0.0013599341036751866\n",
      "Data loss: 0.0004120982775930315, Function loss: 0.0009495686972513795\n",
      "Step: 2619, Loss: 0.0013616669457405806\n",
      "Data loss: 0.0004131255846004933, Function loss: 0.0009521857718937099\n",
      "Step: 2620, Loss: 0.0013653113273903728\n",
      "Data loss: 0.0004093281750101596, Function loss: 0.000962024147156626\n",
      "Step: 2621, Loss: 0.001371352351270616\n",
      "Data loss: 0.00041342011536471546, Function loss: 0.0009629863197915256\n",
      "Step: 2622, Loss: 0.0013764064060524106\n",
      "Data loss: 0.0004083502572029829, Function loss: 0.000971292844042182\n",
      "Step: 2623, Loss: 0.0013796431012451649\n",
      "Data loss: 0.0004116556083317846, Function loss: 0.0009666876867413521\n",
      "Step: 2624, Loss: 0.0013783433241769671\n",
      "Data loss: 0.00040959444595500827, Function loss: 0.0009661531075835228\n",
      "Step: 2625, Loss: 0.001375747611746192\n",
      "Data loss: 0.0004073794116266072, Function loss: 0.0009635916212573647\n",
      "Step: 2626, Loss: 0.0013709710910916328\n",
      "Data loss: 0.00041295072878710926, Function loss: 0.0009564639185555279\n",
      "Step: 2627, Loss: 0.0013694146182388067\n",
      "Data loss: 0.00040181027725338936, Function loss: 0.0009677897905930877\n",
      "Step: 2628, Loss: 0.001369600067846477\n",
      "Data loss: 0.00041792550473473966, Function loss: 0.0009598858305253088\n",
      "Step: 2629, Loss: 0.001377811306156218\n",
      "Data loss: 0.00039619518793188035, Function loss: 0.0009987680241465569\n",
      "Step: 2630, Loss: 0.0013949632411822677\n",
      "Data loss: 0.0004263381415512413, Function loss: 0.0009986869990825653\n",
      "Step: 2631, Loss: 0.001425025169737637\n",
      "Data loss: 0.0003919659648090601, Function loss: 0.0010808053193613887\n",
      "Step: 2632, Loss: 0.0014727712841704488\n",
      "Data loss: 0.0004425309889484197, Function loss: 0.001110649318434298\n",
      "Step: 2633, Loss: 0.0015531802782788873\n",
      "Data loss: 0.00039269193075597286, Function loss: 0.0012954463018104434\n",
      "Step: 2634, Loss: 0.0016881382325664163\n",
      "Data loss: 0.00047957911738194525, Function loss: 0.0014374434249475598\n",
      "Step: 2635, Loss: 0.0019170225132256746\n",
      "Data loss: 0.00040888439980335534, Function loss: 0.0018796182703226805\n",
      "Step: 2636, Loss: 0.0022885026410222054\n",
      "Data loss: 0.0005633163964375854, Function loss: 0.0023354305885732174\n",
      "Step: 2637, Loss: 0.0028987471014261246\n",
      "Data loss: 0.00046731048496440053, Function loss: 0.003419836051762104\n",
      "Step: 2638, Loss: 0.0038871464785188437\n",
      "Data loss: 0.0007477852632291615, Function loss: 0.004704861436039209\n",
      "Step: 2639, Loss: 0.005452646873891354\n",
      "Data loss: 0.000625883461907506, Function loss: 0.007081351242959499\n",
      "Step: 2640, Loss: 0.007707234472036362\n",
      "Data loss: 0.0010776127455756068, Function loss: 0.00962775107473135\n",
      "Step: 2641, Loss: 0.010705363936722279\n",
      "Data loss: 0.0008692156407050788, Function loss: 0.012738513760268688\n",
      "Step: 2642, Loss: 0.013607729226350784\n",
      "Data loss: 0.0013084678212180734, Function loss: 0.014280922710895538\n",
      "Step: 2643, Loss: 0.015589390881359577\n",
      "Data loss: 0.0008077728562057018, Function loss: 0.013683128170669079\n",
      "Step: 2644, Loss: 0.014490900561213493\n",
      "Data loss: 0.0009059638250619173, Function loss: 0.010217099450528622\n",
      "Step: 2645, Loss: 0.011123063042759895\n",
      "Data loss: 0.0004371647664811462, Function loss: 0.006303244270384312\n",
      "Step: 2646, Loss: 0.0067404089495539665\n",
      "Data loss: 0.0005851819296367466, Function loss: 0.004579882603138685\n",
      "Step: 2647, Loss: 0.005165064707398415\n",
      "Data loss: 0.0007438340689986944, Function loss: 0.005557031370699406\n",
      "Step: 2648, Loss: 0.006300865672528744\n",
      "Data loss: 0.0008916126098483801, Function loss: 0.007008642889559269\n",
      "Step: 2649, Loss: 0.007900255732238293\n",
      "Data loss: 0.0009026387706398964, Function loss: 0.006066677626222372\n",
      "Step: 2650, Loss: 0.0069693163968622684\n",
      "Data loss: 0.0006111445836722851, Function loss: 0.0032403585501015186\n",
      "Step: 2651, Loss: 0.0038515031337738037\n",
      "Data loss: 0.0004849822144024074, Function loss: 0.0010985141852870584\n",
      "Step: 2652, Loss: 0.0015834963414818048\n",
      "Data loss: 0.000527059193700552, Function loss: 0.001790492795407772\n",
      "Step: 2653, Loss: 0.002317551989108324\n",
      "Data loss: 0.0006972296396270394, Function loss: 0.003830193541944027\n",
      "Step: 2654, Loss: 0.0045274230651557446\n",
      "Data loss: 0.0007495468016713858, Function loss: 0.0043961782939732075\n",
      "Step: 2655, Loss: 0.00514572486281395\n",
      "Data loss: 0.0006395576056092978, Function loss: 0.002959685865789652\n",
      "Step: 2656, Loss: 0.0035992434713989496\n",
      "Data loss: 0.000483477720990777, Function loss: 0.0014527986058965325\n",
      "Step: 2657, Loss: 0.0019362763268873096\n",
      "Data loss: 0.0005278807948343456, Function loss: 0.0013794100377708673\n",
      "Step: 2658, Loss: 0.0019072908908128738\n",
      "Data loss: 0.0005597374401986599, Function loss: 0.002252213889732957\n",
      "Step: 2659, Loss: 0.0028119513299316168\n",
      "Data loss: 0.0006196185713633895, Function loss: 0.0023804521188139915\n",
      "Step: 2660, Loss: 0.003000070806592703\n",
      "Data loss: 0.000552723475266248, Function loss: 0.0017448606668040156\n",
      "Step: 2661, Loss: 0.0022975842002779245\n",
      "Data loss: 0.0004741690354421735, Function loss: 0.0014032837934792042\n",
      "Step: 2662, Loss: 0.0018774528289213777\n",
      "Data loss: 0.0005471690674312413, Function loss: 0.0017288167728111148\n",
      "Step: 2663, Loss: 0.002275985898450017\n",
      "Data loss: 0.0004905065870843828, Function loss: 0.002192178275436163\n",
      "Step: 2664, Loss: 0.0026826849207282066\n",
      "Data loss: 0.0005680709728039801, Function loss: 0.0017978593241423368\n",
      "Step: 2665, Loss: 0.002365930238738656\n",
      "Data loss: 0.00047080384683795273, Function loss: 0.0011594160459935665\n",
      "Step: 2666, Loss: 0.0016302198637276888\n",
      "Data loss: 0.00047097279457375407, Function loss: 0.000895224860869348\n",
      "Step: 2667, Loss: 0.0013661975972354412\n",
      "Data loss: 0.0005191247328184545, Function loss: 0.0012187639949843287\n",
      "Step: 2668, Loss: 0.0017378886695951223\n",
      "Data loss: 0.00046233032480813563, Function loss: 0.0016561117954552174\n",
      "Step: 2669, Loss: 0.0021184422075748444\n",
      "Data loss: 0.0005358930211514235, Function loss: 0.0015052605886012316\n",
      "Step: 2670, Loss: 0.002041153609752655\n",
      "Data loss: 0.0004417944292072207, Function loss: 0.0012610560515895486\n",
      "Step: 2671, Loss: 0.0017028504516929388\n",
      "Data loss: 0.00048347891424782574, Function loss: 0.0010934063466265798\n",
      "Step: 2672, Loss: 0.001576885231770575\n",
      "Data loss: 0.0004894481971859932, Function loss: 0.0011826595291495323\n",
      "Step: 2673, Loss: 0.0016721077263355255\n",
      "Data loss: 0.00046783886500634253, Function loss: 0.0012368583120405674\n",
      "Step: 2674, Loss: 0.0017046971479430795\n",
      "Data loss: 0.00049121881602332, Function loss: 0.0010641779517754912\n",
      "Step: 2675, Loss: 0.0015553967095911503\n",
      "Data loss: 0.000441656302427873, Function loss: 0.0009428836056031287\n",
      "Step: 2676, Loss: 0.0013845398789271712\n",
      "Data loss: 0.0004580878303386271, Function loss: 0.0009325811988674104\n",
      "Step: 2677, Loss: 0.0013906690292060375\n",
      "Data loss: 0.00046803191071376204, Function loss: 0.0010653360513970256\n",
      "Step: 2678, Loss: 0.0015333679039031267\n",
      "Data loss: 0.0004592548939399421, Function loss: 0.0011590467765927315\n",
      "Step: 2679, Loss: 0.0016183017287403345\n",
      "Data loss: 0.0004658239777199924, Function loss: 0.0010827751830220222\n",
      "Step: 2680, Loss: 0.0015485992189496756\n",
      "Data loss: 0.00045130529906600714, Function loss: 0.0009678982896730304\n",
      "Step: 2681, Loss: 0.0014192035887390375\n",
      "Data loss: 0.000430411339038983, Function loss: 0.0009331771871075034\n",
      "Step: 2682, Loss: 0.001363588497042656\n",
      "Data loss: 0.00046149466652423143, Function loss: 0.0009333014604635537\n",
      "Step: 2683, Loss: 0.001394796185195446\n",
      "Data loss: 0.00042789540020748973, Function loss: 0.000998548697680235\n",
      "Step: 2684, Loss: 0.0014264441560953856\n",
      "Data loss: 0.000456340319942683, Function loss: 0.0009497767896391451\n",
      "Step: 2685, Loss: 0.0014061171095818281\n",
      "Data loss: 0.0004343879409134388, Function loss: 0.0009230640716850758\n",
      "Step: 2686, Loss: 0.0013574520125985146\n",
      "Data loss: 0.00042747173574753106, Function loss: 0.0009074911940842867\n",
      "Step: 2687, Loss: 0.0013349629007279873\n",
      "Data loss: 0.00044847518438473344, Function loss: 0.0009099547751247883\n",
      "Step: 2688, Loss: 0.0013584299013018608\n",
      "Data loss: 0.00041475403122603893, Function loss: 0.0009751543402671814\n",
      "Step: 2689, Loss: 0.0013899083714932203\n",
      "Data loss: 0.00045091102947480977, Function loss: 0.0009300513193011284\n",
      "Step: 2690, Loss: 0.0013809623196721077\n",
      "Data loss: 0.00041686129407025874, Function loss: 0.0009201840148307383\n",
      "Step: 2691, Loss: 0.0013370453380048275\n",
      "Data loss: 0.00043091963743790984, Function loss: 0.0008720150799490511\n",
      "Step: 2692, Loss: 0.001302934717386961\n",
      "Data loss: 0.000429487437941134, Function loss: 0.0008743491489440203\n",
      "Step: 2693, Loss: 0.0013038365868851542\n",
      "Data loss: 0.00041663364390842617, Function loss: 0.0009046104969456792\n",
      "Step: 2694, Loss: 0.0013212441699579358\n",
      "Data loss: 0.0004355178098194301, Function loss: 0.0008910387987270951\n",
      "Step: 2695, Loss: 0.0013265565503388643\n",
      "Data loss: 0.000412907189456746, Function loss: 0.000900941202417016\n",
      "Step: 2696, Loss: 0.0013138484209775925\n",
      "Data loss: 0.00042630176176317036, Function loss: 0.0008742022910155356\n",
      "Step: 2697, Loss: 0.0013005040818825364\n",
      "Data loss: 0.00041809145477600396, Function loss: 0.0008810597355477512\n",
      "Step: 2698, Loss: 0.0012991512194275856\n",
      "Data loss: 0.00041631172643974423, Function loss: 0.0008876286447048187\n",
      "Step: 2699, Loss: 0.001303940312936902\n",
      "Data loss: 0.00042317030602134764, Function loss: 0.0008824729011394083\n",
      "Step: 2700, Loss: 0.0013056432362645864\n",
      "Data loss: 0.00041113453335128725, Function loss: 0.0008853414328768849\n",
      "Step: 2701, Loss: 0.0012964759953320026\n",
      "Data loss: 0.0004185404977761209, Function loss: 0.0008703689672984183\n",
      "Step: 2702, Loss: 0.0012889094650745392\n",
      "Data loss: 0.0004128093714825809, Function loss: 0.0008772888104431331\n",
      "Step: 2703, Loss: 0.001290098181925714\n",
      "Data loss: 0.00041264851461164653, Function loss: 0.0008866391144692898\n",
      "Step: 2704, Loss: 0.0012992876581847668\n",
      "Data loss: 0.0004141949175391346, Function loss: 0.0008877427317202091\n",
      "Step: 2705, Loss: 0.0013019376201555133\n",
      "Data loss: 0.00041204848093912005, Function loss: 0.000885512912645936\n",
      "Step: 2706, Loss: 0.001297561451792717\n",
      "Data loss: 0.0004072724259458482, Function loss: 0.0008846159325912595\n",
      "Step: 2707, Loss: 0.0012918883003294468\n",
      "Data loss: 0.0004146208812016994, Function loss: 0.0008748581167310476\n",
      "Step: 2708, Loss: 0.0012894789688289165\n",
      "Data loss: 0.00040150905260816216, Function loss: 0.0008902573026716709\n",
      "Step: 2709, Loss: 0.0012917662970721722\n",
      "Data loss: 0.00041517603676766157, Function loss: 0.0008762135985307395\n",
      "Step: 2710, Loss: 0.0012913895770907402\n",
      "Data loss: 0.0004005511000286788, Function loss: 0.0008858887013047934\n",
      "Step: 2711, Loss: 0.0012864398304373026\n",
      "Data loss: 0.00041025105747394264, Function loss: 0.0008683808264322579\n",
      "Step: 2712, Loss: 0.00127863185480237\n",
      "Data loss: 0.00040370822534896433, Function loss: 0.0008712074486538768\n",
      "Step: 2713, Loss: 0.0012749156448990107\n",
      "Data loss: 0.00040331200580112636, Function loss: 0.0008717142627574503\n",
      "Step: 2714, Loss: 0.0012750262394547462\n",
      "Data loss: 0.000407223793445155, Function loss: 0.0008683046326041222\n",
      "Step: 2715, Loss: 0.0012755284551531076\n",
      "Data loss: 0.00039912378997541964, Function loss: 0.0008777296170592308\n",
      "Step: 2716, Loss: 0.00127685337793082\n",
      "Data loss: 0.00040727033047005534, Function loss: 0.000866237620357424\n",
      "Step: 2717, Loss: 0.0012735079508274794\n",
      "Data loss: 0.0003979092580266297, Function loss: 0.0008744981023482978\n",
      "Step: 2718, Loss: 0.0012724073603749275\n",
      "Data loss: 0.000404933380195871, Function loss: 0.0008706104708835483\n",
      "Step: 2719, Loss: 0.0012755438219755888\n",
      "Data loss: 0.00039772933814674616, Function loss: 0.0008866213029250503\n",
      "Step: 2720, Loss: 0.0012843506410717964\n",
      "Data loss: 0.00040434315451420844, Function loss: 0.0008937220554798841\n",
      "Step: 2721, Loss: 0.0012980651808902621\n",
      "Data loss: 0.0003975819272454828, Function loss: 0.0009209872805513442\n",
      "Step: 2722, Loss: 0.0013185691786929965\n",
      "Data loss: 0.00040771832573227584, Function loss: 0.0009413028601557016\n",
      "Step: 2723, Loss: 0.001349021214991808\n",
      "Data loss: 0.00039271372952498496, Function loss: 0.0010070574935525656\n",
      "Step: 2724, Loss: 0.00139977119397372\n",
      "Data loss: 0.000418882817029953, Function loss: 0.0010723179439082742\n",
      "Step: 2725, Loss: 0.0014912007609382272\n",
      "Data loss: 0.00038495700573548675, Function loss: 0.001249227556400001\n",
      "Step: 2726, Loss: 0.0016341845039278269\n",
      "Data loss: 0.0004381712933536619, Function loss: 0.0014139945851638913\n",
      "Step: 2727, Loss: 0.0018521659076213837\n",
      "Data loss: 0.0003806978929787874, Function loss: 0.0017804654780775309\n",
      "Step: 2728, Loss: 0.0021611633710563183\n",
      "Data loss: 0.0004709935747087002, Function loss: 0.0021926737390458584\n",
      "Step: 2729, Loss: 0.0026636673137545586\n",
      "Data loss: 0.0003862529993057251, Function loss: 0.00300552137196064\n",
      "Step: 2730, Loss: 0.003391774371266365\n",
      "Data loss: 0.0005385733093135059, Function loss: 0.004121352452784777\n",
      "Step: 2731, Loss: 0.0046599255874753\n",
      "Data loss: 0.0004247119359206408, Function loss: 0.00595279922708869\n",
      "Step: 2732, Loss: 0.006377511192113161\n",
      "Data loss: 0.0006822324940003455, Function loss: 0.008728434331715107\n",
      "Step: 2733, Loss: 0.009410667233169079\n",
      "Data loss: 0.0005391088780015707, Function loss: 0.01222171913832426\n",
      "Step: 2734, Loss: 0.012760828249156475\n",
      "Data loss: 0.0009349217871204019, Function loss: 0.017252476885914803\n",
      "Step: 2735, Loss: 0.018187398090958595\n",
      "Data loss: 0.0007214105571620166, Function loss: 0.020750077441334724\n",
      "Step: 2736, Loss: 0.02147148735821247\n",
      "Data loss: 0.0011594544630497694, Function loss: 0.02454730123281479\n",
      "Step: 2737, Loss: 0.025706754997372627\n",
      "Data loss: 0.0007592421025037766, Function loss: 0.021002551540732384\n",
      "Step: 2738, Loss: 0.02176179364323616\n",
      "Data loss: 0.0009205095120705664, Function loss: 0.014867548830807209\n",
      "Step: 2739, Loss: 0.015788057819008827\n",
      "Data loss: 0.0005004314007237554, Function loss: 0.005706944968551397\n",
      "Step: 2740, Loss: 0.006207376252859831\n",
      "Data loss: 0.0004912737640552223, Function loss: 0.0009713051258586347\n",
      "Step: 2741, Loss: 0.001462578889913857\n",
      "Data loss: 0.0005922873970121145, Function loss: 0.0027741892263293266\n",
      "Step: 2742, Loss: 0.003366476623341441\n",
      "Data loss: 0.0005686990334652364, Function loss: 0.007379142101854086\n",
      "Step: 2743, Loss: 0.007947840727865696\n",
      "Data loss: 0.0008540153503417969, Function loss: 0.009204783476889133\n",
      "Step: 2744, Loss: 0.01005879882723093\n",
      "Data loss: 0.0005759233608841896, Function loss: 0.005780139472335577\n",
      "Step: 2745, Loss: 0.006356062833219767\n",
      "Data loss: 0.0006111467955633998, Function loss: 0.001605817349627614\n",
      "Step: 2746, Loss: 0.002216964028775692\n",
      "Data loss: 0.0005979322595521808, Function loss: 0.001239483361132443\n",
      "Step: 2747, Loss: 0.0018374156206846237\n",
      "Data loss: 0.0005761226639151573, Function loss: 0.004014116711914539\n",
      "Step: 2748, Loss: 0.004590239375829697\n",
      "Data loss: 0.0007865456864237785, Function loss: 0.005392665043473244\n",
      "Step: 2749, Loss: 0.006179210729897022\n",
      "Data loss: 0.0005829568835906684, Function loss: 0.0034740373957902193\n",
      "Step: 2750, Loss: 0.004056994337588549\n",
      "Data loss: 0.0006125349900685251, Function loss: 0.0010783266043290496\n",
      "Step: 2751, Loss: 0.0016908615361899137\n",
      "Data loss: 0.0006271260790526867, Function loss: 0.0012933131074532866\n",
      "Step: 2752, Loss: 0.0019204391865059733\n",
      "Data loss: 0.0005940098199062049, Function loss: 0.0031235532369464636\n",
      "Step: 2753, Loss: 0.0037175631150603294\n",
      "Data loss: 0.0007187397568486631, Function loss: 0.0034245101269334555\n",
      "Step: 2754, Loss: 0.004143249709159136\n",
      "Data loss: 0.0005890948814339936, Function loss: 0.0018553005065768957\n",
      "Step: 2755, Loss: 0.0024443953298032284\n",
      "Data loss: 0.000589770614169538, Function loss: 0.000820204964838922\n",
      "Step: 2756, Loss: 0.00140997557900846\n",
      "Data loss: 0.0006357621168717742, Function loss: 0.0015908718341961503\n",
      "Step: 2757, Loss: 0.0022266339510679245\n",
      "Data loss: 0.0005863749538548291, Function loss: 0.0026183375157415867\n",
      "Step: 2758, Loss: 0.003204712411388755\n",
      "Data loss: 0.0006514577544294298, Function loss: 0.0021990484092384577\n",
      "Step: 2759, Loss: 0.0028505062218755484\n",
      "Data loss: 0.0005675450665876269, Function loss: 0.0011252730619162321\n",
      "Step: 2760, Loss: 0.001692818128503859\n",
      "Data loss: 0.0005565988249145448, Function loss: 0.0008910346077755094\n",
      "Step: 2761, Loss: 0.0014476333744823933\n",
      "Data loss: 0.0006114916177466512, Function loss: 0.001570182736031711\n",
      "Step: 2762, Loss: 0.0021816743537783623\n",
      "Data loss: 0.0005486803711391985, Function loss: 0.0020066103897988796\n",
      "Step: 2763, Loss: 0.0025552907027304173\n",
      "Data loss: 0.0005927114398218691, Function loss: 0.0014735818840563297\n",
      "Step: 2764, Loss: 0.002066293265670538\n",
      "Data loss: 0.0005365581600926816, Function loss: 0.0008882181136868894\n",
      "Step: 2765, Loss: 0.001424776273779571\n",
      "Data loss: 0.0005201208987273276, Function loss: 0.0009243470849469304\n",
      "Step: 2766, Loss: 0.001444468041881919\n",
      "Data loss: 0.0005690447287634015, Function loss: 0.0013255372177809477\n",
      "Step: 2767, Loss: 0.0018945819465443492\n",
      "Data loss: 0.0005093729123473167, Function loss: 0.0015145163051784039\n",
      "Step: 2768, Loss: 0.0020238892175257206\n",
      "Data loss: 0.0005460646934807301, Function loss: 0.0011330118868499994\n",
      "Step: 2769, Loss: 0.0016790765803307295\n",
      "Data loss: 0.0005087466561235487, Function loss: 0.0008293762221001089\n",
      "Step: 2770, Loss: 0.0013381228782236576\n",
      "Data loss: 0.0004943153471685946, Function loss: 0.0009085023775696754\n",
      "Step: 2771, Loss: 0.0014028176665306091\n",
      "Data loss: 0.0005354423192329705, Function loss: 0.0011491735931485891\n",
      "Step: 2772, Loss: 0.0016846158541738987\n",
      "Data loss: 0.0004817364097107202, Function loss: 0.001286291517317295\n",
      "Step: 2773, Loss: 0.0017680278979241848\n",
      "Data loss: 0.0005198002909310162, Function loss: 0.001056994078680873\n",
      "Step: 2774, Loss: 0.0015767943114042282\n",
      "Data loss: 0.0004815178399439901, Function loss: 0.0008593934471718967\n",
      "Step: 2775, Loss: 0.0013409112580120564\n",
      "Data loss: 0.00047825812362134457, Function loss: 0.000824244343675673\n",
      "Step: 2776, Loss: 0.0013025024672970176\n",
      "Data loss: 0.0005003964761272073, Function loss: 0.0009461664594709873\n",
      "Step: 2777, Loss: 0.0014465629355981946\n",
      "Data loss: 0.0004631641786545515, Function loss: 0.0010935348691418767\n",
      "Step: 2778, Loss: 0.0015566990477964282\n",
      "Data loss: 0.0004988327855244279, Function loss: 0.0010029581608250737\n",
      "Step: 2779, Loss: 0.0015017909463495016\n",
      "Data loss: 0.00045682850759476423, Function loss: 0.0009029211942106485\n",
      "Step: 2780, Loss: 0.0013597497018054128\n",
      "Data loss: 0.00047293081297539175, Function loss: 0.0008041030960157514\n",
      "Step: 2781, Loss: 0.0012770339380949736\n",
      "Data loss: 0.0004674970405176282, Function loss: 0.0008285384392365813\n",
      "Step: 2782, Loss: 0.0012960354797542095\n",
      "Data loss: 0.00045178577420301735, Function loss: 0.0009127042721956968\n",
      "Step: 2783, Loss: 0.0013644900172948837\n",
      "Data loss: 0.00047747683129273355, Function loss: 0.0009281865204684436\n",
      "Step: 2784, Loss: 0.0014056633226573467\n",
      "Data loss: 0.00043922901386395097, Function loss: 0.0009407243342138827\n",
      "Step: 2785, Loss: 0.0013799533480778337\n",
      "Data loss: 0.00046762474812567234, Function loss: 0.0008470126194879413\n",
      "Step: 2786, Loss: 0.0013146373676136136\n",
      "Data loss: 0.00044048004201613367, Function loss: 0.0008255250286310911\n",
      "Step: 2787, Loss: 0.0012660050997510552\n",
      "Data loss: 0.000449232233222574, Function loss: 0.0008132115472108126\n",
      "Step: 2788, Loss: 0.0012624438386410475\n",
      "Data loss: 0.00044984143460169435, Function loss: 0.0008393332245759666\n",
      "Step: 2789, Loss: 0.001289174659177661\n",
      "Data loss: 0.00043311319313943386, Function loss: 0.000880617939401418\n",
      "Step: 2790, Loss: 0.001313731074333191\n",
      "Data loss: 0.0004539049114100635, Function loss: 0.0008660985040478408\n",
      "Step: 2791, Loss: 0.0013200034154579043\n",
      "Data loss: 0.00042377461795695126, Function loss: 0.000879799306858331\n",
      "Step: 2792, Loss: 0.0013035739539191127\n",
      "Data loss: 0.0004461608186829835, Function loss: 0.0008262592600658536\n",
      "Step: 2793, Loss: 0.0012724200496450067\n",
      "Data loss: 0.00042646672227419913, Function loss: 0.0008169086650013924\n",
      "Step: 2794, Loss: 0.001243375358171761\n",
      "Data loss: 0.00042923889122903347, Function loss: 0.0008070695912465453\n",
      "Step: 2795, Loss: 0.0012363084824755788\n",
      "Data loss: 0.00043554813601076603, Function loss: 0.0008169281063601375\n",
      "Step: 2796, Loss: 0.0012524762423709035\n",
      "Data loss: 0.0004156700742896646, Function loss: 0.0008572697988711298\n",
      "Step: 2797, Loss: 0.001272939844056964\n",
      "Data loss: 0.00043873314280062914, Function loss: 0.0008414172334596515\n",
      "Step: 2798, Loss: 0.0012801503762602806\n",
      "Data loss: 0.0004108042339794338, Function loss: 0.0008630854426883161\n",
      "Step: 2799, Loss: 0.0012738896766677499\n",
      "Data loss: 0.0004316823906265199, Function loss: 0.0008249516249634326\n",
      "Step: 2800, Loss: 0.0012566340155899525\n",
      "Data loss: 0.00041369127575308084, Function loss: 0.0008236736757680774\n",
      "Step: 2801, Loss: 0.0012373649515211582\n",
      "Data loss: 0.0004188686434645206, Function loss: 0.0008059857646003366\n",
      "Step: 2802, Loss: 0.0012248543789610267\n",
      "Data loss: 0.00041953689651563764, Function loss: 0.0008046845905482769\n",
      "Step: 2803, Loss: 0.0012242214288562536\n",
      "Data loss: 0.0004087516572326422, Function loss: 0.000821018940769136\n",
      "Step: 2804, Loss: 0.0012297705980017781\n",
      "Data loss: 0.0004206210433039814, Function loss: 0.0008120449492707849\n",
      "Step: 2805, Loss: 0.0012326659634709358\n",
      "Data loss: 0.00040558332693763077, Function loss: 0.0008268741075880826\n",
      "Step: 2806, Loss: 0.0012324574636295438\n",
      "Data loss: 0.0004155481292400509, Function loss: 0.000814340659417212\n",
      "Step: 2807, Loss: 0.0012298887595534325\n",
      "Data loss: 0.0004064841486979276, Function loss: 0.0008181756129488349\n",
      "Step: 2808, Loss: 0.001224659732542932\n",
      "Data loss: 0.00040915844147093594, Function loss: 0.0008109381305985153\n",
      "Step: 2809, Loss: 0.0012200966011732817\n",
      "Data loss: 0.0004067948611918837, Function loss: 0.0008087920141406357\n",
      "Step: 2810, Loss: 0.0012155869044363499\n",
      "Data loss: 0.0004048041009809822, Function loss: 0.0008065052679739892\n",
      "Step: 2811, Loss: 0.001211309339851141\n",
      "Data loss: 0.00040512019768357277, Function loss: 0.0008041554247029126\n",
      "Step: 2812, Loss: 0.0012092755641788244\n",
      "Data loss: 0.0004024255904369056, Function loss: 0.0008066447917371988\n",
      "Step: 2813, Loss: 0.0012090704403817654\n",
      "Data loss: 0.0004029886913485825, Function loss: 0.0008062943816184998\n",
      "Step: 2814, Loss: 0.0012092830147594213\n",
      "Data loss: 0.00040027001523412764, Function loss: 0.0008131873910315335\n",
      "Step: 2815, Loss: 0.0012134574353694916\n",
      "Data loss: 0.0004016715392936021, Function loss: 0.0008109343471005559\n",
      "Step: 2816, Loss: 0.0012126058572903275\n",
      "Data loss: 0.0003979981120210141, Function loss: 0.0008141177822835743\n",
      "Step: 2817, Loss: 0.001212115865200758\n",
      "Data loss: 0.00039993913378566504, Function loss: 0.0008079514955170453\n",
      "Step: 2818, Loss: 0.0012078906875103712\n",
      "Data loss: 0.0003959959140047431, Function loss: 0.000808035081718117\n",
      "Step: 2819, Loss: 0.001204031053930521\n",
      "Data loss: 0.0003979360917583108, Function loss: 0.0008032128098420799\n",
      "Step: 2820, Loss: 0.0012011488433927298\n",
      "Data loss: 0.00039468103204853833, Function loss: 0.0008045356371439993\n",
      "Step: 2821, Loss: 0.0011992166982963681\n",
      "Data loss: 0.0003960976318921894, Function loss: 0.0008024552371352911\n",
      "Step: 2822, Loss: 0.001198552898131311\n",
      "Data loss: 0.0003937515430152416, Function loss: 0.0008045544964261353\n",
      "Step: 2823, Loss: 0.001198305981233716\n",
      "Data loss: 0.0003938977897632867, Function loss: 0.0008036301587708294\n",
      "Step: 2824, Loss: 0.0011975279776379466\n",
      "Data loss: 0.00039286803803406656, Function loss: 0.0008023821283131838\n",
      "Step: 2825, Loss: 0.0011952501954510808\n",
      "Data loss: 0.00039175886195153, Function loss: 0.0008026270079426467\n",
      "Step: 2826, Loss: 0.0011943858116865158\n",
      "Data loss: 0.00039202749030664563, Function loss: 0.0008008676813915372\n",
      "Step: 2827, Loss: 0.0011928952299058437\n",
      "Data loss: 0.0003898360882885754, Function loss: 0.0008023318368941545\n",
      "Step: 2828, Loss: 0.001192167866975069\n",
      "Data loss: 0.0003910171799361706, Function loss: 0.000800824724137783\n",
      "Step: 2829, Loss: 0.0011918419040739536\n",
      "Data loss: 0.0003887249040417373, Function loss: 0.0008029668824747205\n",
      "Step: 2830, Loss: 0.0011916917283087969\n",
      "Data loss: 0.00038922150270082057, Function loss: 0.0008013123879209161\n",
      "Step: 2831, Loss: 0.0011905338615179062\n",
      "Data loss: 0.00038880406646057963, Function loss: 0.0008020840468816459\n",
      "Step: 2832, Loss: 0.0011908881133422256\n",
      "Data loss: 0.00038708854117430747, Function loss: 0.000802497670520097\n",
      "Step: 2833, Loss: 0.001189586240798235\n",
      "Data loss: 0.0003888413484673947, Function loss: 0.0008000925881788135\n",
      "Step: 2834, Loss: 0.0011889339657500386\n",
      "Data loss: 0.000384666258469224, Function loss: 0.0008040596731007099\n",
      "Step: 2835, Loss: 0.0011887259315699339\n",
      "Data loss: 0.00038963762926869094, Function loss: 0.0007984722615219653\n",
      "Step: 2836, Loss: 0.0011881098616868258\n",
      "Data loss: 0.0003816857933998108, Function loss: 0.0008092194329947233\n",
      "Step: 2837, Loss: 0.0011909052263945341\n",
      "Data loss: 0.00039197670412249863, Function loss: 0.0008037369116209447\n",
      "Step: 2838, Loss: 0.0011957136448472738\n",
      "Data loss: 0.0003786774177569896, Function loss: 0.000819637905806303\n",
      "Step: 2839, Loss: 0.0011983152944594622\n",
      "Data loss: 0.00039195772842504084, Function loss: 0.0008075148798525333\n",
      "Step: 2840, Loss: 0.0011994725791737437\n",
      "Data loss: 0.00037789507769048214, Function loss: 0.0008188366773538291\n",
      "Step: 2841, Loss: 0.0011967318132519722\n",
      "Data loss: 0.00038962700637057424, Function loss: 0.0008039521635510027\n",
      "Step: 2842, Loss: 0.001193579169921577\n",
      "Data loss: 0.0003788041358347982, Function loss: 0.0008106072782538831\n",
      "Step: 2843, Loss: 0.0011894113849848509\n",
      "Data loss: 0.00038701543235220015, Function loss: 0.0007990958984009922\n",
      "Step: 2844, Loss: 0.0011861113598570228\n",
      "Data loss: 0.00037924520438537, Function loss: 0.0008043353445827961\n",
      "Step: 2845, Loss: 0.001183580607175827\n",
      "Data loss: 0.00038474236498586833, Function loss: 0.0007966157863847911\n",
      "Step: 2846, Loss: 0.001181358122266829\n",
      "Data loss: 0.0003799378755502403, Function loss: 0.0007985947886481881\n",
      "Step: 2847, Loss: 0.0011785326059907675\n",
      "Data loss: 0.0003830939531326294, Function loss: 0.000793923856690526\n",
      "Step: 2848, Loss: 0.0011770178098231554\n",
      "Data loss: 0.0003794573713093996, Function loss: 0.0007960773655213416\n",
      "Step: 2849, Loss: 0.0011755346786230803\n",
      "Data loss: 0.0003831657231785357, Function loss: 0.0007923926459625363\n",
      "Step: 2850, Loss: 0.001175558427348733\n",
      "Data loss: 0.0003775125660467893, Function loss: 0.0007983040413819253\n",
      "Step: 2851, Loss: 0.001175816636532545\n",
      "Data loss: 0.0003843747836071998, Function loss: 0.0007946772966533899\n",
      "Step: 2852, Loss: 0.0011790520511567593\n",
      "Data loss: 0.00037475250428542495, Function loss: 0.000811631151009351\n",
      "Step: 2853, Loss: 0.001186383655294776\n",
      "Data loss: 0.0003864897589664906, Function loss: 0.0008099651895463467\n",
      "Step: 2854, Loss: 0.0011964549776166677\n",
      "Data loss: 0.0003725790011230856, Function loss: 0.0008387318230234087\n",
      "Step: 2855, Loss: 0.0012113108532503247\n",
      "Data loss: 0.00038877263432368636, Function loss: 0.0008396620978601277\n",
      "Step: 2856, Loss: 0.001228434732183814\n",
      "Data loss: 0.0003712989273481071, Function loss: 0.0008770691929385066\n",
      "Step: 2857, Loss: 0.0012483680620789528\n",
      "Data loss: 0.00039107943302951753, Function loss: 0.0008799175266176462\n",
      "Step: 2858, Loss: 0.0012709969887509942\n",
      "Data loss: 0.000370471621863544, Function loss: 0.0009228082490153611\n",
      "Step: 2859, Loss: 0.001293279929086566\n",
      "Data loss: 0.0003931972896680236, Function loss: 0.0009234455646947026\n",
      "Step: 2860, Loss: 0.0013166428543627262\n",
      "Data loss: 0.00036923287552781403, Function loss: 0.0009726288844831288\n",
      "Step: 2861, Loss: 0.0013418617891147733\n",
      "Data loss: 0.0003969043609686196, Function loss: 0.000978496391326189\n",
      "Step: 2862, Loss: 0.0013754006940871477\n",
      "Data loss: 0.0003665095428004861, Function loss: 0.0010395415592938662\n",
      "Step: 2863, Loss: 0.0014060511020943522\n",
      "Data loss: 0.0004017179599031806, Function loss: 0.0010492244036868215\n",
      "Step: 2864, Loss: 0.001450942363590002\n",
      "Data loss: 0.0003629369311966002, Function loss: 0.0011476343497633934\n",
      "Step: 2865, Loss: 0.0015105712227523327\n",
      "Data loss: 0.00041065170080401003, Function loss: 0.0012081583263352513\n",
      "Step: 2866, Loss: 0.001618809998035431\n",
      "Data loss: 0.0003608996339607984, Function loss: 0.00141911581158638\n",
      "Step: 2867, Loss: 0.001780015416443348\n",
      "Data loss: 0.00042726018000394106, Function loss: 0.0015986019279807806\n",
      "Step: 2868, Loss: 0.0020258622244000435\n",
      "Data loss: 0.0003617095935624093, Function loss: 0.0019510153215378523\n",
      "Step: 2869, Loss: 0.002312724944204092\n",
      "Data loss: 0.00045198280713520944, Function loss: 0.0022693441715091467\n",
      "Step: 2870, Loss: 0.0027213268913328648\n",
      "Data loss: 0.0003668713034130633, Function loss: 0.002763293916359544\n",
      "Step: 2871, Loss: 0.003130165161564946\n",
      "Data loss: 0.0004818014276679605, Function loss: 0.0031652955804020166\n",
      "Step: 2872, Loss: 0.003647096920758486\n",
      "Data loss: 0.0003755025682039559, Function loss: 0.003650975413620472\n",
      "Step: 2873, Loss: 0.004026478156447411\n",
      "Data loss: 0.0005086948513053358, Function loss: 0.004013299010694027\n",
      "Step: 2874, Loss: 0.004521993920207024\n",
      "Data loss: 0.00038429515552707016, Function loss: 0.004366720095276833\n",
      "Step: 2875, Loss: 0.004751015454530716\n",
      "Data loss: 0.0005275392904877663, Function loss: 0.004612701945006847\n",
      "Step: 2876, Loss: 0.005140241235494614\n",
      "Data loss: 0.00039195033605210483, Function loss: 0.004772550892084837\n",
      "Step: 2877, Loss: 0.005164501257240772\n",
      "Data loss: 0.0005325694219209254, Function loss: 0.004727756138890982\n",
      "Step: 2878, Loss: 0.005260325502604246\n",
      "Data loss: 0.0003947558579966426, Function loss: 0.0044507491402328014\n",
      "Step: 2879, Loss: 0.004845505114644766\n",
      "Data loss: 0.0005103833973407745, Function loss: 0.0038761741016060114\n",
      "Step: 2880, Loss: 0.004386557266116142\n",
      "Data loss: 0.00039059907430782914, Function loss: 0.003127444302663207\n",
      "Step: 2881, Loss: 0.0035180433187633753\n",
      "Data loss: 0.00046218992793001235, Function loss: 0.0022335434332489967\n",
      "Step: 2882, Loss: 0.0026957334484905005\n",
      "Data loss: 0.00038693335955031216, Function loss: 0.0015350999310612679\n",
      "Step: 2883, Loss: 0.0019220332615077496\n",
      "Data loss: 0.0004150543245486915, Function loss: 0.001005444792099297\n",
      "Step: 2884, Loss: 0.0014204990584403276\n",
      "Data loss: 0.00039909989573061466, Function loss: 0.0008005287381820381\n",
      "Step: 2885, Loss: 0.0011996286921203136\n",
      "Data loss: 0.0003886113699991256, Function loss: 0.0008602930465713143\n",
      "Step: 2886, Loss: 0.0012489043874666095\n",
      "Data loss: 0.00042487980681471527, Function loss: 0.0010548161808401346\n",
      "Step: 2887, Loss: 0.0014796960167586803\n",
      "Data loss: 0.0003855061368085444, Function loss: 0.001385879935696721\n",
      "Step: 2888, Loss: 0.0017713860142976046\n",
      "Data loss: 0.0004485382232815027, Function loss: 0.0015886437613517046\n",
      "Step: 2889, Loss: 0.0020371819846332073\n",
      "Data loss: 0.0003954672138206661, Function loss: 0.0017719793831929564\n",
      "Step: 2890, Loss: 0.0021674465388059616\n",
      "Data loss: 0.0004560632223729044, Function loss: 0.0017404458485543728\n",
      "Step: 2891, Loss: 0.0021965091582387686\n",
      "Data loss: 0.0004060083592776209, Function loss: 0.0016572551103308797\n",
      "Step: 2892, Loss: 0.00206326344050467\n",
      "Data loss: 0.00044245616300031543, Function loss: 0.001418256782926619\n",
      "Step: 2893, Loss: 0.0018607128877192736\n",
      "Data loss: 0.0004088113782927394, Function loss: 0.0012063027825206518\n",
      "Step: 2894, Loss: 0.0016151141608133912\n",
      "Data loss: 0.0004177969822194427, Function loss: 0.0009836251847445965\n",
      "Step: 2895, Loss: 0.0014014221960678697\n",
      "Data loss: 0.00040486871148459613, Function loss: 0.0008354209712706506\n",
      "Step: 2896, Loss: 0.0012402896536514163\n",
      "Data loss: 0.0003988115058746189, Function loss: 0.0007666311576031148\n",
      "Step: 2897, Loss: 0.0011654426343739033\n",
      "Data loss: 0.00040617695776745677, Function loss: 0.0007686692988499999\n",
      "Step: 2898, Loss: 0.0011748461984097958\n",
      "Data loss: 0.00039472844218835235, Function loss: 0.0008468930027447641\n",
      "Step: 2899, Loss: 0.0012416214449331164\n",
      "Data loss: 0.00041358330054208636, Function loss: 0.0009199836058542132\n",
      "Step: 2900, Loss: 0.0013335668481886387\n",
      "Data loss: 0.0003986614174209535, Function loss: 0.0010098371421918273\n",
      "Step: 2901, Loss: 0.00140849850140512\n",
      "Data loss: 0.0004178703820798546, Function loss: 0.0010286931646987796\n",
      "Step: 2902, Loss: 0.0014465635176748037\n",
      "Data loss: 0.0003973566635977477, Function loss: 0.0010327119380235672\n",
      "Step: 2903, Loss: 0.0014300686307251453\n",
      "Data loss: 0.00041379191679880023, Function loss: 0.0009649456478655338\n",
      "Step: 2904, Loss: 0.001378737622871995\n",
      "Data loss: 0.0003905085031874478, Function loss: 0.0009169048862531781\n",
      "Step: 2905, Loss: 0.0013074134476482868\n",
      "Data loss: 0.00040755647933110595, Function loss: 0.000837550382129848\n",
      "Step: 2906, Loss: 0.001245106803253293\n",
      "Data loss: 0.00038522420800291, Function loss: 0.000816384912468493\n",
      "Step: 2907, Loss: 0.0012016091495752335\n",
      "Data loss: 0.0004046647227369249, Function loss: 0.0007733105449005961\n",
      "Step: 2908, Loss: 0.0011779752094298601\n",
      "Data loss: 0.0003857916744891554, Function loss: 0.0007823421619832516\n",
      "Step: 2909, Loss: 0.0011681338073685765\n",
      "Data loss: 0.00040136463940143585, Function loss: 0.0007638480747118592\n",
      "Step: 2910, Loss: 0.001165212714113295\n",
      "Data loss: 0.0003888631472364068, Function loss: 0.0007758454885333776\n",
      "Step: 2911, Loss: 0.0011647086357697845\n",
      "Data loss: 0.00039520527934655547, Function loss: 0.0007665369776077569\n",
      "Step: 2912, Loss: 0.0011617422569543123\n",
      "Data loss: 0.0003920200979337096, Function loss: 0.000766199897043407\n",
      "Step: 2913, Loss: 0.0011582199949771166\n",
      "Data loss: 0.00038646801840513945, Function loss: 0.0007726111798547208\n",
      "Step: 2914, Loss: 0.0011590791400521994\n",
      "Data loss: 0.00039693096186965704, Function loss: 0.0007714045932516456\n",
      "Step: 2915, Loss: 0.0011683355551213026\n",
      "Data loss: 0.00037847409839741886, Function loss: 0.0008052831399254501\n",
      "Step: 2916, Loss: 0.0011837572092190385\n",
      "Data loss: 0.00040306494338437915, Function loss: 0.0008017318323254585\n",
      "Step: 2917, Loss: 0.0012047968339174986\n",
      "Data loss: 0.0003733574412763119, Function loss: 0.0008603494497947395\n",
      "Step: 2918, Loss: 0.0012337069492787123\n",
      "Data loss: 0.0004096228221897036, Function loss: 0.0008600534638389945\n",
      "Step: 2919, Loss: 0.0012696762569248676\n",
      "Data loss: 0.00036912644281983376, Function loss: 0.0009414271917194128\n",
      "Step: 2920, Loss: 0.0013105536345392466\n",
      "Data loss: 0.00041545421117916703, Function loss: 0.0009412420331500471\n",
      "Step: 2921, Loss: 0.001356696244329214\n",
      "Data loss: 0.0003653059247881174, Function loss: 0.0010499530471861362\n",
      "Step: 2922, Loss: 0.0014152589719742537\n",
      "Data loss: 0.0004224668082315475, Function loss: 0.001078793779015541\n",
      "Step: 2923, Loss: 0.001501260558143258\n",
      "Data loss: 0.00036161686875857413, Function loss: 0.0012411812786012888\n",
      "Step: 2924, Loss: 0.0016027981182560325\n",
      "Data loss: 0.00043301444384269416, Function loss: 0.0013156280620023608\n",
      "Step: 2925, Loss: 0.0017486425349488854\n",
      "Data loss: 0.00035884606768377125, Function loss: 0.0015822640853002667\n",
      "Step: 2926, Loss: 0.0019411101238802075\n",
      "Data loss: 0.00045116228284314275, Function loss: 0.0018055485561490059\n",
      "Step: 2927, Loss: 0.0022567107807844877\n",
      "Data loss: 0.00036001187982037663, Function loss: 0.002295777201652527\n",
      "Step: 2928, Loss: 0.0026557890232652426\n",
      "Data loss: 0.0004838046443182975, Function loss: 0.0027982809115201235\n",
      "Step: 2929, Loss: 0.0032820855267345905\n",
      "Data loss: 0.00037069598329253495, Function loss: 0.0036257135216146708\n",
      "Step: 2930, Loss: 0.003996409475803375\n",
      "Data loss: 0.0005345979589037597, Function loss: 0.004507286474108696\n",
      "Step: 2931, Loss: 0.005041884258389473\n",
      "Data loss: 0.00039538543205708265, Function loss: 0.005539741832762957\n",
      "Step: 2932, Loss: 0.005935127381235361\n",
      "Data loss: 0.0005935037042945623, Function loss: 0.006609140895307064\n",
      "Step: 2933, Loss: 0.00720264483243227\n",
      "Data loss: 0.00042982163722626865, Function loss: 0.007453911937773228\n",
      "Step: 2934, Loss: 0.00788373313844204\n",
      "Data loss: 0.0006378411198966205, Function loss: 0.00814205501228571\n",
      "Step: 2935, Loss: 0.008779896423220634\n",
      "Data loss: 0.0004562536778394133, Function loss: 0.007889125496149063\n",
      "Step: 2936, Loss: 0.008345379494130611\n",
      "Data loss: 0.0006205277168191969, Function loss: 0.0071281869895756245\n",
      "Step: 2937, Loss: 0.0077487146481871605\n",
      "Data loss: 0.00044951506424695253, Function loss: 0.005507260095328093\n",
      "Step: 2938, Loss: 0.005956775043159723\n",
      "Data loss: 0.0005345843965187669, Function loss: 0.003685890231281519\n",
      "Step: 2939, Loss: 0.004220474511384964\n",
      "Data loss: 0.00042875943472608924, Function loss: 0.0020765054505318403\n",
      "Step: 2940, Loss: 0.0025052649434655905\n",
      "Data loss: 0.000439762749010697, Function loss: 0.0010903466027230024\n",
      "Step: 2941, Loss: 0.001530109322629869\n",
      "Data loss: 0.0004456227470654994, Function loss: 0.0010222150012850761\n",
      "Step: 2942, Loss: 0.0014678377192467451\n",
      "Data loss: 0.00040428683860227466, Function loss: 0.0016449450049549341\n",
      "Step: 2943, Loss: 0.0020492319017648697\n",
      "Data loss: 0.0004950830480083823, Function loss: 0.0022522772196680307\n",
      "Step: 2944, Loss: 0.0027473601512610912\n",
      "Data loss: 0.0004111227171961218, Function loss: 0.0026227214839309454\n",
      "Step: 2945, Loss: 0.0030338442884385586\n",
      "Data loss: 0.0005050895269960165, Function loss: 0.0024059440474957228\n",
      "Step: 2946, Loss: 0.0029110335744917393\n",
      "Data loss: 0.0004263472219463438, Function loss: 0.001922975410707295\n",
      "Step: 2947, Loss: 0.002349322661757469\n",
      "Data loss: 0.0004699133860412985, Function loss: 0.0013426091754809022\n",
      "Step: 2948, Loss: 0.0018125225324183702\n",
      "Data loss: 0.0004514729662332684, Function loss: 0.0010575343621894717\n",
      "Step: 2949, Loss: 0.0015090072993189096\n",
      "Data loss: 0.0004350746748968959, Function loss: 0.0010656273225322366\n",
      "Step: 2950, Loss: 0.0015007019974291325\n",
      "Data loss: 0.00047496051411144435, Function loss: 0.0011855574557557702\n",
      "Step: 2951, Loss: 0.001660517998971045\n",
      "Data loss: 0.00041275497642345726, Function loss: 0.0013903407379984856\n",
      "Step: 2952, Loss: 0.0018030956853181124\n",
      "Data loss: 0.0004717667761724442, Function loss: 0.00134464621078223\n",
      "Step: 2953, Loss: 0.0018164130160585046\n",
      "Data loss: 0.0004106872947886586, Function loss: 0.0012723278487101197\n",
      "Step: 2954, Loss: 0.0016830151434987783\n",
      "Data loss: 0.0004496191977523267, Function loss: 0.0010798802832141519\n",
      "Step: 2955, Loss: 0.0015294994227588177\n",
      "Data loss: 0.00043862187885679305, Function loss: 0.00100617331918329\n",
      "Step: 2956, Loss: 0.0014447951689362526\n",
      "Data loss: 0.00043517007725313306, Function loss: 0.001047830330207944\n",
      "Step: 2957, Loss: 0.001483000349253416\n",
      "Data loss: 0.0004764088080264628, Function loss: 0.0011098807444795966\n",
      "Step: 2958, Loss: 0.0015862896107137203\n",
      "Data loss: 0.0004249547200743109, Function loss: 0.0012340763350948691\n",
      "Step: 2959, Loss: 0.0016590310260653496\n",
      "Data loss: 0.0004809225210919976, Function loss: 0.001157088903710246\n",
      "Step: 2960, Loss: 0.0016380114248022437\n",
      "Data loss: 0.00040848428034223616, Function loss: 0.0011204651091247797\n",
      "Step: 2961, Loss: 0.0015289493603631854\n",
      "Data loss: 0.00045322944060899317, Function loss: 0.0009264355758205056\n",
      "Step: 2962, Loss: 0.0013796649873256683\n",
      "Data loss: 0.0004000384360551834, Function loss: 0.0008424214902333915\n",
      "Step: 2963, Loss: 0.0012424599844962358\n",
      "Data loss: 0.0004222012066747993, Function loss: 0.000731324078515172\n",
      "Step: 2964, Loss: 0.0011535253142938018\n",
      "Data loss: 0.0004059151979163289, Function loss: 0.000712418113835156\n",
      "Step: 2965, Loss: 0.0011183333117514849\n",
      "Data loss: 0.0004043178050778806, Function loss: 0.0007214304059743881\n",
      "Step: 2966, Loss: 0.0011257482692599297\n",
      "Data loss: 0.00042007010779343545, Function loss: 0.0007448969990946352\n",
      "Step: 2967, Loss: 0.0011649670777842402\n",
      "Data loss: 0.0003956163418479264, Function loss: 0.0008199840667657554\n",
      "Step: 2968, Loss: 0.0012156004086136818\n",
      "Data loss: 0.00043283874401822686, Function loss: 0.0008345350506715477\n",
      "Step: 2969, Loss: 0.0012673737946897745\n",
      "Data loss: 0.00039188243681564927, Function loss: 0.0009031417430378497\n",
      "Step: 2970, Loss: 0.001295024179853499\n",
      "Data loss: 0.0004335510020609945, Function loss: 0.0008557048859074712\n",
      "Step: 2971, Loss: 0.0012892559170722961\n",
      "Data loss: 0.0003892984823323786, Function loss: 0.000867311202455312\n",
      "Step: 2972, Loss: 0.0012566096847876906\n",
      "Data loss: 0.0004206954035907984, Function loss: 0.0007889681146480143\n",
      "Step: 2973, Loss: 0.0012096634600311518\n",
      "Data loss: 0.00038948521250858903, Function loss: 0.00077025406062603\n",
      "Step: 2974, Loss: 0.001159739214926958\n",
      "Data loss: 0.00040348671609535813, Function loss: 0.0007190919714048505\n",
      "Step: 2975, Loss: 0.0011225787457078695\n",
      "Data loss: 0.00039369650767184794, Function loss: 0.0007103659445419908\n",
      "Step: 2976, Loss: 0.0011040624231100082\n",
      "Data loss: 0.0003898206923622638, Function loss: 0.0007131302263587713\n",
      "Step: 2977, Loss: 0.0011029508896172047\n",
      "Data loss: 0.0004005166993010789, Function loss: 0.0007132118917070329\n",
      "Step: 2978, Loss: 0.0011137286201119423\n",
      "Data loss: 0.0003822200815193355, Function loss: 0.000747878453694284\n",
      "Step: 2979, Loss: 0.0011300984770059586\n",
      "Data loss: 0.00040582293877378106, Function loss: 0.0007414651336148381\n",
      "Step: 2980, Loss: 0.00114728813059628\n",
      "Data loss: 0.00037941435584798455, Function loss: 0.0007832074770703912\n",
      "Step: 2981, Loss: 0.0011626218911260366\n",
      "Data loss: 0.0004080696380697191, Function loss: 0.0007652200874872506\n",
      "Step: 2982, Loss: 0.0011732897255569696\n",
      "Data loss: 0.0003802562423516065, Function loss: 0.0007982708048075438\n",
      "Step: 2983, Loss: 0.0011785270180553198\n",
      "Data loss: 0.00040595681639388204, Function loss: 0.0007800140301696956\n",
      "Step: 2984, Loss: 0.0011859708465635777\n",
      "Data loss: 0.0003836807154584676, Function loss: 0.0008160028955899179\n",
      "Step: 2985, Loss: 0.001199683640152216\n",
      "Data loss: 0.00040259287925437093, Function loss: 0.0008190968655981123\n",
      "Step: 2986, Loss: 0.0012216897448524833\n",
      "Data loss: 0.00039020198164507747, Function loss: 0.0008730011177249253\n",
      "Step: 2987, Loss: 0.0012632030993700027\n",
      "Data loss: 0.0003995114238932729, Function loss: 0.0009258904028683901\n",
      "Step: 2988, Loss: 0.001325401826761663\n",
      "Data loss: 0.0003993673308286816, Function loss: 0.0010036517633125186\n",
      "Step: 2989, Loss: 0.0014030190650373697\n",
      "Data loss: 0.000400148710468784, Function loss: 0.001086799311451614\n",
      "Step: 2990, Loss: 0.0014869479928165674\n",
      "Data loss: 0.00040896996506489813, Function loss: 0.0011821917723864317\n",
      "Step: 2991, Loss: 0.0015911617083474994\n",
      "Data loss: 0.0004017612664029002, Function loss: 0.0013013796415179968\n",
      "Step: 2992, Loss: 0.001703140907920897\n",
      "Data loss: 0.00042034004582092166, Function loss: 0.0014186864718794823\n",
      "Step: 2993, Loss: 0.0018390265759080648\n",
      "Data loss: 0.0004025071393698454, Function loss: 0.0015721437521278858\n",
      "Step: 2994, Loss: 0.001974650891497731\n",
      "Data loss: 0.00043347731116227806, Function loss: 0.001710715820081532\n",
      "Step: 2995, Loss: 0.0021441930439323187\n",
      "Data loss: 0.0004063646192662418, Function loss: 0.001896158093586564\n",
      "Step: 2996, Loss: 0.0023025227710604668\n",
      "Data loss: 0.00044838758185505867, Function loss: 0.0020537632517516613\n",
      "Step: 2997, Loss: 0.00250215083360672\n",
      "Data loss: 0.0004115081101190299, Function loss: 0.002263033529743552\n",
      "Step: 2998, Loss: 0.0026745416689664125\n",
      "Data loss: 0.0004636499797925353, Function loss: 0.0024327714927494526\n",
      "Step: 2999, Loss: 0.0028964215889573097\n",
      "Data loss: 0.0004137184005230665, Function loss: 0.002644387073814869\n",
      "Step: 3000, Loss: 0.0030581054743379354\n",
      "Data loss: 0.0004776072164531797, Function loss: 0.0028158877976238728\n",
      "Step: 3001, Loss: 0.003293494926765561\n",
      "Data loss: 0.0004139295779168606, Function loss: 0.0030356557108461857\n",
      "Step: 3002, Loss: 0.0034495852887630463\n",
      "Data loss: 0.0004908253904432058, Function loss: 0.003186976769939065\n",
      "Step: 3003, Loss: 0.003677802160382271\n",
      "Data loss: 0.0004120364901609719, Function loss: 0.003292153123766184\n",
      "Step: 3004, Loss: 0.003704189555719495\n",
      "Data loss: 0.0004930470022372901, Function loss: 0.0032439338974654675\n",
      "Step: 3005, Loss: 0.0037369809579104185\n",
      "Data loss: 0.00040431859088130295, Function loss: 0.003121656132861972\n",
      "Step: 3006, Loss: 0.0035259746946394444\n",
      "Data loss: 0.00047855457523837686, Function loss: 0.0028209539595991373\n",
      "Step: 3007, Loss: 0.0032995084766298532\n",
      "Data loss: 0.0003925718483515084, Function loss: 0.002462922828271985\n",
      "Step: 3008, Loss: 0.0028554946184158325\n",
      "Data loss: 0.0004491237341426313, Function loss: 0.0019689633045345545\n",
      "Step: 3009, Loss: 0.0024180870968848467\n",
      "Data loss: 0.00038319407030940056, Function loss: 0.0015762074617668986\n",
      "Step: 3010, Loss: 0.001959401648491621\n",
      "Data loss: 0.00041787693044170737, Function loss: 0.00117740617133677\n",
      "Step: 3011, Loss: 0.0015952831599861383\n",
      "Data loss: 0.0003818123077508062, Function loss: 0.0009272153838537633\n",
      "Step: 3012, Loss: 0.001309027662500739\n",
      "Data loss: 0.00039366164128296077, Function loss: 0.0007526298286393285\n",
      "Step: 3013, Loss: 0.0011462914990261197\n",
      "Data loss: 0.00038858558400534093, Function loss: 0.000694962393026799\n",
      "Step: 3014, Loss: 0.0010835479479283094\n",
      "Data loss: 0.000381377583835274, Function loss: 0.000713004672434181\n",
      "Step: 3015, Loss: 0.001094382256269455\n",
      "Data loss: 0.0003986879892181605, Function loss: 0.0007593808113597333\n",
      "Step: 3016, Loss: 0.0011580687714740634\n",
      "Data loss: 0.00037725112633779645, Function loss: 0.0008740437915548682\n",
      "Step: 3017, Loss: 0.0012512949761003256\n",
      "Data loss: 0.0004084756365045905, Function loss: 0.0009461973095312715\n",
      "Step: 3018, Loss: 0.001354672946035862\n",
      "Data loss: 0.00037862794124521315, Function loss: 0.0010562746319919825\n",
      "Step: 3019, Loss: 0.0014349025441333652\n",
      "Data loss: 0.00041354805580340326, Function loss: 0.0010818419978022575\n",
      "Step: 3020, Loss: 0.0014953900827094913\n",
      "Data loss: 0.0003806439053732902, Function loss: 0.0011375969043001533\n",
      "Step: 3021, Loss: 0.001518240780569613\n",
      "Data loss: 0.0004137813812121749, Function loss: 0.0011202538153156638\n",
      "Step: 3022, Loss: 0.0015340351965278387\n",
      "Data loss: 0.0003827525652013719, Function loss: 0.0011455820640549064\n",
      "Step: 3023, Loss: 0.0015283345710486174\n",
      "Data loss: 0.0004117849748581648, Function loss: 0.0011061321711167693\n",
      "Step: 3024, Loss: 0.001517917145974934\n",
      "Data loss: 0.0003840303688775748, Function loss: 0.0010923747904598713\n",
      "Step: 3025, Loss: 0.0014764051884412766\n",
      "Data loss: 0.00040622815140523016, Function loss: 0.0010223868303000927\n",
      "Step: 3026, Loss: 0.0014286149526014924\n",
      "Data loss: 0.00038655908429063857, Function loss: 0.0009894062532112002\n",
      "Step: 3027, Loss: 0.0013759653083980083\n",
      "Data loss: 0.0003995953593403101, Function loss: 0.0009349497850053012\n",
      "Step: 3028, Loss: 0.0013345452025532722\n",
      "Data loss: 0.00039089942583814263, Function loss: 0.0009102968615479767\n",
      "Step: 3029, Loss: 0.0013011962873861194\n",
      "Data loss: 0.0003939131856895983, Function loss: 0.0008868529694154859\n",
      "Step: 3030, Loss: 0.001280766213312745\n",
      "Data loss: 0.00039683308568783104, Function loss: 0.0008762162178754807\n",
      "Step: 3031, Loss: 0.0012730492744594812\n",
      "Data loss: 0.00039000128163024783, Function loss: 0.0008841250091791153\n",
      "Step: 3032, Loss: 0.001274126349017024\n",
      "Data loss: 0.00040476725553162396, Function loss: 0.0008780164062045515\n",
      "Step: 3033, Loss: 0.0012827836908400059\n",
      "Data loss: 0.00038667532498948276, Function loss: 0.0009150850237347186\n",
      "Step: 3034, Loss: 0.0013017603196203709\n",
      "Data loss: 0.0004154457419645041, Function loss: 0.0009198918705806136\n",
      "Step: 3035, Loss: 0.0013353376416489482\n",
      "Data loss: 0.0003853513626381755, Function loss: 0.0010080115171149373\n",
      "Step: 3036, Loss: 0.0013933628797531128\n",
      "Data loss: 0.00043401698349043727, Function loss: 0.0010597952641546726\n",
      "Step: 3037, Loss: 0.0014938123058527708\n",
      "Data loss: 0.00039067844045348465, Function loss: 0.0012643790105357766\n",
      "Step: 3038, Loss: 0.0016550574218854308\n",
      "Data loss: 0.0004712192458100617, Function loss: 0.0014152516378089786\n",
      "Step: 3039, Loss: 0.0018864709418267012\n",
      "Data loss: 0.0004085705440957099, Function loss: 0.0017872637836262584\n",
      "Step: 3040, Loss: 0.0021958344150334597\n",
      "Data loss: 0.000530324992723763, Function loss: 0.002081802114844322\n",
      "Step: 3041, Loss: 0.0026121269911527634\n",
      "Data loss: 0.00044371350668370724, Function loss: 0.002698741387575865\n",
      "Step: 3042, Loss: 0.003142454894259572\n",
      "Data loss: 0.0006098272278904915, Function loss: 0.0031384960748255253\n",
      "Step: 3043, Loss: 0.0037483233027160168\n",
      "Data loss: 0.0004809949896298349, Function loss: 0.0038441731594502926\n",
      "Step: 3044, Loss: 0.004325168207287788\n",
      "Data loss: 0.0006637493497692049, Function loss: 0.004136428236961365\n",
      "Step: 3045, Loss: 0.0048001776449382305\n",
      "Data loss: 0.0004783780314028263, Function loss: 0.004496164154261351\n",
      "Step: 3046, Loss: 0.004974542185664177\n",
      "Data loss: 0.000634326075669378, Function loss: 0.004259620327502489\n",
      "Step: 3047, Loss: 0.004893946461379528\n",
      "Data loss: 0.00041975415660999715, Function loss: 0.0039993212558329105\n",
      "Step: 3048, Loss: 0.004419075325131416\n",
      "Data loss: 0.0005317857139743865, Function loss: 0.003309294581413269\n",
      "Step: 3049, Loss: 0.0038410802371799946\n",
      "Data loss: 0.0003690419252961874, Function loss: 0.0028313894290477037\n",
      "Step: 3050, Loss: 0.003200431354343891\n",
      "Data loss: 0.00046138223842717707, Function loss: 0.002468586666509509\n",
      "Step: 3051, Loss: 0.0029299689922481775\n",
      "Data loss: 0.00041041808435693383, Function loss: 0.0024696202017366886\n",
      "Step: 3052, Loss: 0.0028800382278859615\n",
      "Data loss: 0.00048421917017549276, Function loss: 0.002572435885667801\n",
      "Step: 3053, Loss: 0.0030566551722586155\n",
      "Data loss: 0.0004964861436747015, Function loss: 0.002660411177203059\n",
      "Step: 3054, Loss: 0.0031568973790854216\n",
      "Data loss: 0.0005092194187454879, Function loss: 0.0025871414691209793\n",
      "Step: 3055, Loss: 0.003096360946074128\n",
      "Data loss: 0.0005032918998040259, Function loss: 0.0022231563925743103\n",
      "Step: 3056, Loss: 0.0027264482341706753\n",
      "Data loss: 0.0004557764041237533, Function loss: 0.0016786301275715232\n",
      "Step: 3057, Loss: 0.0021344064734876156\n",
      "Data loss: 0.00042512398795224726, Function loss: 0.0010828773956745863\n",
      "Step: 3058, Loss: 0.001508001354523003\n",
      "Data loss: 0.0003926526114810258, Function loss: 0.0007178143714554608\n",
      "Step: 3059, Loss: 0.001110467012040317\n",
      "Data loss: 0.00039112550439313054, Function loss: 0.0006916552083566785\n",
      "Step: 3060, Loss: 0.00108278077095747\n",
      "Data loss: 0.0004150467866566032, Function loss: 0.0009286645799875259\n",
      "Step: 3061, Loss: 0.0013437113957479596\n",
      "Data loss: 0.0004277966800145805, Function loss: 0.0012222969671711326\n",
      "Step: 3062, Loss: 0.0016500935889780521\n",
      "Data loss: 0.00044361522304825485, Function loss: 0.0013494521845132113\n",
      "Step: 3063, Loss: 0.0017930674366652966\n",
      "Data loss: 0.0004329187795519829, Function loss: 0.001294087851420045\n",
      "Step: 3064, Loss: 0.0017270066309720278\n",
      "Data loss: 0.0004101059166714549, Function loss: 0.001121251960285008\n",
      "Step: 3065, Loss: 0.0015313578769564629\n",
      "Data loss: 0.00041111669270321727, Function loss: 0.0009394267690367997\n",
      "Step: 3066, Loss: 0.001350543461740017\n",
      "Data loss: 0.00037706029252149165, Function loss: 0.0008907612063921988\n",
      "Step: 3067, Loss: 0.001267821528017521\n",
      "Data loss: 0.0004161497636232525, Function loss: 0.0008665418718010187\n",
      "Step: 3068, Loss: 0.0012826916063204408\n",
      "Data loss: 0.0003804802836384624, Function loss: 0.0009369545732624829\n",
      "Step: 3069, Loss: 0.0013174348277971148\n",
      "Data loss: 0.00042420264799147844, Function loss: 0.0008964812732301652\n",
      "Step: 3070, Loss: 0.0013206838630139828\n",
      "Data loss: 0.00038726592902094126, Function loss: 0.0008955953526310623\n",
      "Step: 3071, Loss: 0.0012828612234443426\n",
      "Data loss: 0.0004101073427591473, Function loss: 0.0008046402363106608\n",
      "Step: 3072, Loss: 0.0012147475499659777\n",
      "Data loss: 0.0003884917532559484, Function loss: 0.000760705559514463\n",
      "Step: 3073, Loss: 0.0011491973418742418\n",
      "Data loss: 0.0003855995018966496, Function loss: 0.0007476697792299092\n",
      "Step: 3074, Loss: 0.0011332692811265588\n",
      "Data loss: 0.0003984900831710547, Function loss: 0.0007907605613581836\n",
      "Step: 3075, Loss: 0.0011892506154254079\n",
      "Data loss: 0.0003686601121444255, Function loss: 0.0009504797635599971\n",
      "Step: 3076, Loss: 0.0013191398466005921\n",
      "Data loss: 0.0004197655071038753, Function loss: 0.0011047747684642673\n",
      "Step: 3077, Loss: 0.001524540246464312\n",
      "Data loss: 0.00036178805748932064, Function loss: 0.0014084061840549111\n",
      "Step: 3078, Loss: 0.0017701942706480622\n",
      "Data loss: 0.0004416646552272141, Function loss: 0.0016199270030483603\n",
      "Step: 3079, Loss: 0.0020615917164832354\n",
      "Data loss: 0.00036218090099282563, Function loss: 0.001985511975362897\n",
      "Step: 3080, Loss: 0.002347692847251892\n",
      "Data loss: 0.0004606366856023669, Function loss: 0.002264136215671897\n",
      "Step: 3081, Loss: 0.002724772784858942\n",
      "Data loss: 0.000371996546164155, Function loss: 0.0027290666475892067\n",
      "Step: 3082, Loss: 0.0031010631937533617\n",
      "Data loss: 0.00048775796312838793, Function loss: 0.0031925267539918423\n",
      "Step: 3083, Loss: 0.0036802846007049084\n",
      "Data loss: 0.0003954255080316216, Function loss: 0.0038300605956465006\n",
      "Step: 3084, Loss: 0.00422548595815897\n",
      "Data loss: 0.000530663994140923, Function loss: 0.0045028990134596825\n",
      "Step: 3085, Loss: 0.005033562891185284\n",
      "Data loss: 0.00042661651968955994, Function loss: 0.005166663322597742\n",
      "Step: 3086, Loss: 0.005593279842287302\n",
      "Data loss: 0.000572479038964957, Function loss: 0.005723431706428528\n",
      "Step: 3087, Loss: 0.006295910570770502\n",
      "Data loss: 0.00044237362453714013, Function loss: 0.005859122611582279\n",
      "Step: 3088, Loss: 0.006301496177911758\n",
      "Data loss: 0.0005752118886448443, Function loss: 0.0056798215955495834\n",
      "Step: 3089, Loss: 0.006255033425986767\n",
      "Data loss: 0.0004289967764634639, Function loss: 0.004978942684829235\n",
      "Step: 3090, Loss: 0.005407939665019512\n",
      "Data loss: 0.0005277590244077146, Function loss: 0.003975524567067623\n",
      "Step: 3091, Loss: 0.004503283649682999\n",
      "Data loss: 0.0004002895439043641, Function loss: 0.002816760214045644\n",
      "Step: 3092, Loss: 0.0032170498743653297\n",
      "Data loss: 0.0004533425671979785, Function loss: 0.0016975610051304102\n",
      "Step: 3093, Loss: 0.002150903455913067\n",
      "Data loss: 0.00038784253410995007, Function loss: 0.0009872374357655644\n",
      "Step: 3094, Loss: 0.0013750799698755145\n",
      "Data loss: 0.00040165052632801235, Function loss: 0.0006466165650635958\n",
      "Step: 3095, Loss: 0.0010482671204954386\n",
      "Data loss: 0.0004137098731007427, Function loss: 0.0007420148467645049\n",
      "Step: 3096, Loss: 0.001155724748969078\n",
      "Data loss: 0.0003897573915310204, Function loss: 0.0011501831468194723\n",
      "Step: 3097, Loss: 0.0015399404801428318\n",
      "Data loss: 0.0004540276131592691, Function loss: 0.001531656482256949\n",
      "Step: 3098, Loss: 0.001985684037208557\n",
      "Data loss: 0.00039596803253516555, Function loss: 0.0018409892218187451\n",
      "Step: 3099, Loss: 0.0022369571961462498\n",
      "Data loss: 0.0004647531022783369, Function loss: 0.00179381447378546\n",
      "Step: 3100, Loss: 0.0022585676051676273\n",
      "Data loss: 0.0003963579365517944, Function loss: 0.001590538420714438\n",
      "Step: 3101, Loss: 0.001986896386370063\n",
      "Data loss: 0.00043878989527001977, Function loss: 0.0011842161184176803\n",
      "Step: 3102, Loss: 0.001623006071895361\n",
      "Data loss: 0.00039498129626736045, Function loss: 0.0008801951771602035\n",
      "Step: 3103, Loss: 0.001275176415219903\n",
      "Data loss: 0.00040825671749189496, Function loss: 0.0006652014562860131\n",
      "Step: 3104, Loss: 0.001073458231985569\n",
      "Data loss: 0.0004042672517243773, Function loss: 0.0006326226866804063\n",
      "Step: 3105, Loss: 0.001036889967508614\n",
      "Data loss: 0.00039404098060913384, Function loss: 0.0007257047109305859\n",
      "Step: 3106, Loss: 0.0011197456624358892\n",
      "Data loss: 0.00041803467320278287, Function loss: 0.0008327268878929317\n",
      "Step: 3107, Loss: 0.0012507615610957146\n",
      "Data loss: 0.00039218575693666935, Function loss: 0.0009713865001685917\n",
      "Step: 3108, Loss: 0.0013635721988976002\n",
      "Data loss: 0.0004237125103827566, Function loss: 0.0010038829641416669\n",
      "Step: 3109, Loss: 0.001427595503628254\n",
      "Data loss: 0.0003932839899789542, Function loss: 0.0010259998962283134\n",
      "Step: 3110, Loss: 0.001419283915311098\n",
      "Data loss: 0.00041791456169448793, Function loss: 0.0009445984032936394\n",
      "Step: 3111, Loss: 0.001362512935884297\n",
      "Data loss: 0.00039199364255182445, Function loss: 0.0008787880069576204\n",
      "Step: 3112, Loss: 0.0012707816204056144\n",
      "Data loss: 0.00040642134263180196, Function loss: 0.0007810124661773443\n",
      "Step: 3113, Loss: 0.0011874338379129767\n",
      "Data loss: 0.00039060183917172253, Function loss: 0.0007305860053747892\n",
      "Step: 3114, Loss: 0.0011211878154426813\n",
      "Data loss: 0.00039647019002586603, Function loss: 0.0006780606345273554\n",
      "Step: 3115, Loss: 0.0010745308827608824\n",
      "Data loss: 0.0003896266862284392, Function loss: 0.0006530478713102639\n",
      "Step: 3116, Loss: 0.0010426745284348726\n",
      "Data loss: 0.0003895443514920771, Function loss: 0.0006337337545119226\n",
      "Step: 3117, Loss: 0.0010232781060039997\n",
      "Data loss: 0.0003889831714332104, Function loss: 0.0006252395105548203\n",
      "Step: 3118, Loss: 0.0010142226237803698\n",
      "Data loss: 0.0003844540915451944, Function loss: 0.0006293923361226916\n",
      "Step: 3119, Loss: 0.001013846369460225\n",
      "Data loss: 0.0003890763327945024, Function loss: 0.0006292839534580708\n",
      "Step: 3120, Loss: 0.0010183602571487427\n",
      "Data loss: 0.0003806376189459115, Function loss: 0.0006441308069042861\n",
      "Step: 3121, Loss: 0.0010247684549540281\n",
      "Data loss: 0.0003894590481650084, Function loss: 0.0006438462296500802\n",
      "Step: 3122, Loss: 0.001033305306918919\n",
      "Data loss: 0.00037641229573637247, Function loss: 0.000670428154990077\n",
      "Step: 3123, Loss: 0.0010468404507264495\n",
      "Data loss: 0.0003919497539754957, Function loss: 0.0006798637914471328\n",
      "Step: 3124, Loss: 0.001071813516318798\n",
      "Data loss: 0.0003706103016156703, Function loss: 0.0007432344136759639\n",
      "Step: 3125, Loss: 0.0011138446861878037\n",
      "Data loss: 0.0003985623479820788, Function loss: 0.0007851551636122167\n",
      "Step: 3126, Loss: 0.0011837175115942955\n",
      "Data loss: 0.0003653931780718267, Function loss: 0.0009230091236531734\n",
      "Step: 3127, Loss: 0.001288402359932661\n",
      "Data loss: 0.00041005457751452923, Function loss: 0.0010296570835635066\n",
      "Step: 3128, Loss: 0.0014397116610780358\n",
      "Data loss: 0.00036265025846660137, Function loss: 0.0012517143040895462\n",
      "Step: 3129, Loss: 0.0016143645625561476\n",
      "Data loss: 0.0004243008734192699, Function loss: 0.0013958016643300653\n",
      "Step: 3130, Loss: 0.0018201025668531656\n",
      "Data loss: 0.0003624152159318328, Function loss: 0.001663415809161961\n",
      "Step: 3131, Loss: 0.002025831025093794\n",
      "Data loss: 0.00044070297735743225, Function loss: 0.0018641114002093673\n",
      "Step: 3132, Loss: 0.002304814290255308\n",
      "Data loss: 0.0003657766501419246, Function loss: 0.0022234239149838686\n",
      "Step: 3133, Loss: 0.002589200623333454\n",
      "Data loss: 0.0004626684240065515, Function loss: 0.0025379317812621593\n",
      "Step: 3134, Loss: 0.0030006002634763718\n",
      "Data loss: 0.0003732490004040301, Function loss: 0.00304176053032279\n",
      "Step: 3135, Loss: 0.0034150094725191593\n",
      "Data loss: 0.0004946674453094602, Function loss: 0.003566326340660453\n",
      "Step: 3136, Loss: 0.004060993902385235\n",
      "Data loss: 0.0003873541718348861, Function loss: 0.004242076072841883\n",
      "Step: 3137, Loss: 0.004629430361092091\n",
      "Data loss: 0.0005347480182535946, Function loss: 0.004914239980280399\n",
      "Step: 3138, Loss: 0.005448988173156977\n",
      "Data loss: 0.00040509580867365, Function loss: 0.005463299807161093\n",
      "Step: 3139, Loss: 0.0058683957904577255\n",
      "Data loss: 0.0005614827387034893, Function loss: 0.005824005231261253\n",
      "Step: 3140, Loss: 0.006385487969964743\n",
      "Data loss: 0.0004105382540728897, Function loss: 0.005704542156308889\n",
      "Step: 3141, Loss: 0.006115080323070288\n",
      "Data loss: 0.0005461147520691156, Function loss: 0.005204938352108002\n",
      "Step: 3142, Loss: 0.005751052871346474\n",
      "Data loss: 0.0003945477365050465, Function loss: 0.004253028426319361\n",
      "Step: 3143, Loss: 0.004647576250135899\n",
      "Data loss: 0.00048780287033878267, Function loss: 0.003085037926211953\n",
      "Step: 3144, Loss: 0.0035728407092392445\n",
      "Data loss: 0.0003758575767278671, Function loss: 0.0020164253655821085\n",
      "Step: 3145, Loss: 0.0023922829423099756\n",
      "Data loss: 0.0004267703334335238, Function loss: 0.0011220170417800546\n",
      "Step: 3146, Loss: 0.0015487873461097479\n",
      "Data loss: 0.0003855468239635229, Function loss: 0.0007345463964156806\n",
      "Step: 3147, Loss: 0.0011200932785868645\n",
      "Data loss: 0.00039841525722295046, Function loss: 0.0007277536205947399\n",
      "Step: 3148, Loss: 0.0011261688778176904\n",
      "Data loss: 0.0004235489177517593, Function loss: 0.0010081161744892597\n",
      "Step: 3149, Loss: 0.00143166515044868\n",
      "Data loss: 0.0003949680831283331, Function loss: 0.001422400469891727\n",
      "Step: 3150, Loss: 0.00181736855302006\n",
      "Data loss: 0.00045347458217293024, Function loss: 0.001690233824774623\n",
      "Step: 3151, Loss: 0.002143708523362875\n",
      "Data loss: 0.0003926276694983244, Function loss: 0.0018613392021507025\n",
      "Step: 3152, Loss: 0.002253966871649027\n",
      "Data loss: 0.00045477261301130056, Function loss: 0.0017328707035630941\n",
      "Step: 3153, Loss: 0.002187643200159073\n",
      "Data loss: 0.00038624959415756166, Function loss: 0.0015143402852118015\n",
      "Step: 3154, Loss: 0.0019005899084731936\n",
      "Data loss: 0.00043333088979125023, Function loss: 0.0011432832106947899\n",
      "Step: 3155, Loss: 0.0015766141004860401\n",
      "Data loss: 0.00038514271727763116, Function loss: 0.0008832400781102479\n",
      "Step: 3156, Loss: 0.0012683827662840486\n",
      "Data loss: 0.00041026456165127456, Function loss: 0.0006680822116322815\n",
      "Step: 3157, Loss: 0.0010783467441797256\n",
      "Data loss: 0.00039562289020977914, Function loss: 0.0006349065224640071\n",
      "Step: 3158, Loss: 0.0010305293835699558\n",
      "Data loss: 0.0003961799666285515, Function loss: 0.0006937191938050091\n",
      "Step: 3159, Loss: 0.0010898991022258997\n",
      "Data loss: 0.0004108090070076287, Function loss: 0.000797561660874635\n",
      "Step: 3160, Loss: 0.0012083706678822637\n",
      "Data loss: 0.00038682756712660193, Function loss: 0.0009533313568681479\n",
      "Step: 3161, Loss: 0.0013401589822024107\n",
      "Data loss: 0.00042202090844511986, Function loss: 0.0010214140638709068\n",
      "Step: 3162, Loss: 0.0014434349723160267\n",
      "Data loss: 0.00037978301406838, Function loss: 0.0010983094107359648\n",
      "Step: 3163, Loss: 0.0014780923957005143\n",
      "Data loss: 0.00042355232289992273, Function loss: 0.0010397114092484117\n",
      "Step: 3164, Loss: 0.0014632637612521648\n",
      "Data loss: 0.0003742624830920249, Function loss: 0.0010219962568953633\n",
      "Step: 3165, Loss: 0.0013962587108835578\n",
      "Data loss: 0.00041863424121402204, Function loss: 0.0009075519046746194\n",
      "Step: 3166, Loss: 0.001326186116784811\n",
      "Data loss: 0.00037168426206335425, Function loss: 0.000886753958184272\n",
      "Step: 3167, Loss: 0.0012584382202476263\n",
      "Data loss: 0.00041286260238848627, Function loss: 0.0007874579750932753\n",
      "Step: 3168, Loss: 0.0012003205483779311\n",
      "Data loss: 0.00037182445521466434, Function loss: 0.0007747153867967427\n",
      "Step: 3169, Loss: 0.0011465398129075766\n",
      "Data loss: 0.0004039680352434516, Function loss: 0.0006944510969333351\n",
      "Step: 3170, Loss: 0.0010984190739691257\n",
      "Data loss: 0.0003733917837962508, Function loss: 0.0006802143761888146\n",
      "Step: 3171, Loss: 0.0010536061599850655\n",
      "Data loss: 0.0003930334351025522, Function loss: 0.0006258604116737843\n",
      "Step: 3172, Loss: 0.0010188939049839973\n",
      "Data loss: 0.0003764552529901266, Function loss: 0.0006196351605467498\n",
      "Step: 3173, Loss: 0.0009960904717445374\n",
      "Data loss: 0.0003835777461063117, Function loss: 0.0006023100577294827\n",
      "Step: 3174, Loss: 0.0009858878329396248\n",
      "Data loss: 0.0003797878453042358, Function loss: 0.0006043575122021139\n",
      "Step: 3175, Loss: 0.0009841453284025192\n",
      "Data loss: 0.0003766476293094456, Function loss: 0.0006134799914434552\n",
      "Step: 3176, Loss: 0.0009901276789605618\n",
      "Data loss: 0.00038247581687755883, Function loss: 0.0006177537725307047\n",
      "Step: 3177, Loss: 0.001000229618512094\n",
      "Data loss: 0.00037112346035428345, Function loss: 0.0006404747837223113\n",
      "Step: 3178, Loss: 0.0010115982731804252\n",
      "Data loss: 0.0003845824103336781, Function loss: 0.0006397286197170615\n",
      "Step: 3179, Loss: 0.00102431105915457\n",
      "Data loss: 0.00036664100480265915, Function loss: 0.0006714330520480871\n",
      "Step: 3180, Loss: 0.0010380740277469158\n",
      "Data loss: 0.0003871421213261783, Function loss: 0.0006703969556838274\n",
      "Step: 3181, Loss: 0.0010575391352176666\n",
      "Data loss: 0.00036185653880238533, Function loss: 0.0007171340985223651\n",
      "Step: 3182, Loss: 0.0010789906373247504\n",
      "Data loss: 0.00039090614882297814, Function loss: 0.0007121644448488951\n",
      "Step: 3183, Loss: 0.0011030705645680428\n",
      "Data loss: 0.00035769122769124806, Function loss: 0.0007646969170309603\n",
      "Step: 3184, Loss: 0.0011223881738260388\n",
      "Data loss: 0.0003945020434912294, Function loss: 0.0007455804734490812\n",
      "Step: 3185, Loss: 0.0011400824878364801\n",
      "Data loss: 0.00035554554779082537, Function loss: 0.0007954472093842924\n",
      "Step: 3186, Loss: 0.0011509926989674568\n",
      "Data loss: 0.0003975859144702554, Function loss: 0.0007644108263775706\n",
      "Step: 3187, Loss: 0.001161996740847826\n",
      "Data loss: 0.0003561088815331459, Function loss: 0.0008204305777326226\n",
      "Step: 3188, Loss: 0.0011765394592657685\n",
      "Data loss: 0.0004026948008686304, Function loss: 0.0007982560782693326\n",
      "Step: 3189, Loss: 0.001200950937345624\n",
      "Data loss: 0.0003605758538469672, Function loss: 0.0008813391323201358\n",
      "Step: 3190, Loss: 0.0012419149279594421\n",
      "Data loss: 0.0004143786209169775, Function loss: 0.0008977505494840443\n",
      "Step: 3191, Loss: 0.0013121291995048523\n",
      "Data loss: 0.0003727937873918563, Function loss: 0.0010532535379752517\n",
      "Step: 3192, Loss: 0.0014260472962632775\n",
      "Data loss: 0.0004404424980748445, Function loss: 0.0011609727516770363\n",
      "Step: 3193, Loss: 0.0016014152206480503\n",
      "Data loss: 0.0003999799082521349, Function loss: 0.0014621897134929895\n",
      "Step: 3194, Loss: 0.001862169592641294\n",
      "Data loss: 0.0004923539818264544, Function loss: 0.0017230865778401494\n",
      "Step: 3195, Loss: 0.0022154406178742647\n",
      "Data loss: 0.0004503880045376718, Function loss: 0.002202264266088605\n",
      "Step: 3196, Loss: 0.0026526523288339376\n",
      "Data loss: 0.0005702407215721905, Function loss: 0.0026015022303909063\n",
      "Step: 3197, Loss: 0.003171742893755436\n",
      "Data loss: 0.0005239934544079006, Function loss: 0.0032426961697638035\n",
      "Step: 3198, Loss: 0.003766689682379365\n",
      "Data loss: 0.000665432249661535, Function loss: 0.003697269130498171\n",
      "Step: 3199, Loss: 0.004362701438367367\n",
      "Data loss: 0.000600242055952549, Function loss: 0.004265511408448219\n",
      "Step: 3200, Loss: 0.004865753464400768\n",
      "Data loss: 0.0007183250854723155, Function loss: 0.004393254406750202\n",
      "Step: 3201, Loss: 0.005111579317599535\n",
      "Data loss: 0.0006149199907667935, Function loss: 0.004380302038043737\n",
      "Step: 3202, Loss: 0.004995222203433514\n",
      "Data loss: 0.0006477466668002307, Function loss: 0.0037763265427201986\n",
      "Step: 3203, Loss: 0.00442407326772809\n",
      "Data loss: 0.0005207843496464193, Function loss: 0.0030234993901103735\n",
      "Step: 3204, Loss: 0.0035442837979644537\n",
      "Data loss: 0.0004687995824497193, Function loss: 0.0021403662394732237\n",
      "Step: 3205, Loss: 0.0026091658510267735\n",
      "Data loss: 0.0004199675458949059, Function loss: 0.001600625109858811\n",
      "Step: 3206, Loss: 0.0020205925684422255\n",
      "Data loss: 0.0003578018513508141, Function loss: 0.0016787981148809195\n",
      "Step: 3207, Loss: 0.0020365999080240726\n",
      "Data loss: 0.00046449556248262525, Function loss: 0.0022337031550705433\n",
      "Step: 3208, Loss: 0.0026981986593455076\n",
      "Data loss: 0.00040231531602330506, Function loss: 0.0031997577752918005\n",
      "Step: 3209, Loss: 0.003602073062211275\n",
      "Data loss: 0.0005714143626391888, Function loss: 0.003871203400194645\n",
      "Step: 3210, Loss: 0.004442617762833834\n",
      "Data loss: 0.00044059730134904385, Function loss: 0.004295075312256813\n",
      "Step: 3211, Loss: 0.0047356728464365005\n",
      "Data loss: 0.0005615041591227055, Function loss: 0.004087078385055065\n",
      "Step: 3212, Loss: 0.004648582544177771\n",
      "Data loss: 0.00039482643478550017, Function loss: 0.0037373199593275785\n",
      "Step: 3213, Loss: 0.00413214648142457\n",
      "Data loss: 0.0004859401669818908, Function loss: 0.003271595574915409\n",
      "Step: 3214, Loss: 0.0037575357127934694\n",
      "Data loss: 0.00038981455145403743, Function loss: 0.003082485403865576\n",
      "Step: 3215, Loss: 0.003472300013527274\n",
      "Data loss: 0.0004849704564549029, Function loss: 0.0030532542150467634\n",
      "Step: 3216, Loss: 0.0035382246132940054\n",
      "Data loss: 0.00045100130955688655, Function loss: 0.0030397383961826563\n",
      "Step: 3217, Loss: 0.0034907397348433733\n",
      "Data loss: 0.0005013248301111162, Function loss: 0.0028024218045175076\n",
      "Step: 3218, Loss: 0.0033037466928362846\n",
      "Data loss: 0.0004500769719015807, Function loss: 0.002285399241372943\n",
      "Step: 3219, Loss: 0.002735476242378354\n",
      "Data loss: 0.00044544064439833164, Function loss: 0.001591797568835318\n",
      "Step: 3220, Loss: 0.0020372383296489716\n",
      "Data loss: 0.0003905160119757056, Function loss: 0.0010267008328810334\n",
      "Step: 3221, Loss: 0.001417216844856739\n",
      "Data loss: 0.0004033846198581159, Function loss: 0.0007085675606504083\n",
      "Step: 3222, Loss: 0.0011119521223008633\n",
      "Data loss: 0.0003880714066326618, Function loss: 0.0007115544285625219\n",
      "Step: 3223, Loss: 0.0010996258351951838\n",
      "Data loss: 0.00041612485074438155, Function loss: 0.0008292007260024548\n",
      "Step: 3224, Loss: 0.0012453255476430058\n",
      "Data loss: 0.0004172888584434986, Function loss: 0.000985196908004582\n",
      "Step: 3225, Loss: 0.0014024857664480805\n",
      "Data loss: 0.0004113302566111088, Function loss: 0.0010896435705944896\n",
      "Step: 3226, Loss: 0.0015009738272055984\n",
      "Data loss: 0.000425052538048476, Function loss: 0.0011650989763438702\n",
      "Step: 3227, Loss: 0.001590151572600007\n",
      "Data loss: 0.0003840389254037291, Function loss: 0.0013231071643531322\n",
      "Step: 3228, Loss: 0.0017071460606530309\n",
      "Data loss: 0.0004349999944679439, Function loss: 0.0014284092467278242\n",
      "Step: 3229, Loss: 0.001863409299403429\n",
      "Data loss: 0.00037274722126312554, Function loss: 0.0015662292717024684\n",
      "Step: 3230, Loss: 0.0019389764638617635\n",
      "Data loss: 0.000442198826931417, Function loss: 0.0014922000700607896\n",
      "Step: 3231, Loss: 0.0019343988969922066\n",
      "Data loss: 0.00037080683978274465, Function loss: 0.0014071110635995865\n",
      "Step: 3232, Loss: 0.0017779178451746702\n",
      "Data loss: 0.00042592830141074955, Function loss: 0.0011249292874708772\n",
      "Step: 3233, Loss: 0.0015508575597777963\n",
      "Data loss: 0.000371071306290105, Function loss: 0.0009312361944466829\n",
      "Step: 3234, Loss: 0.0013023074716329575\n",
      "Data loss: 0.00039890603511594236, Function loss: 0.0007230554474517703\n",
      "Step: 3235, Loss: 0.0011219615116715431\n",
      "Data loss: 0.00038461212534457445, Function loss: 0.0006522256298922002\n",
      "Step: 3236, Loss: 0.0010368376970291138\n",
      "Data loss: 0.0003822351573035121, Function loss: 0.0006512633408419788\n",
      "Step: 3237, Loss: 0.00103349843993783\n",
      "Data loss: 0.00040007790084928274, Function loss: 0.0006587226525880396\n",
      "Step: 3238, Loss: 0.0010588006116449833\n",
      "Data loss: 0.000372273352695629, Function loss: 0.0007034632726572454\n",
      "Step: 3239, Loss: 0.001075736596249044\n",
      "Data loss: 0.0003992318524979055, Function loss: 0.000672888127155602\n",
      "Step: 3240, Loss: 0.0010721199214458466\n",
      "Data loss: 0.0003673373139463365, Function loss: 0.0006913879187777638\n",
      "Step: 3241, Loss: 0.0010587251745164394\n",
      "Data loss: 0.00039003725396469235, Function loss: 0.0006687634740956128\n",
      "Step: 3242, Loss: 0.0010588007280603051\n",
      "Data loss: 0.00037525754305534065, Function loss: 0.0007057582261040807\n",
      "Step: 3243, Loss: 0.0010810157982632518\n",
      "Data loss: 0.0003860449942294508, Function loss: 0.0007337235729210079\n",
      "Step: 3244, Loss: 0.0011197685962542892\n",
      "Data loss: 0.0003873464011121541, Function loss: 0.0007628640742041171\n",
      "Step: 3245, Loss: 0.0011502105044201016\n",
      "Data loss: 0.00038417216273956, Function loss: 0.0007722734590061009\n",
      "Step: 3246, Loss: 0.0011564455926418304\n",
      "Data loss: 0.00038594077341258526, Function loss: 0.0007453258731402457\n",
      "Step: 3247, Loss: 0.00113126658834517\n",
      "Data loss: 0.0003799468104261905, Function loss: 0.0007099562790244818\n",
      "Step: 3248, Loss: 0.0010899030603468418\n",
      "Data loss: 0.0003743783454410732, Function loss: 0.0006634729215875268\n",
      "Step: 3249, Loss: 0.001037851208820939\n",
      "Data loss: 0.0003766552254091948, Function loss: 0.0006158306496217847\n",
      "Step: 3250, Loss: 0.00099248590413481\n",
      "Data loss: 0.00036395093775354326, Function loss: 0.0006041940650902689\n",
      "Step: 3251, Loss: 0.0009681449737399817\n",
      "Data loss: 0.00037940137553960085, Function loss: 0.0005929932231083512\n",
      "Step: 3252, Loss: 0.0009723945986479521\n",
      "Data loss: 0.00036236672895029187, Function loss: 0.0006282611284404993\n",
      "Step: 3253, Loss: 0.0009906277991831303\n",
      "Data loss: 0.0003821861173491925, Function loss: 0.0006228367565199733\n",
      "Step: 3254, Loss: 0.0010050229029729962\n",
      "Data loss: 0.00036376542993821204, Function loss: 0.0006401940481737256\n",
      "Step: 3255, Loss: 0.0010039594490081072\n",
      "Data loss: 0.000377375865355134, Function loss: 0.0006105611682869494\n",
      "Step: 3256, Loss: 0.0009879369754344225\n",
      "Data loss: 0.0003637310001067817, Function loss: 0.0006049725925549865\n",
      "Step: 3257, Loss: 0.0009687035926617682\n",
      "Data loss: 0.0003686524578370154, Function loss: 0.0005834696348756552\n",
      "Step: 3258, Loss: 0.0009521220927126706\n",
      "Data loss: 0.00036593229742720723, Function loss: 0.0005799000500701368\n",
      "Step: 3259, Loss: 0.000945832347497344\n",
      "Data loss: 0.0003604321100283414, Function loss: 0.0005897545488551259\n",
      "Step: 3260, Loss: 0.0009501866297796369\n",
      "Data loss: 0.0003715595812536776, Function loss: 0.0005998494452796876\n",
      "Step: 3261, Loss: 0.0009714090265333652\n",
      "Data loss: 0.0003535105206537992, Function loss: 0.000654066214337945\n",
      "Step: 3262, Loss: 0.0010075767058879137\n",
      "Data loss: 0.0003792300121858716, Function loss: 0.0006873314850963652\n",
      "Step: 3263, Loss: 0.0010665615554898977\n",
      "Data loss: 0.00034795678220689297, Function loss: 0.0008143764571286738\n",
      "Step: 3264, Loss: 0.0011623331811279058\n",
      "Data loss: 0.00039200985338538885, Function loss: 0.0009266558918170631\n",
      "Step: 3265, Loss: 0.001318665686994791\n",
      "Data loss: 0.00034436269197613, Function loss: 0.0012071565724909306\n",
      "Step: 3266, Loss: 0.0015515192644670606\n",
      "Data loss: 0.00041472582961432636, Function loss: 0.0015251411823555827\n",
      "Step: 3267, Loss: 0.0019398670410737395\n",
      "Data loss: 0.0003488366783130914, Function loss: 0.0021885037422180176\n",
      "Step: 3268, Loss: 0.0025373403914272785\n",
      "Data loss: 0.0004625926667358726, Function loss: 0.003084741532802582\n",
      "Step: 3269, Loss: 0.003547334112226963\n",
      "Data loss: 0.0003766012960113585, Function loss: 0.004567474126815796\n",
      "Step: 3270, Loss: 0.0049440753646194935\n",
      "Data loss: 0.0005655291606672108, Function loss: 0.0068428656086325645\n",
      "Step: 3271, Loss: 0.007408394943922758\n",
      "Data loss: 0.0004609217867255211, Function loss: 0.009908545762300491\n",
      "Step: 3272, Loss: 0.010369467549026012\n",
      "Data loss: 0.0007658856338821352, Function loss: 0.014592775143682957\n",
      "Step: 3273, Loss: 0.015358660370111465\n",
      "Data loss: 0.0006235705805011094, Function loss: 0.018677428364753723\n",
      "Step: 3274, Loss: 0.019300999119877815\n",
      "Data loss: 0.001003833720460534, Function loss: 0.0238635390996933\n",
      "Step: 3275, Loss: 0.02486737258732319\n",
      "Data loss: 0.0007189484895206988, Function loss: 0.02222299762070179\n",
      "Step: 3276, Loss: 0.022941946983337402\n",
      "Data loss: 0.0008844382828101516, Function loss: 0.017907531931996346\n",
      "Step: 3277, Loss: 0.01879196986556053\n",
      "Data loss: 0.0005111693171784282, Function loss: 0.008054896257817745\n",
      "Step: 3278, Loss: 0.008566065691411495\n",
      "Data loss: 0.0004794325213879347, Function loss: 0.001398937776684761\n",
      "Step: 3279, Loss: 0.0018783702980726957\n",
      "Data loss: 0.0004917028709314764, Function loss: 0.0014871557941660285\n",
      "Step: 3280, Loss: 0.001978858606889844\n",
      "Data loss: 0.0005226955399848521, Function loss: 0.006187696009874344\n",
      "Step: 3281, Loss: 0.006710391491651535\n",
      "Data loss: 0.0007589420420117676, Function loss: 0.009450400248169899\n",
      "Step: 3282, Loss: 0.01020934246480465\n",
      "Data loss: 0.0005605023470707238, Function loss: 0.00663919048383832\n",
      "Step: 3283, Loss: 0.007199693005532026\n",
      "Data loss: 0.0005872512701898813, Function loss: 0.002010138239711523\n",
      "Step: 3284, Loss: 0.0025973895099014044\n",
      "Data loss: 0.0005370480939745903, Function loss: 0.0007533988100476563\n",
      "Step: 3285, Loss: 0.0012904468458145857\n",
      "Data loss: 0.0005452160257846117, Function loss: 0.0033528166823089123\n",
      "Step: 3286, Loss: 0.003898032708093524\n",
      "Data loss: 0.0007040207274258137, Function loss: 0.005260910838842392\n",
      "Step: 3287, Loss: 0.005964931566268206\n",
      "Data loss: 0.0005576280527748168, Function loss: 0.0035368448588997126\n",
      "Step: 3288, Loss: 0.004094473086297512\n",
      "Data loss: 0.0005821168306283653, Function loss: 0.00097482098499313\n",
      "Step: 3289, Loss: 0.0015569378156214952\n",
      "Data loss: 0.0005757081671617925, Function loss: 0.0008877370855771005\n",
      "Step: 3290, Loss: 0.001463445252738893\n",
      "Data loss: 0.0005628940998576581, Function loss: 0.0026571706403046846\n",
      "Step: 3291, Loss: 0.0032200647983700037\n",
      "Data loss: 0.0006610345444642007, Function loss: 0.0032100165262818336\n",
      "Step: 3292, Loss: 0.0038710511289536953\n",
      "Data loss: 0.0005594942485913634, Function loss: 0.001736797858029604\n",
      "Step: 3293, Loss: 0.002296292223036289\n",
      "Data loss: 0.0005619488074444234, Function loss: 0.0005700997426174581\n",
      "Step: 3294, Loss: 0.0011320485500618815\n",
      "Data loss: 0.000587686721701175, Function loss: 0.001113146310672164\n",
      "Step: 3295, Loss: 0.001700832974165678\n",
      "Data loss: 0.000543723872397095, Function loss: 0.0021277389023452997\n",
      "Step: 3296, Loss: 0.0026714627165347338\n",
      "Data loss: 0.0006047702627256513, Function loss: 0.0019355054246261716\n",
      "Step: 3297, Loss: 0.002540275687351823\n",
      "Data loss: 0.0005260622128844261, Function loss: 0.0009570099646225572\n",
      "Step: 3298, Loss: 0.0014830721775069833\n",
      "Data loss: 0.0005236472352407873, Function loss: 0.0005522947758436203\n",
      "Step: 3299, Loss: 0.0010759420692920685\n",
      "Data loss: 0.0005611631786450744, Function loss: 0.0010910799028351903\n",
      "Step: 3300, Loss: 0.0016522430814802647\n",
      "Data loss: 0.0004985246923752129, Function loss: 0.0015935706906020641\n",
      "Step: 3301, Loss: 0.002092095324769616\n",
      "Data loss: 0.0005507211899384856, Function loss: 0.0011949825566262007\n",
      "Step: 3302, Loss: 0.0017457037465646863\n",
      "Data loss: 0.0004871686687693, Function loss: 0.0006408959161490202\n",
      "Step: 3303, Loss: 0.0011280645849183202\n",
      "Data loss: 0.00047915236791595817, Function loss: 0.0006235254695639014\n",
      "Step: 3304, Loss: 0.0011026777792721987\n",
      "Data loss: 0.0005224375054240227, Function loss: 0.000999148003757\n",
      "Step: 3305, Loss: 0.0015215855091810226\n",
      "Data loss: 0.0004550692974589765, Function loss: 0.0011805719695985317\n",
      "Step: 3306, Loss: 0.0016356413252651691\n",
      "Data loss: 0.000502290204167366, Function loss: 0.000816116516944021\n",
      "Step: 3307, Loss: 0.001318406779319048\n",
      "Data loss: 0.00046101605403237045, Function loss: 0.0005539335543289781\n",
      "Step: 3308, Loss: 0.001014949637465179\n",
      "Data loss: 0.0004475208988878876, Function loss: 0.0006398343248292804\n",
      "Step: 3309, Loss: 0.0010873551946133375\n",
      "Data loss: 0.0004908429109491408, Function loss: 0.0008532523061148822\n",
      "Step: 3310, Loss: 0.001344095217064023\n",
      "Data loss: 0.00042718887561932206, Function loss: 0.0009596282034181058\n",
      "Step: 3311, Loss: 0.001386817079037428\n",
      "Data loss: 0.0004718692507594824, Function loss: 0.000714419293217361\n",
      "Step: 3312, Loss: 0.0011862885439768434\n",
      "Data loss: 0.00043345295125618577, Function loss: 0.0005604096222668886\n",
      "Step: 3313, Loss: 0.0009938625153154135\n",
      "Data loss: 0.00042878196109086275, Function loss: 0.0005786368274129927\n",
      "Step: 3314, Loss: 0.0010074188467115164\n",
      "Data loss: 0.00045781349763274193, Function loss: 0.0007035596063360572\n",
      "Step: 3315, Loss: 0.0011613731039687991\n",
      "Data loss: 0.0004088912974111736, Function loss: 0.0008235196582973003\n",
      "Step: 3316, Loss: 0.0012324110139161348\n",
      "Data loss: 0.0004501553485170007, Function loss: 0.0006872581434436142\n",
      "Step: 3317, Loss: 0.0011374135501682758\n",
      "Data loss: 0.00041322552715428174, Function loss: 0.0005759631749242544\n",
      "Step: 3318, Loss: 0.0009891886729747057\n",
      "Data loss: 0.000414047681260854, Function loss: 0.0005451367469504476\n",
      "Step: 3319, Loss: 0.0009591844282113016\n",
      "Data loss: 0.0004329007351770997, Function loss: 0.0006112103001214564\n",
      "Step: 3320, Loss: 0.001044111093506217\n",
      "Data loss: 0.0003954171552322805, Function loss: 0.0007259823032654822\n",
      "Step: 3321, Loss: 0.0011213994584977627\n",
      "Data loss: 0.00043097572051919997, Function loss: 0.0006736750947311521\n",
      "Step: 3322, Loss: 0.0011046507861465216\n",
      "Data loss: 0.0003979253233410418, Function loss: 0.0006206964608281851\n",
      "Step: 3323, Loss: 0.001018621725961566\n",
      "Data loss: 0.0004051604773849249, Function loss: 0.0005478638340719044\n",
      "Step: 3324, Loss: 0.0009530243114568293\n",
      "Data loss: 0.00041157123632729053, Function loss: 0.0005432249163277447\n",
      "Step: 3325, Loss: 0.0009547961526550353\n",
      "Data loss: 0.0003855256945826113, Function loss: 0.0006147901294752955\n",
      "Step: 3326, Loss: 0.0010003158822655678\n",
      "Data loss: 0.00041460368083789945, Function loss: 0.0006118214223533869\n",
      "Step: 3327, Loss: 0.0010264250449836254\n",
      "Data loss: 0.00038383781793527305, Function loss: 0.0006145021179690957\n",
      "Step: 3328, Loss: 0.0009983399650081992\n",
      "Data loss: 0.0003973297425545752, Function loss: 0.000553638325072825\n",
      "Step: 3329, Loss: 0.0009509680676274002\n",
      "Data loss: 0.0003946904616896063, Function loss: 0.0005368781858123839\n",
      "Step: 3330, Loss: 0.0009315686766058207\n",
      "Data loss: 0.0003782210114877671, Function loss: 0.0005776258185505867\n",
      "Step: 3331, Loss: 0.0009558468591421843\n",
      "Data loss: 0.0004044748784508556, Function loss: 0.0005883673438802361\n",
      "Step: 3332, Loss: 0.0009928422514349222\n",
      "Data loss: 0.00037035220884718, Function loss: 0.0006299655651673675\n",
      "Step: 3333, Loss: 0.001000317744910717\n",
      "Data loss: 0.0003977493033744395, Function loss: 0.0005761088686995208\n",
      "Step: 3334, Loss: 0.0009738581720739603\n",
      "Data loss: 0.00037314731162041426, Function loss: 0.0005631427047774196\n",
      "Step: 3335, Loss: 0.0009362900163978338\n",
      "Data loss: 0.00038282317109405994, Function loss: 0.0005337978363968432\n",
      "Step: 3336, Loss: 0.0009166210074909031\n",
      "Data loss: 0.0003794303920585662, Function loss: 0.0005376901244744658\n",
      "Step: 3337, Loss: 0.0009171204874292016\n",
      "Data loss: 0.00037070352118462324, Function loss: 0.0005597050185315311\n",
      "Step: 3338, Loss: 0.0009304085397161543\n",
      "Data loss: 0.0003842707665171474, Function loss: 0.0005593420355580747\n",
      "Step: 3339, Loss: 0.0009436127729713917\n",
      "Data loss: 0.0003646060358732939, Function loss: 0.0005811585579067469\n",
      "Step: 3340, Loss: 0.0009457645937800407\n",
      "Data loss: 0.0003821360587608069, Function loss: 0.0005538320983760059\n",
      "Step: 3341, Loss: 0.0009359681280329823\n",
      "Data loss: 0.0003644832759164274, Function loss: 0.0005583896418102086\n",
      "Step: 3342, Loss: 0.0009228729177266359\n",
      "Data loss: 0.0003748170274775475, Function loss: 0.0005396117339842021\n",
      "Step: 3343, Loss: 0.0009144287323579192\n",
      "Data loss: 0.00036749037099070847, Function loss: 0.0005448202136904001\n",
      "Step: 3344, Loss: 0.0009123105555772781\n",
      "Data loss: 0.00036725413519889116, Function loss: 0.0005493152420967817\n",
      "Step: 3345, Loss: 0.0009165693772956729\n",
      "Data loss: 0.0003692292666528374, Function loss: 0.0005487428279593587\n",
      "Step: 3346, Loss: 0.0009179720655083656\n",
      "Data loss: 0.0003622195799835026, Function loss: 0.0005554079543799162\n",
      "Step: 3347, Loss: 0.0009176275343634188\n",
      "Data loss: 0.00036847867886535823, Function loss: 0.0005462882691062987\n",
      "Step: 3348, Loss: 0.0009147669188678265\n",
      "Data loss: 0.0003603085642680526, Function loss: 0.0005478328675962985\n",
      "Step: 3349, Loss: 0.000908141431864351\n",
      "Data loss: 0.0003648344718385488, Function loss: 0.000536851875949651\n",
      "Step: 3350, Loss: 0.0009016863768920302\n",
      "Data loss: 0.0003610670391935855, Function loss: 0.0005364730604924262\n",
      "Step: 3351, Loss: 0.0008975401287898421\n",
      "Data loss: 0.00035981895052827895, Function loss: 0.0005372668383643031\n",
      "Step: 3352, Loss: 0.0008970857597887516\n",
      "Data loss: 0.00036279676714912057, Function loss: 0.0005354894674383104\n",
      "Step: 3353, Loss: 0.000898286234587431\n",
      "Data loss: 0.00035587570164352655, Function loss: 0.0005445603164844215\n",
      "Step: 3354, Loss: 0.000900436018127948\n",
      "Data loss: 0.00036297435872256756, Function loss: 0.0005387468263506889\n",
      "Step: 3355, Loss: 0.0009017211850732565\n",
      "Data loss: 0.0003539399185683578, Function loss: 0.0005462684785015881\n",
      "Step: 3356, Loss: 0.0009002083679661155\n",
      "Data loss: 0.00036061290302313864, Function loss: 0.000535851635504514\n",
      "Step: 3357, Loss: 0.0008964645676314831\n",
      "Data loss: 0.0003539304598234594, Function loss: 0.0005400180816650391\n",
      "Step: 3358, Loss: 0.0008939485414884984\n",
      "Data loss: 0.0003575805458240211, Function loss: 0.0005348083795979619\n",
      "Step: 3359, Loss: 0.000892388925421983\n",
      "Data loss: 0.0003543033089954406, Function loss: 0.000537591811735183\n",
      "Step: 3360, Loss: 0.0008918951498344541\n",
      "Data loss: 0.00035528308944776654, Function loss: 0.0005378835485316813\n",
      "Step: 3361, Loss: 0.0008931666379794478\n",
      "Data loss: 0.00035421771463006735, Function loss: 0.0005401377566158772\n",
      "Step: 3362, Loss: 0.0008943554712459445\n",
      "Data loss: 0.00035380240296944976, Function loss: 0.0005424990085884929\n",
      "Step: 3363, Loss: 0.0008963014115579426\n",
      "Data loss: 0.00035364317591302097, Function loss: 0.0005451910546980798\n",
      "Step: 3364, Loss: 0.0008988342015072703\n",
      "Data loss: 0.00035305769415572286, Function loss: 0.0005450524040497839\n",
      "Step: 3365, Loss: 0.0008981100982055068\n",
      "Data loss: 0.0003521491016726941, Function loss: 0.0005434644990600646\n",
      "Step: 3366, Loss: 0.0008956135716289282\n",
      "Data loss: 0.00035248915082775056, Function loss: 0.0005378875648602843\n",
      "Step: 3367, Loss: 0.0008903767447918653\n",
      "Data loss: 0.000350155372871086, Function loss: 0.0005363328964449465\n",
      "Step: 3368, Loss: 0.0008864882402122021\n",
      "Data loss: 0.00035189324989914894, Function loss: 0.0005334328161552548\n",
      "Step: 3369, Loss: 0.0008853260660544038\n",
      "Data loss: 0.0003489265509415418, Function loss: 0.0005365580436773598\n",
      "Step: 3370, Loss: 0.0008854846237227321\n",
      "Data loss: 0.0003514448180794716, Function loss: 0.00053656060481444\n",
      "Step: 3371, Loss: 0.0008880054228939116\n",
      "Data loss: 0.00034818792482838035, Function loss: 0.0005431818426586688\n",
      "Step: 3372, Loss: 0.0008913697674870491\n",
      "Data loss: 0.0003511510731186718, Function loss: 0.0005432183970697224\n",
      "Step: 3373, Loss: 0.0008943694410845637\n",
      "Data loss: 0.00034781909198500216, Function loss: 0.0005492257769219577\n",
      "Step: 3374, Loss: 0.0008970448980107903\n",
      "Data loss: 0.00035072388709522784, Function loss: 0.0005489747854880989\n",
      "Step: 3375, Loss: 0.0008996987016871572\n",
      "Data loss: 0.0003476175479590893, Function loss: 0.0005539985722862184\n",
      "Step: 3376, Loss: 0.0009016161202453077\n",
      "Data loss: 0.00034996867179870605, Function loss: 0.0005518679972738028\n",
      "Step: 3377, Loss: 0.0009018366690725088\n",
      "Data loss: 0.00034771577338688076, Function loss: 0.0005529252230189741\n",
      "Step: 3378, Loss: 0.0009006409673020244\n",
      "Data loss: 0.00034834997495636344, Function loss: 0.000548768148291856\n",
      "Step: 3379, Loss: 0.0008971181232482195\n",
      "Data loss: 0.00034874462289735675, Function loss: 0.0005470199976116419\n",
      "Step: 3380, Loss: 0.0008957646205089986\n",
      "Data loss: 0.0003462208842393011, Function loss: 0.0005473007913678885\n",
      "Step: 3381, Loss: 0.00089352170471102\n",
      "Data loss: 0.00035029163700528443, Function loss: 0.0005438965163193643\n",
      "Step: 3382, Loss: 0.0008941881824284792\n",
      "Data loss: 0.0003444509638939053, Function loss: 0.000551296048797667\n",
      "Step: 3383, Loss: 0.0008957469835877419\n",
      "Data loss: 0.000351537368260324, Function loss: 0.0005474269273690879\n",
      "Step: 3384, Loss: 0.0008989642956294119\n",
      "Data loss: 0.00034319559927098453, Function loss: 0.0005582627491094172\n",
      "Step: 3385, Loss: 0.0009014583192765713\n",
      "Data loss: 0.00035285064950585365, Function loss: 0.000550659082364291\n",
      "Step: 3386, Loss: 0.0009035097318701446\n",
      "Data loss: 0.00034189337748102844, Function loss: 0.0005629321676678956\n",
      "Step: 3387, Loss: 0.0009048255160450935\n",
      "Data loss: 0.0003534522547852248, Function loss: 0.0005525817396119237\n",
      "Step: 3388, Loss: 0.000906034023500979\n",
      "Data loss: 0.0003404599556233734, Function loss: 0.0005650919047184289\n",
      "Step: 3389, Loss: 0.0009055518312379718\n",
      "Data loss: 0.00035347792436368763, Function loss: 0.0005545238964259624\n",
      "Step: 3390, Loss: 0.0009080017916858196\n",
      "Data loss: 0.00033900761627592146, Function loss: 0.0005715057486668229\n",
      "Step: 3391, Loss: 0.0009105133358389139\n",
      "Data loss: 0.0003538231540005654, Function loss: 0.0005599030409939587\n",
      "Step: 3392, Loss: 0.0009137261658906937\n",
      "Data loss: 0.00033767198328860104, Function loss: 0.0005845619598403573\n",
      "Step: 3393, Loss: 0.0009222339140251279\n",
      "Data loss: 0.0003554727300070226, Function loss: 0.0005826179403811693\n",
      "Step: 3394, Loss: 0.0009380906703881919\n",
      "Data loss: 0.0003359141992405057, Function loss: 0.0006250289734452963\n",
      "Step: 3395, Loss: 0.000960943172685802\n",
      "Data loss: 0.0003585521480999887, Function loss: 0.0006322708213701844\n",
      "Step: 3396, Loss: 0.0009908229112625122\n",
      "Data loss: 0.0003343809221405536, Function loss: 0.0006944101769477129\n",
      "Step: 3397, Loss: 0.001028791069984436\n",
      "Data loss: 0.00036268067196942866, Function loss: 0.0007140145753510296\n",
      "Step: 3398, Loss: 0.0010766952764242887\n",
      "Data loss: 0.0003331581247039139, Function loss: 0.0008025785209611058\n",
      "Step: 3399, Loss: 0.0011357367038726807\n",
      "Data loss: 0.00036776447086595, Function loss: 0.0008372996817342937\n",
      "Step: 3400, Loss: 0.0012050641234964132\n",
      "Data loss: 0.00033246708335354924, Function loss: 0.0009512300021015108\n",
      "Step: 3401, Loss: 0.00128369708545506\n",
      "Data loss: 0.00037443466135300696, Function loss: 0.0010166115825995803\n",
      "Step: 3402, Loss: 0.0013910462148487568\n",
      "Data loss: 0.0003331334737595171, Function loss: 0.0011874259216710925\n",
      "Step: 3403, Loss: 0.00152055942453444\n",
      "Data loss: 0.00038480458897538483, Function loss: 0.0013200185494497418\n",
      "Step: 3404, Loss: 0.0017048231093212962\n",
      "Data loss: 0.00033548095962032676, Function loss: 0.0015623098006471992\n",
      "Step: 3405, Loss: 0.001897790702059865\n",
      "Data loss: 0.00039813618059270084, Function loss: 0.0017462876858189702\n",
      "Step: 3406, Loss: 0.0021444237791001797\n",
      "Data loss: 0.00033981146407313645, Function loss: 0.00202255230396986\n",
      "Step: 3407, Loss: 0.002362363738939166\n",
      "Data loss: 0.0004129060835111886, Function loss: 0.0022421572357416153\n",
      "Step: 3408, Loss: 0.0026550632901489735\n",
      "Data loss: 0.00034615007461979985, Function loss: 0.002519267378374934\n",
      "Step: 3409, Loss: 0.002865417394787073\n",
      "Data loss: 0.0004263190203346312, Function loss: 0.0027151231188327074\n",
      "Step: 3410, Loss: 0.0031414420809596777\n",
      "Data loss: 0.000352635484887287, Function loss: 0.002912066411226988\n",
      "Step: 3411, Loss: 0.0032647019252181053\n",
      "Data loss: 0.000434636342106387, Function loss: 0.0029971383046358824\n",
      "Step: 3412, Loss: 0.0034317746758461\n",
      "Data loss: 0.000357372424332425, Function loss: 0.003008271800354123\n",
      "Step: 3413, Loss: 0.0033656442537903786\n",
      "Data loss: 0.0004328763170633465, Function loss: 0.002890196396037936\n",
      "Step: 3414, Loss: 0.003323072800412774\n",
      "Data loss: 0.00036159917362965643, Function loss: 0.0027051912620663643\n",
      "Step: 3415, Loss: 0.003066790523007512\n",
      "Data loss: 0.0004225518205203116, Function loss: 0.00240921089425683\n",
      "Step: 3416, Loss: 0.002831762656569481\n",
      "Data loss: 0.00036639446625486016, Function loss: 0.0020816167816519737\n",
      "Step: 3417, Loss: 0.002448011189699173\n",
      "Data loss: 0.0004077082558069378, Function loss: 0.0017101047560572624\n",
      "Step: 3418, Loss: 0.0021178130991756916\n",
      "Data loss: 0.0003788959002122283, Function loss: 0.0014273927081376314\n",
      "Step: 3419, Loss: 0.0018062886083498597\n",
      "Data loss: 0.00039889608160592616, Function loss: 0.0012194636510685086\n",
      "Step: 3420, Loss: 0.0016183597035706043\n",
      "Data loss: 0.00040416725096292794, Function loss: 0.0011566209141165018\n",
      "Step: 3421, Loss: 0.0015607881359755993\n",
      "Data loss: 0.0004030625568702817, Function loss: 0.001247852691449225\n",
      "Step: 3422, Loss: 0.0016509152483195066\n",
      "Data loss: 0.00044406772940419614, Function loss: 0.001422425266355276\n",
      "Step: 3423, Loss: 0.0018664930248633027\n",
      "Data loss: 0.00042189480154775083, Function loss: 0.0017404056852683425\n",
      "Step: 3424, Loss: 0.0021623005159199238\n",
      "Data loss: 0.0004903283552266657, Function loss: 0.001992610516026616\n",
      "Step: 3425, Loss: 0.002482938813045621\n",
      "Data loss: 0.000445435696747154, Function loss: 0.0023183233570307493\n",
      "Step: 3426, Loss: 0.0027637591119855642\n",
      "Data loss: 0.0005256158765405416, Function loss: 0.0024361948017030954\n",
      "Step: 3427, Loss: 0.002961810678243637\n",
      "Data loss: 0.00045880177640356123, Function loss: 0.002555765910074115\n",
      "Step: 3428, Loss: 0.0030145677737891674\n",
      "Data loss: 0.0005289856344461441, Function loss: 0.0023792225401848555\n",
      "Step: 3429, Loss: 0.0029082081746309996\n",
      "Data loss: 0.0004480255302041769, Function loss: 0.002206494566053152\n",
      "Step: 3430, Loss: 0.002654520096257329\n",
      "Data loss: 0.0004902328946627676, Function loss: 0.001810619723983109\n",
      "Step: 3431, Loss: 0.0023008526768535376\n",
      "Data loss: 0.00041552470065653324, Function loss: 0.0014977206010371447\n",
      "Step: 3432, Loss: 0.001913245301693678\n",
      "Data loss: 0.0004262157017365098, Function loss: 0.0011388777056708932\n",
      "Step: 3433, Loss: 0.001565093407407403\n",
      "Data loss: 0.0003841622674372047, Function loss: 0.0009227031259797513\n",
      "Step: 3434, Loss: 0.0013068653643131256\n",
      "Data loss: 0.0003725440183188766, Function loss: 0.0007921542273834348\n",
      "Step: 3435, Loss: 0.0011646982748061419\n",
      "Data loss: 0.00037723797140643, Function loss: 0.0007553413161076605\n",
      "Step: 3436, Loss: 0.0011325792875140905\n",
      "Data loss: 0.0003511937102302909, Function loss: 0.0008165513863787055\n",
      "Step: 3437, Loss: 0.0011677450966089964\n",
      "Data loss: 0.00039105070754885674, Function loss: 0.0008451734902337193\n",
      "Step: 3438, Loss: 0.001236224197782576\n",
      "Data loss: 0.0003540849720593542, Function loss: 0.0009346366277895868\n",
      "Step: 3439, Loss: 0.0012887215707451105\n",
      "Data loss: 0.00040447988430969417, Function loss: 0.0009048847132362425\n",
      "Step: 3440, Loss: 0.0013093645684421062\n",
      "Data loss: 0.0003631068393588066, Function loss: 0.0009296645293943584\n",
      "Step: 3441, Loss: 0.001292771426960826\n",
      "Data loss: 0.0004040749918203801, Function loss: 0.0008448381558991969\n",
      "Step: 3442, Loss: 0.0012489131186157465\n",
      "Data loss: 0.0003674845793284476, Function loss: 0.0008136999676935375\n",
      "Step: 3443, Loss: 0.001181184547021985\n",
      "Data loss: 0.00038727751234546304, Function loss: 0.0007198863895609975\n",
      "Step: 3444, Loss: 0.0011071639601141214\n",
      "Data loss: 0.00036618413287214935, Function loss: 0.000679454009514302\n",
      "Step: 3445, Loss: 0.001045638113282621\n",
      "Data loss: 0.0003668883291538805, Function loss: 0.0006372373318299651\n",
      "Step: 3446, Loss: 0.001004125690087676\n",
      "Data loss: 0.00036551992525346577, Function loss: 0.0006144322105683386\n",
      "Step: 3447, Loss: 0.0009799521649256349\n",
      "Data loss: 0.00035336168366484344, Function loss: 0.0006116600125096738\n",
      "Step: 3448, Loss: 0.0009650216670706868\n",
      "Data loss: 0.00036691472632810473, Function loss: 0.0005918197566643357\n",
      "Step: 3449, Loss: 0.0009587344829924405\n",
      "Data loss: 0.0003470250521786511, Function loss: 0.0006097090663388371\n",
      "Step: 3450, Loss: 0.0009567341185174882\n",
      "Data loss: 0.0003691565652843565, Function loss: 0.000588367460295558\n",
      "Step: 3451, Loss: 0.0009575240546837449\n",
      "Data loss: 0.0003456755948718637, Function loss: 0.0006086712819524109\n",
      "Step: 3452, Loss: 0.0009543468477204442\n",
      "Data loss: 0.00036883613211102784, Function loss: 0.0005773736047558486\n",
      "Step: 3453, Loss: 0.0009462097659707069\n",
      "Data loss: 0.00034520128974691033, Function loss: 0.0005913266795687377\n",
      "Step: 3454, Loss: 0.0009365279693156481\n",
      "Data loss: 0.0003669667348731309, Function loss: 0.0005607325001619756\n",
      "Step: 3455, Loss: 0.000927699264138937\n",
      "Data loss: 0.000345047126756981, Function loss: 0.0005734366131946445\n",
      "Step: 3456, Loss: 0.000918483710847795\n",
      "Data loss: 0.00036440175608731806, Function loss: 0.0005452071782201529\n",
      "Step: 3457, Loss: 0.0009096089052036405\n",
      "Data loss: 0.0003448889183346182, Function loss: 0.0005540846032090485\n",
      "Step: 3458, Loss: 0.0008989735506474972\n",
      "Data loss: 0.0003609670384321362, Function loss: 0.0005265521467663348\n",
      "Step: 3459, Loss: 0.0008875192143023014\n",
      "Data loss: 0.0003449463110882789, Function loss: 0.0005300006014294922\n",
      "Step: 3460, Loss: 0.0008749469416216016\n",
      "Data loss: 0.0003562794590834528, Function loss: 0.0005053464556112885\n",
      "Step: 3461, Loss: 0.0008616258855909109\n",
      "Data loss: 0.00034566555405035615, Function loss: 0.0005068464670330286\n",
      "Step: 3462, Loss: 0.0008525120210833848\n",
      "Data loss: 0.0003521647013258189, Function loss: 0.000495963147841394\n",
      "Step: 3463, Loss: 0.0008481278782710433\n",
      "Data loss: 0.00034693817724473774, Function loss: 0.0004994369228370488\n",
      "Step: 3464, Loss: 0.000846375129185617\n",
      "Data loss: 0.0003495907294563949, Function loss: 0.0004973618779331446\n",
      "Step: 3465, Loss: 0.0008469526073895395\n",
      "Data loss: 0.0003482690954115242, Function loss: 0.0004999310476705432\n",
      "Step: 3466, Loss: 0.0008482001721858978\n",
      "Data loss: 0.00034750468330457807, Function loss: 0.0005037093069404364\n",
      "Step: 3467, Loss: 0.0008512139902450144\n",
      "Data loss: 0.0003498885198496282, Function loss: 0.000504857103805989\n",
      "Step: 3468, Loss: 0.0008547456236556172\n",
      "Data loss: 0.000345773616572842, Function loss: 0.000514241517521441\n",
      "Step: 3469, Loss: 0.0008600151631981134\n",
      "Data loss: 0.00035189522895962, Function loss: 0.0005125704919919372\n",
      "Step: 3470, Loss: 0.0008644657209515572\n",
      "Data loss: 0.0003442091110628098, Function loss: 0.0005229012458585203\n",
      "Step: 3471, Loss: 0.0008671103278174996\n",
      "Data loss: 0.0003526671789586544, Function loss: 0.0005155589897185564\n",
      "Step: 3472, Loss: 0.0008682261686772108\n",
      "Data loss: 0.00034292537020519376, Function loss: 0.0005258067976683378\n",
      "Step: 3473, Loss: 0.0008687321678735316\n",
      "Data loss: 0.00035237681004218757, Function loss: 0.0005145515315234661\n",
      "Step: 3474, Loss: 0.0008669283706694841\n",
      "Data loss: 0.00034246957511641085, Function loss: 0.0005216649151407182\n",
      "Step: 3475, Loss: 0.0008641345193609595\n",
      "Data loss: 0.00035053916508331895, Function loss: 0.0005114427185617387\n",
      "Step: 3476, Loss: 0.0008619818836450577\n",
      "Data loss: 0.0003428537165746093, Function loss: 0.0005180047010071576\n",
      "Step: 3477, Loss: 0.0008608584175817668\n",
      "Data loss: 0.0003479802398942411, Function loss: 0.0005131630459800363\n",
      "Step: 3478, Loss: 0.0008611432858742774\n",
      "Data loss: 0.00034476787550374866, Function loss: 0.0005213533877395093\n",
      "Step: 3479, Loss: 0.000866121263243258\n",
      "Data loss: 0.00034452902036719024, Function loss: 0.0005343618104234338\n",
      "Step: 3480, Loss: 0.0008788908598944545\n",
      "Data loss: 0.00034884270280599594, Function loss: 0.0005592118832282722\n",
      "Step: 3481, Loss: 0.0009080545860342681\n",
      "Data loss: 0.0003416574327275157, Function loss: 0.0006254129111766815\n",
      "Step: 3482, Loss: 0.0009670703439041972\n",
      "Data loss: 0.00035762920742854476, Function loss: 0.0007191836484707892\n",
      "Step: 3483, Loss: 0.001076812855899334\n",
      "Data loss: 0.00034220764064230025, Function loss: 0.0009230637224391103\n",
      "Step: 3484, Loss: 0.00126527133397758\n",
      "Data loss: 0.00037801865255460143, Function loss: 0.0012256368063390255\n",
      "Step: 3485, Loss: 0.0016036555171012878\n",
      "Data loss: 0.0003542681806720793, Function loss: 0.0018146542133763433\n",
      "Step: 3486, Loss: 0.0021689224522560835\n",
      "Data loss: 0.000429606152465567, Function loss: 0.002771819941699505\n",
      "Step: 3487, Loss: 0.0032014260068535805\n",
      "Data loss: 0.0004017032333649695, Function loss: 0.00439456244930625\n",
      "Step: 3488, Loss: 0.004796265624463558\n",
      "Data loss: 0.0005578910349868238, Function loss: 0.007031168323010206\n",
      "Step: 3489, Loss: 0.007589059416204691\n",
      "Data loss: 0.0005386624252423644, Function loss: 0.01051921583712101\n",
      "Step: 3490, Loss: 0.011057877913117409\n",
      "Data loss: 0.0008250083192251623, Function loss: 0.01593436300754547\n",
      "Step: 3491, Loss: 0.016759371384978294\n",
      "Data loss: 0.0007958840578794479, Function loss: 0.02037695050239563\n",
      "Step: 3492, Loss: 0.021172834560275078\n",
      "Data loss: 0.0011475273640826344, Function loss: 0.025672566145658493\n",
      "Step: 3493, Loss: 0.026820093393325806\n",
      "Data loss: 0.0009218511986546218, Function loss: 0.02314876765012741\n",
      "Step: 3494, Loss: 0.02407061867415905\n",
      "Data loss: 0.0009818481048569083, Function loss: 0.017150992527604103\n",
      "Step: 3495, Loss: 0.01813284121453762\n",
      "Data loss: 0.0005563597660511732, Function loss: 0.00663775485008955\n",
      "Step: 3496, Loss: 0.007194114848971367\n",
      "Data loss: 0.0004756259440910071, Function loss: 0.0009305421845056117\n",
      "Step: 3497, Loss: 0.0014061680994927883\n",
      "Data loss: 0.0005234534037299454, Function loss: 0.0030769282020628452\n",
      "Step: 3498, Loss: 0.0036003815475851297\n",
      "Data loss: 0.0005623477627523243, Function loss: 0.008427748456597328\n",
      "Step: 3499, Loss: 0.008990095928311348\n",
      "Data loss: 0.0008044629939831793, Function loss: 0.010138200595974922\n",
      "Step: 3500, Loss: 0.010942663997411728\n",
      "Data loss: 0.0005525517626665533, Function loss: 0.005291747860610485\n",
      "Step: 3501, Loss: 0.005844299681484699\n",
      "Data loss: 0.0005492796190083027, Function loss: 0.0008655281853862107\n",
      "Step: 3502, Loss: 0.0014148077461868525\n",
      "Data loss: 0.0005731459241360426, Function loss: 0.0018617716850712895\n",
      "Step: 3503, Loss: 0.002434917725622654\n",
      "Data loss: 0.0005705227958969772, Function loss: 0.0052916682325303555\n",
      "Step: 3504, Loss: 0.00586219085380435\n",
      "Data loss: 0.0007165499846450984, Function loss: 0.00543028162792325\n",
      "Step: 3505, Loss: 0.006146831437945366\n",
      "Data loss: 0.0005419072695076466, Function loss: 0.0020654406398534775\n",
      "Step: 3506, Loss: 0.002607347909361124\n",
      "Data loss: 0.000545441871508956, Function loss: 0.0005547066684812307\n",
      "Step: 3507, Loss: 0.0011001485399901867\n",
      "Data loss: 0.0006264556432142854, Function loss: 0.0024594119749963284\n",
      "Step: 3508, Loss: 0.003085867501795292\n",
      "Data loss: 0.0005856556817889214, Function loss: 0.0037823065649718046\n",
      "Step: 3509, Loss: 0.00436796247959137\n",
      "Data loss: 0.0006276538479141891, Function loss: 0.0022873778361827135\n",
      "Step: 3510, Loss: 0.0029150317423045635\n",
      "Data loss: 0.0005544133018702269, Function loss: 0.0005444838316179812\n",
      "Step: 3511, Loss: 0.0010988970752805471\n",
      "Data loss: 0.000553447287529707, Function loss: 0.0011907939333468676\n",
      "Step: 3512, Loss: 0.0017442412208765745\n",
      "Data loss: 0.0006263727555051446, Function loss: 0.0025829877704381943\n",
      "Step: 3513, Loss: 0.003209360409528017\n",
      "Data loss: 0.0005517444224096835, Function loss: 0.0021864622831344604\n",
      "Step: 3514, Loss: 0.002738206647336483\n",
      "Data loss: 0.0005619256990030408, Function loss: 0.0007835687720216811\n",
      "Step: 3515, Loss: 0.0013454945292323828\n",
      "Data loss: 0.000542704074177891, Function loss: 0.0005756898317486048\n",
      "Step: 3516, Loss: 0.0011183938477188349\n",
      "Data loss: 0.0005316315218806267, Function loss: 0.0014985090820118785\n",
      "Step: 3517, Loss: 0.002030140720307827\n",
      "Data loss: 0.0005790576688013971, Function loss: 0.0018014024244621396\n",
      "Step: 3518, Loss: 0.0023804600350558758\n",
      "Data loss: 0.0004993371549062431, Function loss: 0.0010677612153813243\n",
      "Step: 3519, Loss: 0.0015670983120799065\n",
      "Data loss: 0.0005100408452562988, Function loss: 0.0004698241828009486\n",
      "Step: 3520, Loss: 0.0009798649698495865\n",
      "Data loss: 0.0005156443221494555, Function loss: 0.0008271964616142213\n",
      "Step: 3521, Loss: 0.001342840725556016\n",
      "Data loss: 0.00048519112169742584, Function loss: 0.0013634603237733245\n",
      "Step: 3522, Loss: 0.0018486514454707503\n",
      "Data loss: 0.0005210686940699816, Function loss: 0.001173045369796455\n",
      "Step: 3523, Loss: 0.0016941140638664365\n",
      "Data loss: 0.0004608115996234119, Function loss: 0.0006514639244414866\n",
      "Step: 3524, Loss: 0.0011122755240648985\n",
      "Data loss: 0.0004684376763179898, Function loss: 0.00048823346151039004\n",
      "Step: 3525, Loss: 0.0009566711378283799\n",
      "Data loss: 0.00048685618094168603, Function loss: 0.0008287975797429681\n",
      "Step: 3526, Loss: 0.0013156537897884846\n",
      "Data loss: 0.0004509883001446724, Function loss: 0.001067435834556818\n",
      "Step: 3527, Loss: 0.0015184241347014904\n",
      "Data loss: 0.00047666984028182924, Function loss: 0.0008074176148511469\n",
      "Step: 3528, Loss: 0.0012840874260291457\n",
      "Data loss: 0.0004368974477984011, Function loss: 0.0005159733700565994\n",
      "Step: 3529, Loss: 0.0009528708178550005\n",
      "Data loss: 0.0004359492158982903, Function loss: 0.0005027303122915328\n",
      "Step: 3530, Loss: 0.0009386795572936535\n",
      "Data loss: 0.00045566572225652635, Function loss: 0.0007076918263919652\n",
      "Step: 3531, Loss: 0.001163357519544661\n",
      "Data loss: 0.0004246993630658835, Function loss: 0.0008350141579285264\n",
      "Step: 3532, Loss: 0.0012597135500982404\n",
      "Data loss: 0.0004449677071534097, Function loss: 0.0006666752742603421\n",
      "Step: 3533, Loss: 0.001111642923206091\n",
      "Data loss: 0.00041819008765742183, Function loss: 0.0004951473674736917\n",
      "Step: 3534, Loss: 0.0009133374551311135\n",
      "Data loss: 0.00041330038220621645, Function loss: 0.00047631090274080634\n",
      "Step: 3535, Loss: 0.0008896113140508533\n",
      "Data loss: 0.0004286826297175139, Function loss: 0.0005824985564686358\n",
      "Step: 3536, Loss: 0.0010111811570823193\n",
      "Data loss: 0.0004038644547108561, Function loss: 0.0006734608323313296\n",
      "Step: 3537, Loss: 0.0010773253161460161\n",
      "Data loss: 0.000418903335230425, Function loss: 0.0005778927588835359\n",
      "Step: 3538, Loss: 0.0009967960650101304\n",
      "Data loss: 0.00040434818947687745, Function loss: 0.00048191097448579967\n",
      "Step: 3539, Loss: 0.0008862591348588467\n",
      "Data loss: 0.0003931249084416777, Function loss: 0.0004784711927641183\n",
      "Step: 3540, Loss: 0.000871596101205796\n",
      "Data loss: 0.0004116141935810447, Function loss: 0.0005250952672213316\n",
      "Step: 3541, Loss: 0.0009367094608023763\n",
      "Data loss: 0.00038347410736605525, Function loss: 0.0005958195542916656\n",
      "Step: 3542, Loss: 0.0009792936034500599\n",
      "Data loss: 0.0004048922273796052, Function loss: 0.0005356924375519156\n",
      "Step: 3543, Loss: 0.0009405846940353513\n",
      "Data loss: 0.0003836168034467846, Function loss: 0.0004866387462243438\n",
      "Step: 3544, Loss: 0.0008702555205672979\n",
      "Data loss: 0.00038635788951069117, Function loss: 0.000454818073194474\n",
      "Step: 3545, Loss: 0.0008411759627051651\n",
      "Data loss: 0.00038947921711951494, Function loss: 0.0004750555963255465\n",
      "Step: 3546, Loss: 0.0008645348134450614\n",
      "Data loss: 0.00037540195626206696, Function loss: 0.0005189615767449141\n",
      "Step: 3547, Loss: 0.0008943635039031506\n",
      "Data loss: 0.0003886248159687966, Function loss: 0.0004998841905035079\n",
      "Step: 3548, Loss: 0.000888508977368474\n",
      "Data loss: 0.0003724483831319958, Function loss: 0.00048002900439314544\n",
      "Step: 3549, Loss: 0.0008524773875251412\n",
      "Data loss: 0.0003771383489947766, Function loss: 0.0004547760763671249\n",
      "Step: 3550, Loss: 0.0008319144253619015\n",
      "Data loss: 0.00037633354077115655, Function loss: 0.0004709553031716496\n",
      "Step: 3551, Loss: 0.0008472888730466366\n",
      "Data loss: 0.00036854002973996103, Function loss: 0.0005043166456744075\n",
      "Step: 3552, Loss: 0.000872856704518199\n",
      "Data loss: 0.000375552277546376, Function loss: 0.0004977824864909053\n",
      "Step: 3553, Loss: 0.0008733347640372813\n",
      "Data loss: 0.0003672755847219378, Function loss: 0.0004813868144992739\n",
      "Step: 3554, Loss: 0.0008486623992212117\n",
      "Data loss: 0.00036595153505913913, Function loss: 0.0004607737937476486\n",
      "Step: 3555, Loss: 0.0008267253288067877\n",
      "Data loss: 0.00036899076076224446, Function loss: 0.00045861874241381884\n",
      "Step: 3556, Loss: 0.0008276095031760633\n",
      "Data loss: 0.00035932770697399974, Function loss: 0.0004823110066354275\n",
      "Step: 3557, Loss: 0.0008416387136094272\n",
      "Data loss: 0.0003680596419144422, Function loss: 0.00048432612675242126\n",
      "Step: 3558, Loss: 0.0008523857686668634\n",
      "Data loss: 0.0003582144563551992, Function loss: 0.0004902587388642132\n",
      "Step: 3559, Loss: 0.000848473166115582\n",
      "Data loss: 0.0003622847143560648, Function loss: 0.0004735658585559577\n",
      "Step: 3560, Loss: 0.0008358506020158529\n",
      "Data loss: 0.00035909528378397226, Function loss: 0.00046234819456003606\n",
      "Step: 3561, Loss: 0.0008214435074478388\n",
      "Data loss: 0.0003550790133886039, Function loss: 0.00046137309982441366\n",
      "Step: 3562, Loss: 0.0008164520841091871\n",
      "Data loss: 0.0003605548699852079, Function loss: 0.00046153931180015206\n",
      "Step: 3563, Loss: 0.0008220941526815295\n",
      "Data loss: 0.00035076920175924897, Function loss: 0.00047991194878704846\n",
      "Step: 3564, Loss: 0.0008306811796501279\n",
      "Data loss: 0.00035941251553595066, Function loss: 0.0004731345397885889\n",
      "Step: 3565, Loss: 0.00083254708442837\n",
      "Data loss: 0.000350240123225376, Function loss: 0.00047571517643518746\n",
      "Step: 3566, Loss: 0.0008259552996605635\n",
      "Data loss: 0.0003537834563758224, Function loss: 0.00046037998981773853\n",
      "Step: 3567, Loss: 0.0008141634752973914\n",
      "Data loss: 0.0003523400810081512, Function loss: 0.0004570653836708516\n",
      "Step: 3568, Loss: 0.0008094054646790028\n",
      "Data loss: 0.0003478545986581594, Function loss: 0.0004663925210479647\n",
      "Step: 3569, Loss: 0.0008142471197061241\n",
      "Data loss: 0.00035385627415962517, Function loss: 0.00046724872663617134\n",
      "Step: 3570, Loss: 0.0008211049716919661\n",
      "Data loss: 0.0003451844968367368, Function loss: 0.0004760871233884245\n",
      "Step: 3571, Loss: 0.0008212716202251613\n",
      "Data loss: 0.0003524938365444541, Function loss: 0.00046496259164996445\n",
      "Step: 3572, Loss: 0.0008174563990905881\n",
      "Data loss: 0.0003443530004005879, Function loss: 0.00046795158414170146\n",
      "Step: 3573, Loss: 0.0008123045554384589\n",
      "Data loss: 0.00034966770908795297, Function loss: 0.000458270194940269\n",
      "Step: 3574, Loss: 0.0008079379331320524\n",
      "Data loss: 0.0003449968062341213, Function loss: 0.0004609835450537503\n",
      "Step: 3575, Loss: 0.0008059803512878716\n",
      "Data loss: 0.00034657237119972706, Function loss: 0.00046094675781205297\n",
      "Step: 3576, Loss: 0.00080751912901178\n",
      "Data loss: 0.0003457077546045184, Function loss: 0.00046521646436303854\n",
      "Step: 3577, Loss: 0.000810924218967557\n",
      "Data loss: 0.0003442780871409923, Function loss: 0.0004704008169937879\n",
      "Step: 3578, Loss: 0.0008146789041347802\n",
      "Data loss: 0.00034536924795247614, Function loss: 0.0004714112146757543\n",
      "Step: 3579, Loss: 0.0008167804917320609\n",
      "Data loss: 0.0003429851494729519, Function loss: 0.000471273873699829\n",
      "Step: 3580, Loss: 0.0008142590522766113\n",
      "Data loss: 0.0003435585822444409, Function loss: 0.00046452521928586066\n",
      "Step: 3581, Loss: 0.0008080838015303016\n",
      "Data loss: 0.0003428606141824275, Function loss: 0.0004603340057656169\n",
      "Step: 3582, Loss: 0.000803194590844214\n",
      "Data loss: 0.0003415430255699903, Function loss: 0.00045829109149053693\n",
      "Step: 3583, Loss: 0.0007998341461643577\n",
      "Data loss: 0.0003423504822421819, Function loss: 0.0004558799264486879\n",
      "Step: 3584, Loss: 0.0007982304086908698\n",
      "Data loss: 0.00034003378823399544, Function loss: 0.0004575963248498738\n",
      "Step: 3585, Loss: 0.0007976301130838692\n",
      "Data loss: 0.00034168019192293286, Function loss: 0.0004559907829388976\n",
      "Step: 3586, Loss: 0.0007976709748618305\n",
      "Data loss: 0.0003394352679606527, Function loss: 0.0004579621599987149\n",
      "Step: 3587, Loss: 0.0007973974570631981\n",
      "Data loss: 0.0003408066986594349, Function loss: 0.00045657192822545767\n",
      "Step: 3588, Loss: 0.0007973785977810621\n",
      "Data loss: 0.0003390033380128443, Function loss: 0.00045814181794412434\n",
      "Step: 3589, Loss: 0.0007971451850607991\n",
      "Data loss: 0.0003394840459804982, Function loss: 0.0004568018193822354\n",
      "Step: 3590, Loss: 0.0007962858653627336\n",
      "Data loss: 0.0003389241173863411, Function loss: 0.00045659919851459563\n",
      "Step: 3591, Loss: 0.0007955232867971063\n",
      "Data loss: 0.00033806590363383293, Function loss: 0.0004566846473608166\n",
      "Step: 3592, Loss: 0.0007947505218908191\n",
      "Data loss: 0.0003391406498849392, Function loss: 0.0004555807390715927\n",
      "Step: 3593, Loss: 0.0007947214180603623\n",
      "Data loss: 0.0003365635056979954, Function loss: 0.0004593334160745144\n",
      "Step: 3594, Loss: 0.0007958969217725098\n",
      "Data loss: 0.0003401468857191503, Function loss: 0.00045720336493104696\n",
      "Step: 3595, Loss: 0.0007973502506501973\n",
      "Data loss: 0.0003346699522808194, Function loss: 0.00046783892321400344\n",
      "Step: 3596, Loss: 0.0008025089045986533\n",
      "Data loss: 0.0003424535389058292, Function loss: 0.0004666508757509291\n",
      "Step: 3597, Loss: 0.0008091044146567583\n",
      "Data loss: 0.00033345675910823047, Function loss: 0.0004880045889876783\n",
      "Step: 3598, Loss: 0.0008214613189920783\n",
      "Data loss: 0.0003468147187959403, Function loss: 0.0004927857080474496\n",
      "Step: 3599, Loss: 0.0008396004559472203\n",
      "Data loss: 0.00033362096291966736, Function loss: 0.0005348538979887962\n",
      "Step: 3600, Loss: 0.0008684748318046331\n",
      "Data loss: 0.00035471850424073637, Function loss: 0.0005561701254919171\n",
      "Step: 3601, Loss: 0.000910888658836484\n",
      "Data loss: 0.00033716802136041224, Function loss: 0.0006414767703972757\n",
      "Step: 3602, Loss: 0.0009786448208615184\n",
      "Data loss: 0.0003722300461959094, Function loss: 0.0007151419413276017\n",
      "Step: 3603, Loss: 0.0010873719584196806\n",
      "Data loss: 0.0003504717315081507, Function loss: 0.0009110913961194456\n",
      "Step: 3604, Loss: 0.0012615631567314267\n",
      "Data loss: 0.0004125012783333659, Function loss: 0.0011240069288760424\n",
      "Step: 3605, Loss: 0.0015365082072094083\n",
      "Data loss: 0.0003893951652571559, Function loss: 0.001568339066579938\n",
      "Step: 3606, Loss: 0.001957734115421772\n",
      "Data loss: 0.0005003797705285251, Function loss: 0.002096977084875107\n",
      "Step: 3607, Loss: 0.002597356913611293\n",
      "Data loss: 0.00048543038428761065, Function loss: 0.0030337590724229813\n",
      "Step: 3608, Loss: 0.0035191895440220833\n",
      "Data loss: 0.0006704319384880364, Function loss: 0.0040886360220611095\n",
      "Step: 3609, Loss: 0.004759068135172129\n",
      "Data loss: 0.0006594789447262883, Function loss: 0.005592572968453169\n",
      "Step: 3610, Loss: 0.006252051796764135\n",
      "Data loss: 0.0008933712379075587, Function loss: 0.006841615308076143\n",
      "Step: 3611, Loss: 0.007734986487776041\n",
      "Data loss: 0.0008096956298686564, Function loss: 0.007886246778070927\n",
      "Step: 3612, Loss: 0.008695942349731922\n",
      "Data loss: 0.0009292507893405855, Function loss: 0.007519384380429983\n",
      "Step: 3613, Loss: 0.00844863522797823\n",
      "Data loss: 0.000654806150123477, Function loss: 0.005994285922497511\n",
      "Step: 3614, Loss: 0.006649091839790344\n",
      "Data loss: 0.0005762194050475955, Function loss: 0.003433027770370245\n",
      "Step: 3615, Loss: 0.004009247291833162\n",
      "Data loss: 0.00035602060961537063, Function loss: 0.001533546601422131\n",
      "Step: 3616, Loss: 0.0018895672401413321\n",
      "Data loss: 0.0003705860290210694, Function loss: 0.0010019243927672505\n",
      "Step: 3617, Loss: 0.0013725104508921504\n",
      "Data loss: 0.0004580295935738832, Function loss: 0.0018279318464919925\n",
      "Step: 3618, Loss: 0.002285961527377367\n",
      "Data loss: 0.0005229587550275028, Function loss: 0.0029545461293309927\n",
      "Step: 3619, Loss: 0.0034775049425661564\n",
      "Data loss: 0.0005927116726525128, Function loss: 0.003153471043333411\n",
      "Step: 3620, Loss: 0.003746182657778263\n",
      "Data loss: 0.00047426449600607157, Function loss: 0.0023582756984978914\n",
      "Step: 3621, Loss: 0.002832540310919285\n",
      "Data loss: 0.0004188298189546913, Function loss: 0.0011295718140900135\n",
      "Step: 3622, Loss: 0.0015484016621485353\n",
      "Data loss: 0.00034958127071149647, Function loss: 0.0005993829108774662\n",
      "Step: 3623, Loss: 0.0009489641524851322\n",
      "Data loss: 0.0003903701435774565, Function loss: 0.0009725495474413037\n",
      "Step: 3624, Loss: 0.0013629196910187602\n",
      "Data loss: 0.0004598188679665327, Function loss: 0.001674510887823999\n",
      "Step: 3625, Loss: 0.00213432963937521\n",
      "Data loss: 0.00046218125498853624, Function loss: 0.0019085091771557927\n",
      "Step: 3626, Loss: 0.0023706904612481594\n",
      "Data loss: 0.00044366836664266884, Function loss: 0.0013816456776112318\n",
      "Step: 3627, Loss: 0.0018253140151500702\n",
      "Data loss: 0.00037182425148785114, Function loss: 0.0006978120072744787\n",
      "Step: 3628, Loss: 0.0010696363169699907\n",
      "Data loss: 0.00036198171437717974, Function loss: 0.00045786111149936914\n",
      "Step: 3629, Loss: 0.0008198427967727184\n",
      "Data loss: 0.0003905585908796638, Function loss: 0.0008032791665755212\n",
      "Step: 3630, Loss: 0.0011938377283513546\n",
      "Data loss: 0.00041427556425333023, Function loss: 0.0012487791245803237\n",
      "Step: 3631, Loss: 0.001663054688833654\n",
      "Data loss: 0.0004324351320974529, Function loss: 0.0012661553919315338\n",
      "Step: 3632, Loss: 0.0016985905822366476\n",
      "Data loss: 0.0003872676461469382, Function loss: 0.0008969357004389167\n",
      "Step: 3633, Loss: 0.0012842033756896853\n",
      "Data loss: 0.00036607470246963203, Function loss: 0.000500082562211901\n",
      "Step: 3634, Loss: 0.0008661572355777025\n",
      "Data loss: 0.0003604944213293493, Function loss: 0.0004623275017365813\n",
      "Step: 3635, Loss: 0.0008228219230659306\n",
      "Data loss: 0.0003721814719028771, Function loss: 0.0007198828388936818\n",
      "Step: 3636, Loss: 0.0010920643107965589\n",
      "Data loss: 0.00040265193092636764, Function loss: 0.0009302595281042159\n",
      "Step: 3637, Loss: 0.001332911429926753\n",
      "Data loss: 0.000383546605007723, Function loss: 0.0009167794487439096\n",
      "Step: 3638, Loss: 0.001300326082855463\n",
      "Data loss: 0.0003773381467908621, Function loss: 0.0006709664594382048\n",
      "Step: 3639, Loss: 0.0010483046062290668\n",
      "Data loss: 0.0003548666718415916, Function loss: 0.00047172518679872155\n",
      "Step: 3640, Loss: 0.0008265918586403131\n",
      "Data loss: 0.00034997789771296084, Function loss: 0.0004539159999694675\n",
      "Step: 3641, Loss: 0.0008038938976824284\n",
      "Data loss: 0.00037045241333544254, Function loss: 0.0005701383925043046\n",
      "Step: 3642, Loss: 0.0009405908058397472\n",
      "Data loss: 0.00036399895907379687, Function loss: 0.0007192303892225027\n",
      "Step: 3643, Loss: 0.0010832293191924691\n",
      "Data loss: 0.0003804070292972028, Function loss: 0.0007230403134599328\n",
      "Step: 3644, Loss: 0.0011034472845494747\n",
      "Data loss: 0.0003599417395889759, Function loss: 0.0006405417807400227\n",
      "Step: 3645, Loss: 0.0010004835203289986\n",
      "Data loss: 0.00035188745823688805, Function loss: 0.0005259858444333076\n",
      "Step: 3646, Loss: 0.0008778732735663652\n",
      "Data loss: 0.0003570514963939786, Function loss: 0.00048547290498390794\n",
      "Step: 3647, Loss: 0.0008425244013778865\n",
      "Data loss: 0.00034012063406407833, Function loss: 0.0005571888759732246\n",
      "Step: 3648, Loss: 0.000897309510037303\n",
      "Data loss: 0.00037226721178740263, Function loss: 0.0005918894894421101\n",
      "Step: 3649, Loss: 0.0009641567012295127\n",
      "Data loss: 0.0003430237411521375, Function loss: 0.0006249872967600822\n",
      "Step: 3650, Loss: 0.0009680110379122198\n",
      "Data loss: 0.00036550380173139274, Function loss: 0.0005367019912227988\n",
      "Step: 3651, Loss: 0.000902205822058022\n",
      "Data loss: 0.00033988605719059706, Function loss: 0.0004831278638448566\n",
      "Step: 3652, Loss: 0.0008230139501392841\n",
      "Data loss: 0.0003483086184132844, Function loss: 0.00043747900053858757\n",
      "Step: 3653, Loss: 0.0007857875898480415\n",
      "Data loss: 0.0003469589282758534, Function loss: 0.0004512548039201647\n",
      "Step: 3654, Loss: 0.0007982137612998486\n",
      "Data loss: 0.00034397401032038033, Function loss: 0.00048653880367055535\n",
      "Step: 3655, Loss: 0.0008305128430947661\n",
      "Data loss: 0.0003544703940860927, Function loss: 0.0004909367416985333\n",
      "Step: 3656, Loss: 0.000845407135784626\n",
      "Data loss: 0.00034169224090874195, Function loss: 0.0004885600646957755\n",
      "Step: 3657, Loss: 0.0008302523056045175\n",
      "Data loss: 0.00034870431409217417, Function loss: 0.00044820350012741983\n",
      "Step: 3658, Loss: 0.000796907814219594\n",
      "Data loss: 0.00034038160811178386, Function loss: 0.0004323830362409353\n",
      "Step: 3659, Loss: 0.0007727646734565496\n",
      "Data loss: 0.00034130128915421665, Function loss: 0.0004324565234128386\n",
      "Step: 3660, Loss: 0.0007737578125670552\n",
      "Data loss: 0.00034519005566835403, Function loss: 0.00044830390834249556\n",
      "Step: 3661, Loss: 0.0007934939349070191\n",
      "Data loss: 0.0003405684547033161, Function loss: 0.0004746470076497644\n",
      "Step: 3662, Loss: 0.0008152154623530805\n",
      "Data loss: 0.00034734338987618685, Function loss: 0.0004794759734068066\n",
      "Step: 3663, Loss: 0.000826819334179163\n",
      "Data loss: 0.0003415399114601314, Function loss: 0.0004791224200744182\n",
      "Step: 3664, Loss: 0.00082066236063838\n",
      "Data loss: 0.0003410330100450665, Function loss: 0.0004675480886362493\n",
      "Step: 3665, Loss: 0.0008085811277851462\n",
      "Data loss: 0.00034293520729988813, Function loss: 0.0004544850380625576\n",
      "Step: 3666, Loss: 0.0007974202744662762\n",
      "Data loss: 0.00033467763569206, Function loss: 0.0004605594731401652\n",
      "Step: 3667, Loss: 0.0007952371379360557\n",
      "Data loss: 0.0003453546087257564, Function loss: 0.0004531379963736981\n",
      "Step: 3668, Loss: 0.0007984925759956241\n",
      "Data loss: 0.00033276985050179064, Function loss: 0.00046859285794198513\n",
      "Step: 3669, Loss: 0.0008013626793399453\n",
      "Data loss: 0.0003459388099145144, Function loss: 0.00045482118730433285\n",
      "Step: 3670, Loss: 0.0008007599972188473\n",
      "Data loss: 0.0003325320139992982, Function loss: 0.00046312721678987145\n",
      "Step: 3671, Loss: 0.0007956592598930001\n",
      "Data loss: 0.000343224557582289, Function loss: 0.0004462053475435823\n",
      "Step: 3672, Loss: 0.0007894298760220408\n",
      "Data loss: 0.0003332702617626637, Function loss: 0.00045307830441743135\n",
      "Step: 3673, Loss: 0.0007863485952839255\n",
      "Data loss: 0.0003405697934795171, Function loss: 0.00044625060399994254\n",
      "Step: 3674, Loss: 0.0007868204265832901\n",
      "Data loss: 0.00033485848689451814, Function loss: 0.0004562204412650317\n",
      "Step: 3675, Loss: 0.0007910788990557194\n",
      "Data loss: 0.00033942353911697865, Function loss: 0.0004559002409223467\n",
      "Step: 3676, Loss: 0.0007953237509354949\n",
      "Data loss: 0.0003352179192006588, Function loss: 0.0004627110611181706\n",
      "Step: 3677, Loss: 0.0007979290094226599\n",
      "Data loss: 0.00033952639205381274, Function loss: 0.0004619655665010214\n",
      "Step: 3678, Loss: 0.0008014919585548341\n",
      "Data loss: 0.00033300332142971456, Function loss: 0.00047563324915245175\n",
      "Step: 3679, Loss: 0.0008086365414783359\n",
      "Data loss: 0.00034213432809337974, Function loss: 0.0004875703598372638\n",
      "Step: 3680, Loss: 0.0008297046879306436\n",
      "Data loss: 0.00032913434552028775, Function loss: 0.0005368462298065424\n",
      "Step: 3681, Loss: 0.0008659805753268301\n",
      "Data loss: 0.000348793575540185, Function loss: 0.0005810534348711371\n",
      "Step: 3682, Loss: 0.0009298470104113221\n",
      "Data loss: 0.0003257356001995504, Function loss: 0.0007074750028550625\n",
      "Step: 3683, Loss: 0.0010332106612622738\n",
      "Data loss: 0.00036113327951170504, Function loss: 0.0008350677671842277\n",
      "Step: 3684, Loss: 0.0011962010757997632\n",
      "Data loss: 0.00032521382672712207, Function loss: 0.0010870638070628047\n",
      "Step: 3685, Loss: 0.0014122775755822659\n",
      "Data loss: 0.00037788375630043447, Function loss: 0.0013189773308113217\n",
      "Step: 3686, Loss: 0.0016968611162155867\n",
      "Data loss: 0.000329460424836725, Function loss: 0.0016837459988892078\n",
      "Step: 3687, Loss: 0.0020132064819335938\n",
      "Data loss: 0.00039930996717885137, Function loss: 0.0020640490110963583\n",
      "Step: 3688, Loss: 0.0024633589200675488\n",
      "Data loss: 0.0003397161199245602, Function loss: 0.0026177612598985434\n",
      "Step: 3689, Loss: 0.002957477467134595\n",
      "Data loss: 0.00043222945532761514, Function loss: 0.0032879195641726255\n",
      "Step: 3690, Loss: 0.0037201489321887493\n",
      "Data loss: 0.00036090140929445624, Function loss: 0.004180950112640858\n",
      "Step: 3691, Loss: 0.004541851580142975\n",
      "Data loss: 0.0004828564706258476, Function loss: 0.005292258225381374\n",
      "Step: 3692, Loss: 0.005775114521384239\n",
      "Data loss: 0.000396586925489828, Function loss: 0.006390696857124567\n",
      "Step: 3693, Loss: 0.0067872838117182255\n",
      "Data loss: 0.0005424830014817417, Function loss: 0.007719721179455519\n",
      "Step: 3694, Loss: 0.008262204006314278\n",
      "Data loss: 0.0004338991129770875, Function loss: 0.008360594511032104\n",
      "Step: 3695, Loss: 0.008794493973255157\n",
      "Data loss: 0.0005682631162926555, Function loss: 0.008681626990437508\n",
      "Step: 3696, Loss: 0.009249890223145485\n",
      "Data loss: 0.000428781466325745, Function loss: 0.007376603316515684\n",
      "Step: 3697, Loss: 0.007805384695529938\n",
      "Data loss: 0.0004976042546331882, Function loss: 0.005470266565680504\n",
      "Step: 3698, Loss: 0.005967870820313692\n",
      "Data loss: 0.0003743185952771455, Function loss: 0.003010454820469022\n",
      "Step: 3699, Loss: 0.003384773386642337\n",
      "Data loss: 0.0003907618229277432, Function loss: 0.001123471069149673\n",
      "Step: 3700, Loss: 0.001514232950285077\n",
      "Data loss: 0.00036313492455519736, Function loss: 0.0004220175906084478\n",
      "Step: 3701, Loss: 0.0007851525442674756\n",
      "Data loss: 0.00036330646253190935, Function loss: 0.0009077329887077212\n",
      "Step: 3702, Loss: 0.001271039480343461\n",
      "Data loss: 0.0004295133694540709, Function loss: 0.0019803065806627274\n",
      "Step: 3703, Loss: 0.0024098199792206287\n",
      "Data loss: 0.000389125372748822, Function loss: 0.002871584612876177\n",
      "Step: 3704, Loss: 0.0032607100438326597\n",
      "Data loss: 0.0004620279651135206, Function loss: 0.0029618721455335617\n",
      "Step: 3705, Loss: 0.0034239001106470823\n",
      "Data loss: 0.00038494699401780963, Function loss: 0.0022446652874350548\n",
      "Step: 3706, Loss: 0.0026296123396605253\n",
      "Data loss: 0.000414540059864521, Function loss: 0.0011971001513302326\n",
      "Step: 3707, Loss: 0.0016116402111947536\n",
      "Data loss: 0.00037595597677864134, Function loss: 0.0005259561003185809\n",
      "Step: 3708, Loss: 0.0009019121062010527\n",
      "Data loss: 0.00037807549233548343, Function loss: 0.00047467686817981303\n",
      "Step: 3709, Loss: 0.0008527523605152965\n",
      "Data loss: 0.0004068401758559048, Function loss: 0.0008924785070121288\n",
      "Step: 3710, Loss: 0.0012993186246603727\n",
      "Data loss: 0.0003816788084805012, Function loss: 0.0014255258720368147\n",
      "Step: 3711, Loss: 0.0018072046805173159\n",
      "Data loss: 0.0004309807263780385, Function loss: 0.001614003791473806\n",
      "Step: 3712, Loss: 0.002044984605163336\n",
      "Data loss: 0.00038056328776292503, Function loss: 0.0014289015671238303\n",
      "Step: 3713, Loss: 0.0018094648839905858\n",
      "Data loss: 0.0004106767592020333, Function loss: 0.0009406290482729673\n",
      "Step: 3714, Loss: 0.0013513057492673397\n",
      "Data loss: 0.0003746171132661402, Function loss: 0.0005580586148425937\n",
      "Step: 3715, Loss: 0.0009326757281087339\n",
      "Data loss: 0.00038130642496980727, Function loss: 0.00041464652167633176\n",
      "Step: 3716, Loss: 0.0007959529757499695\n",
      "Data loss: 0.0003895866684615612, Function loss: 0.000565087073482573\n",
      "Step: 3717, Loss: 0.0009546737419441342\n",
      "Data loss: 0.0003696510102599859, Function loss: 0.0008752921130508184\n",
      "Step: 3718, Loss: 0.0012449431233108044\n",
      "Data loss: 0.00040858236025087535, Function loss: 0.0010450438130646944\n",
      "Step: 3719, Loss: 0.0014536262024194002\n",
      "Data loss: 0.00036374019691720605, Function loss: 0.0010501286014914513\n",
      "Step: 3720, Loss: 0.0014138687402009964\n",
      "Data loss: 0.00039997612475417554, Function loss: 0.0008016362553462386\n",
      "Step: 3721, Loss: 0.0012016124092042446\n",
      "Data loss: 0.00036076741525903344, Function loss: 0.0005775398458354175\n",
      "Step: 3722, Loss: 0.000938307261094451\n",
      "Data loss: 0.00037383974995464087, Function loss: 0.00041372206760570407\n",
      "Step: 3723, Loss: 0.0007875618175603449\n",
      "Data loss: 0.0003710828605107963, Function loss: 0.0004260962305124849\n",
      "Step: 3724, Loss: 0.0007971790619194508\n",
      "Data loss: 0.00035657588159665465, Function loss: 0.0005513798096217215\n",
      "Step: 3725, Loss: 0.0009079556912183762\n",
      "Data loss: 0.00038405958912335336, Function loss: 0.0006322933477349579\n",
      "Step: 3726, Loss: 0.0010163529077544808\n",
      "Data loss: 0.0003511664108373225, Function loss: 0.0006912430399097502\n",
      "Step: 3727, Loss: 0.0010424094507470727\n",
      "Data loss: 0.0003801624698098749, Function loss: 0.0006070900708436966\n",
      "Step: 3728, Loss: 0.000987252569757402\n",
      "Data loss: 0.0003518872836139053, Function loss: 0.0005322621436789632\n",
      "Step: 3729, Loss: 0.000884149456396699\n",
      "Data loss: 0.00036361615639179945, Function loss: 0.0004307318013161421\n",
      "Step: 3730, Loss: 0.0007943479577079415\n",
      "Data loss: 0.00035914580803364515, Function loss: 0.0004039984196424484\n",
      "Step: 3731, Loss: 0.0007631442276760936\n",
      "Data loss: 0.00034985924139618874, Function loss: 0.0004440690972842276\n",
      "Step: 3732, Loss: 0.0007939283386804163\n",
      "Data loss: 0.0003670603909995407, Function loss: 0.00048121256986632943\n",
      "Step: 3733, Loss: 0.0008482729317620397\n",
      "Data loss: 0.00034456566208973527, Function loss: 0.0005463010747916996\n",
      "Step: 3734, Loss: 0.0008908667368814349\n",
      "Data loss: 0.00036726848338730633, Function loss: 0.0005382141098380089\n",
      "Step: 3735, Loss: 0.0009054825641214848\n",
      "Data loss: 0.00034428955405019224, Function loss: 0.0005467472365126014\n",
      "Step: 3736, Loss: 0.0008910368196666241\n",
      "Data loss: 0.00036109003121964633, Function loss: 0.00049767637392506\n",
      "Step: 3737, Loss: 0.0008587663760408759\n",
      "Data loss: 0.00034565624082461, Function loss: 0.0004712723311968148\n",
      "Step: 3738, Loss: 0.0008169285720214248\n",
      "Data loss: 0.0003529473324306309, Function loss: 0.0004258662520442158\n",
      "Step: 3739, Loss: 0.0007788136135786772\n",
      "Data loss: 0.0003469386138021946, Function loss: 0.00040675271884538233\n",
      "Step: 3740, Loss: 0.0007536913035437465\n",
      "Data loss: 0.0003464484470896423, Function loss: 0.00040194892790168524\n",
      "Step: 3741, Loss: 0.0007483973749913275\n",
      "Data loss: 0.00034877119469456375, Function loss: 0.00041129725286737084\n",
      "Step: 3742, Loss: 0.0007600684184581041\n",
      "Data loss: 0.0003433619858697057, Function loss: 0.000438477291027084\n",
      "Step: 3743, Loss: 0.0007818392477929592\n",
      "Data loss: 0.0003492809773888439, Function loss: 0.0004460914060473442\n",
      "Step: 3744, Loss: 0.0007953724125400186\n",
      "Data loss: 0.00034330348717048764, Function loss: 0.0004520234651863575\n",
      "Step: 3745, Loss: 0.0007953269523568451\n",
      "Data loss: 0.0003461310116108507, Function loss: 0.00043400254799053073\n",
      "Step: 3746, Loss: 0.000780133530497551\n",
      "Data loss: 0.0003430635842960328, Function loss: 0.0004179352254141122\n",
      "Step: 3747, Loss: 0.000760998809710145\n",
      "Data loss: 0.0003410016652196646, Function loss: 0.0004071985022164881\n",
      "Step: 3748, Loss: 0.0007482001674361527\n",
      "Data loss: 0.00034342907019890845, Function loss: 0.0004015496524516493\n",
      "Step: 3749, Loss: 0.0007449787226505578\n",
      "Data loss: 0.0003376046079210937, Function loss: 0.0004113898321520537\n",
      "Step: 3750, Loss: 0.000748994410969317\n",
      "Data loss: 0.0003438910935074091, Function loss: 0.0004095689218956977\n",
      "Step: 3751, Loss: 0.0007534599862992764\n",
      "Data loss: 0.0003358207468409091, Function loss: 0.0004181408730801195\n",
      "Step: 3752, Loss: 0.0007539616199210286\n",
      "Data loss: 0.0003425871836952865, Function loss: 0.00040796937537379563\n",
      "Step: 3753, Loss: 0.0007505565881729126\n",
      "Data loss: 0.0003356756060384214, Function loss: 0.0004087531706318259\n",
      "Step: 3754, Loss: 0.0007444287766702473\n",
      "Data loss: 0.00033958288258872926, Function loss: 0.00040031556272879243\n",
      "Step: 3755, Loss: 0.0007398984162136912\n",
      "Data loss: 0.0003368663019500673, Function loss: 0.0004013452853541821\n",
      "Step: 3756, Loss: 0.000738211558200419\n",
      "Data loss: 0.00033661528141237795, Function loss: 0.0004018872568849474\n",
      "Step: 3757, Loss: 0.0007385025382973254\n",
      "Data loss: 0.00033736127079464495, Function loss: 0.0004020532069262117\n",
      "Step: 3758, Loss: 0.0007394144777208567\n",
      "Data loss: 0.000334867654601112, Function loss: 0.00040536152664572\n",
      "Step: 3759, Loss: 0.0007402291521430016\n",
      "Data loss: 0.0003368472680449486, Function loss: 0.00040424655890092254\n",
      "Step: 3760, Loss: 0.0007410938269458711\n",
      "Data loss: 0.00033386872382834554, Function loss: 0.0004063726228196174\n",
      "Step: 3761, Loss: 0.0007402413757517934\n",
      "Data loss: 0.00033637447631917894, Function loss: 0.0004034206212963909\n",
      "Step: 3762, Loss: 0.0007397950976155698\n",
      "Data loss: 0.0003326100704725832, Function loss: 0.000406840379582718\n",
      "Step: 3763, Loss: 0.0007394504500553012\n",
      "Data loss: 0.00033626038930378854, Function loss: 0.000404021906433627\n",
      "Step: 3764, Loss: 0.0007402822957374156\n",
      "Data loss: 0.00033096870174631476, Function loss: 0.0004114684124942869\n",
      "Step: 3765, Loss: 0.0007424370851367712\n",
      "Data loss: 0.0003368780598975718, Function loss: 0.0004090988077223301\n",
      "Step: 3766, Loss: 0.0007459768676199019\n",
      "Data loss: 0.0003292401961516589, Function loss: 0.0004216798988636583\n",
      "Step: 3767, Loss: 0.0007509200950153172\n",
      "Data loss: 0.0003380937851034105, Function loss: 0.00041685477481223643\n",
      "Step: 3768, Loss: 0.0007549485890194774\n",
      "Data loss: 0.00032769673271104693, Function loss: 0.0004317972925491631\n",
      "Step: 3769, Loss: 0.00075949402526021\n",
      "Data loss: 0.000339500664267689, Function loss: 0.0004247220349498093\n",
      "Step: 3770, Loss: 0.0007642226992174983\n",
      "Data loss: 0.0003269660519436002, Function loss: 0.0004415278963278979\n",
      "Step: 3771, Loss: 0.0007684939773753285\n",
      "Data loss: 0.00034069130197167397, Function loss: 0.0004330531810410321\n",
      "Step: 3772, Loss: 0.000773744483012706\n",
      "Data loss: 0.0003267201827839017, Function loss: 0.00045074126683175564\n",
      "Step: 3773, Loss: 0.0007774614496156573\n",
      "Data loss: 0.0003409106284379959, Function loss: 0.00043721438851207495\n",
      "Step: 3774, Loss: 0.0007781250169500709\n",
      "Data loss: 0.0003269372973591089, Function loss: 0.00044810547842644155\n",
      "Step: 3775, Loss: 0.0007750428048893809\n",
      "Data loss: 0.00033957045525312424, Function loss: 0.00043130075209774077\n",
      "Step: 3776, Loss: 0.0007708711782470345\n",
      "Data loss: 0.0003269663720857352, Function loss: 0.0004398316377773881\n",
      "Step: 3777, Loss: 0.0007667980389669538\n",
      "Data loss: 0.00033795193303376436, Function loss: 0.0004235806118231267\n",
      "Step: 3778, Loss: 0.0007615325739607215\n",
      "Data loss: 0.00032658473355695605, Function loss: 0.0004304868634790182\n",
      "Step: 3779, Loss: 0.0007570715970359743\n",
      "Data loss: 0.0003359440015628934, Function loss: 0.0004146669525653124\n",
      "Step: 3780, Loss: 0.0007506109541282058\n",
      "Data loss: 0.0003262630198150873, Function loss: 0.00041895502363331616\n",
      "Step: 3781, Loss: 0.000745218014344573\n",
      "Data loss: 0.0003336424706503749, Function loss: 0.00040519950562156737\n",
      "Step: 3782, Loss: 0.0007388419471681118\n",
      "Data loss: 0.00032646904583089054, Function loss: 0.0004083842213731259\n",
      "Step: 3783, Loss: 0.0007348532672040164\n",
      "Data loss: 0.0003323109995108098, Function loss: 0.000401660188799724\n",
      "Step: 3784, Loss: 0.0007339711883105338\n",
      "Data loss: 0.00032600288977846503, Function loss: 0.00041287022759206593\n",
      "Step: 3785, Loss: 0.0007388731464743614\n",
      "Data loss: 0.000333123083692044, Function loss: 0.00041803106432780623\n",
      "Step: 3786, Loss: 0.0007511541480198503\n",
      "Data loss: 0.00032424740493297577, Function loss: 0.0004482720687519759\n",
      "Step: 3787, Loss: 0.0007725195027887821\n",
      "Data loss: 0.00033643582719378173, Function loss: 0.0004674619995057583\n",
      "Step: 3788, Loss: 0.0008038978558033705\n",
      "Data loss: 0.0003225606051273644, Function loss: 0.0005295212031342089\n",
      "Step: 3789, Loss: 0.0008520818082615733\n",
      "Data loss: 0.00034250362659804523, Function loss: 0.000591976277064532\n",
      "Step: 3790, Loss: 0.0009344798745587468\n",
      "Data loss: 0.0003217578341718763, Function loss: 0.0007434136932715774\n",
      "Step: 3791, Loss: 0.0010651715565472841\n",
      "Data loss: 0.0003546791267581284, Function loss: 0.0009210996795445681\n",
      "Step: 3792, Loss: 0.0012757787480950356\n",
      "Data loss: 0.00032425421522930264, Function loss: 0.0012566481018438935\n",
      "Step: 3793, Loss: 0.001580902375280857\n",
      "Data loss: 0.00037809088826179504, Function loss: 0.0016783582977950573\n",
      "Step: 3794, Loss: 0.0020564491860568523\n",
      "Data loss: 0.00033581015304662287, Function loss: 0.0023748779203742743\n",
      "Step: 3795, Loss: 0.0027106881607323885\n",
      "Data loss: 0.0004250268393661827, Function loss: 0.0034206805285066366\n",
      "Step: 3796, Loss: 0.003845707280561328\n",
      "Data loss: 0.0003726266440935433, Function loss: 0.00501751434057951\n",
      "Step: 3797, Loss: 0.005390141159296036\n",
      "Data loss: 0.0005242805927991867, Function loss: 0.007501582615077496\n",
      "Step: 3798, Loss: 0.008025863207876682\n",
      "Data loss: 0.00046185048995539546, Function loss: 0.01045143697410822\n",
      "Step: 3799, Loss: 0.010913287289440632\n",
      "Data loss: 0.0006887911586090922, Function loss: 0.01462196372449398\n",
      "Step: 3800, Loss: 0.015310754999518394\n",
      "Data loss: 0.0005811821320094168, Function loss: 0.016723955050110817\n",
      "Step: 3801, Loss: 0.01730513758957386\n",
      "Data loss: 0.000776408240199089, Function loss: 0.0182002205401659\n",
      "Step: 3802, Loss: 0.01897662878036499\n",
      "Data loss: 0.0005647597718052566, Function loss: 0.013953162357211113\n",
      "Step: 3803, Loss: 0.014517921954393387\n",
      "Data loss: 0.0005807842826470733, Function loss: 0.008316532708704472\n",
      "Step: 3804, Loss: 0.00889731664210558\n",
      "Data loss: 0.0004065292014274746, Function loss: 0.0025484433863312006\n",
      "Step: 3805, Loss: 0.0029549726750701666\n",
      "Data loss: 0.0004159400414209813, Function loss: 0.0006680576479993761\n",
      "Step: 3806, Loss: 0.0010839976603165269\n",
      "Data loss: 0.0004648235044442117, Function loss: 0.0026514388155192137\n",
      "Step: 3807, Loss: 0.0031162623781710863\n",
      "Data loss: 0.0004882587236352265, Function loss: 0.005477475933730602\n",
      "Step: 3808, Loss: 0.0059657348319888115\n",
      "Data loss: 0.0005753247532993555, Function loss: 0.006327453535050154\n",
      "Step: 3809, Loss: 0.006902778521180153\n",
      "Data loss: 0.00046387064503505826, Function loss: 0.0038868531119078398\n",
      "Step: 3810, Loss: 0.004350723698735237\n",
      "Data loss: 0.00047156205982901156, Function loss: 0.0011169047793373466\n",
      "Step: 3811, Loss: 0.0015884668100625277\n",
      "Data loss: 0.0004507170233409852, Function loss: 0.0006122208433225751\n",
      "Step: 3812, Loss: 0.0010629378957673907\n",
      "Data loss: 0.00047074476606212556, Function loss: 0.0021566397044807673\n",
      "Step: 3813, Loss: 0.0026273843832314014\n",
      "Data loss: 0.0005389979341998696, Function loss: 0.0033961483277380466\n",
      "Step: 3814, Loss: 0.0039351461455225945\n",
      "Data loss: 0.0004746088234242052, Function loss: 0.0027016273234039545\n",
      "Step: 3815, Loss: 0.0031762360595166683\n",
      "Data loss: 0.0004862495406996459, Function loss: 0.001110891462303698\n",
      "Step: 3816, Loss: 0.0015971410321071744\n",
      "Data loss: 0.00045911152847111225, Function loss: 0.0003886717022396624\n",
      "Step: 3817, Loss: 0.0008477832307107747\n",
      "Data loss: 0.00046142327482812107, Function loss: 0.0010308106429874897\n",
      "Step: 3818, Loss: 0.0014922339469194412\n",
      "Data loss: 0.0005156234255991876, Function loss: 0.0018747263820841908\n",
      "Step: 3819, Loss: 0.0023903497494757175\n",
      "Data loss: 0.0004627735470421612, Function loss: 0.00175416748970747\n",
      "Step: 3820, Loss: 0.0022169409785419703\n",
      "Data loss: 0.00047779010492376983, Function loss: 0.0008611195953562856\n",
      "Step: 3821, Loss: 0.0013389097293838859\n",
      "Data loss: 0.0004493964370340109, Function loss: 0.00037900256575085223\n",
      "Step: 3822, Loss: 0.0008283989736810327\n",
      "Data loss: 0.0004384430358186364, Function loss: 0.0007131563615985215\n",
      "Step: 3823, Loss: 0.0011515994556248188\n",
      "Data loss: 0.0004833507409784943, Function loss: 0.001179815037176013\n",
      "Step: 3824, Loss: 0.0016631657490506768\n",
      "Data loss: 0.00042855989886447787, Function loss: 0.0011733741266652942\n",
      "Step: 3825, Loss: 0.0016019339673221111\n",
      "Data loss: 0.0004540607624221593, Function loss: 0.0006600464112125337\n",
      "Step: 3826, Loss: 0.0011141072027385235\n",
      "Data loss: 0.00042394688352942467, Function loss: 0.00037346454337239265\n",
      "Step: 3827, Loss: 0.0007974114269018173\n",
      "Data loss: 0.0004100209043826908, Function loss: 0.0005471084150485694\n",
      "Step: 3828, Loss: 0.0009571292903274298\n",
      "Data loss: 0.0004482600779738277, Function loss: 0.0008091573836281896\n",
      "Step: 3829, Loss: 0.0012574174907058477\n",
      "Data loss: 0.00039602897595614195, Function loss: 0.0008552894578315318\n",
      "Step: 3830, Loss: 0.0012513184919953346\n",
      "Data loss: 0.00042910646880045533, Function loss: 0.0005606514168903232\n",
      "Step: 3831, Loss: 0.0009897579438984394\n",
      "Data loss: 0.0003983592032454908, Function loss: 0.0003915269044227898\n",
      "Step: 3832, Loss: 0.0007898861076682806\n",
      "Data loss: 0.0003931204555556178, Function loss: 0.0004469489213079214\n",
      "Step: 3833, Loss: 0.0008400693768635392\n",
      "Data loss: 0.0004179337702225894, Function loss: 0.0005932413041591644\n",
      "Step: 3834, Loss: 0.0010111751034855843\n",
      "Data loss: 0.00037725167931057513, Function loss: 0.0006900555454194546\n",
      "Step: 3835, Loss: 0.0010673071956261992\n",
      "Data loss: 0.00041256481199525297, Function loss: 0.000554928439669311\n",
      "Step: 3836, Loss: 0.0009674932807683945\n",
      "Data loss: 0.0003769940522033721, Function loss: 0.00045313488226383924\n",
      "Step: 3837, Loss: 0.0008301289053633809\n",
      "Data loss: 0.00038808590034022927, Function loss: 0.00040890637319535017\n",
      "Step: 3838, Loss: 0.0007969922735355794\n",
      "Data loss: 0.00038941341335885227, Function loss: 0.000478823872981593\n",
      "Step: 3839, Loss: 0.0008682372863404453\n",
      "Data loss: 0.0003714166523423046, Function loss: 0.0005694903084076941\n",
      "Step: 3840, Loss: 0.0009409069316461682\n",
      "Data loss: 0.0003915636916644871, Function loss: 0.0005456919898279011\n",
      "Step: 3841, Loss: 0.0009372556814923882\n",
      "Data loss: 0.0003642850206233561, Function loss: 0.0004881980421487242\n",
      "Step: 3842, Loss: 0.0008524830918759108\n",
      "Data loss: 0.00037600676296278834, Function loss: 0.000392006040783599\n",
      "Step: 3843, Loss: 0.0007680128328502178\n",
      "Data loss: 0.00036715183523483574, Function loss: 0.0003688374417833984\n",
      "Step: 3844, Loss: 0.0007359893061220646\n",
      "Data loss: 0.0003607527178246528, Function loss: 0.00040324614383280277\n",
      "Step: 3845, Loss: 0.0007639988325536251\n",
      "Data loss: 0.0003735668142326176, Function loss: 0.0004400844918563962\n",
      "Step: 3846, Loss: 0.0008136513060890138\n",
      "Data loss: 0.0003547533415257931, Function loss: 0.0004806759243365377\n",
      "Step: 3847, Loss: 0.0008354292949661613\n",
      "Data loss: 0.00036941198050044477, Function loss: 0.0004492590669542551\n",
      "Step: 3848, Loss: 0.0008186710765585303\n",
      "Data loss: 0.00035588303580880165, Function loss: 0.00042395596392452717\n",
      "Step: 3849, Loss: 0.0007798389997333288\n",
      "Data loss: 0.00035765429493039846, Function loss: 0.00038974807830527425\n",
      "Step: 3850, Loss: 0.0007474023732356727\n",
      "Data loss: 0.0003596915630623698, Function loss: 0.00037836862611584365\n",
      "Step: 3851, Loss: 0.0007380602182820439\n",
      "Data loss: 0.00034762927680276334, Function loss: 0.0003988233220297843\n",
      "Step: 3852, Loss: 0.0007464525988325477\n",
      "Data loss: 0.00036270622513256967, Function loss: 0.0003976027946919203\n",
      "Step: 3853, Loss: 0.0007603090489283204\n",
      "Data loss: 0.00034281861735507846, Function loss: 0.00041881686775013804\n",
      "Step: 3854, Loss: 0.0007616354851052165\n",
      "Data loss: 0.0003583758953027427, Function loss: 0.0003928947262465954\n",
      "Step: 3855, Loss: 0.0007512706215493381\n",
      "Data loss: 0.00034326163586229086, Function loss: 0.0003885187325067818\n",
      "Step: 3856, Loss: 0.0007317803683690727\n",
      "Data loss: 0.00034815818071365356, Function loss: 0.0003692144528031349\n",
      "Step: 3857, Loss: 0.0007173726335167885\n",
      "Data loss: 0.0003483006439637393, Function loss: 0.00036935543175786734\n",
      "Step: 3858, Loss: 0.0007176561048254371\n",
      "Data loss: 0.0003398866392672062, Function loss: 0.0003900153678841889\n",
      "Step: 3859, Loss: 0.0007299020071513951\n",
      "Data loss: 0.0003527298104017973, Function loss: 0.000391218374716118\n",
      "Step: 3860, Loss: 0.0007439481560140848\n",
      "Data loss: 0.0003359132388141006, Function loss: 0.00041463138768449426\n",
      "Step: 3861, Loss: 0.0007505445973947644\n",
      "Data loss: 0.00035140503314323723, Function loss: 0.0003944833006244153\n",
      "Step: 3862, Loss: 0.0007458883337676525\n",
      "Data loss: 0.000335845397785306, Function loss: 0.0003967004595324397\n",
      "Step: 3863, Loss: 0.0007325458573177457\n",
      "Data loss: 0.00034471473190933466, Function loss: 0.0003753212804440409\n",
      "Step: 3864, Loss: 0.0007200359832495451\n",
      "Data loss: 0.00033813496702350676, Function loss: 0.0003752225311473012\n",
      "Step: 3865, Loss: 0.0007133574690669775\n",
      "Data loss: 0.00033759535290300846, Function loss: 0.00037709414027631283\n",
      "Step: 3866, Loss: 0.0007146894931793213\n",
      "Data loss: 0.00034103638608939946, Function loss: 0.0003794888616539538\n",
      "Step: 3867, Loss: 0.0007205252768471837\n",
      "Data loss: 0.0003326884761918336, Function loss: 0.00039342997479252517\n",
      "Step: 3868, Loss: 0.0007261184509843588\n",
      "Data loss: 0.0003426283074077219, Function loss: 0.00038902860251255333\n",
      "Step: 3869, Loss: 0.0007316569099202752\n",
      "Data loss: 0.0003300835087429732, Function loss: 0.0004054569872096181\n",
      "Step: 3870, Loss: 0.0007355405250564218\n",
      "Data loss: 0.0003438962739892304, Function loss: 0.0003958121524192393\n",
      "Step: 3871, Loss: 0.0007397084264084697\n",
      "Data loss: 0.0003293676709290594, Function loss: 0.00041477856575511396\n",
      "Step: 3872, Loss: 0.0007441462366841733\n",
      "Data loss: 0.0003439896390773356, Function loss: 0.00040341372368857265\n",
      "Step: 3873, Loss: 0.0007474033627659082\n",
      "Data loss: 0.00032974075293168426, Function loss: 0.00042391131864860654\n",
      "Step: 3874, Loss: 0.0007536520715802908\n",
      "Data loss: 0.0003436253755353391, Function loss: 0.00041382634663023055\n",
      "Step: 3875, Loss: 0.0007574517512694001\n",
      "Data loss: 0.00032973152701742947, Function loss: 0.00042688415851444006\n",
      "Step: 3876, Loss: 0.0007566156564280391\n",
      "Data loss: 0.00034145606332458556, Function loss: 0.00040877333958633244\n",
      "Step: 3877, Loss: 0.000750229402910918\n",
      "Data loss: 0.00032931086025200784, Function loss: 0.000410882115829736\n",
      "Step: 3878, Loss: 0.0007401929469779134\n",
      "Data loss: 0.0003368279431015253, Function loss: 0.00039326786645688117\n",
      "Step: 3879, Loss: 0.000730095780454576\n",
      "Data loss: 0.00032951636239886284, Function loss: 0.00039168447256088257\n",
      "Step: 3880, Loss: 0.0007212008349597454\n",
      "Data loss: 0.0003319827083032578, Function loss: 0.0003806742315646261\n",
      "Step: 3881, Loss: 0.0007126569398678839\n",
      "Data loss: 0.00033031904604285955, Function loss: 0.00037709635216742754\n",
      "Step: 3882, Loss: 0.0007074153982102871\n",
      "Data loss: 0.0003281952522229403, Function loss: 0.00037711841287091374\n",
      "Step: 3883, Loss: 0.0007053136359900236\n",
      "Data loss: 0.0003315517387818545, Function loss: 0.00037379865534603596\n",
      "Step: 3884, Loss: 0.0007053504232317209\n",
      "Data loss: 0.0003260962257627398, Function loss: 0.000379873818019405\n",
      "Step: 3885, Loss: 0.0007059700437821448\n",
      "Data loss: 0.00033251027343794703, Function loss: 0.0003763252461794764\n",
      "Step: 3886, Loss: 0.0007088355487212539\n",
      "Data loss: 0.0003256476193200797, Function loss: 0.0003886942286044359\n",
      "Step: 3887, Loss: 0.0007143418770283461\n",
      "Data loss: 0.00033365318085998297, Function loss: 0.00038933497853577137\n",
      "Step: 3888, Loss: 0.0007229881593957543\n",
      "Data loss: 0.0003260100493207574, Function loss: 0.0004069299320690334\n",
      "Step: 3889, Loss: 0.0007329399813897908\n",
      "Data loss: 0.00033581300522200763, Function loss: 0.0004119361692573875\n",
      "Step: 3890, Loss: 0.0007477491744793952\n",
      "Data loss: 0.0003268480650149286, Function loss: 0.00044311326928436756\n",
      "Step: 3891, Loss: 0.0007699613342992961\n",
      "Data loss: 0.00034071854315698147, Function loss: 0.0004597285878844559\n",
      "Step: 3892, Loss: 0.0008004471310414374\n",
      "Data loss: 0.0003298221272416413, Function loss: 0.0005120497080497444\n",
      "Step: 3893, Loss: 0.0008418718352913857\n",
      "Data loss: 0.000349855370586738, Function loss: 0.0005513890646398067\n",
      "Step: 3894, Loss: 0.0009012444643303752\n",
      "Data loss: 0.0003373912477400154, Function loss: 0.0006478476570919156\n",
      "Step: 3895, Loss: 0.0009852389339357615\n",
      "Data loss: 0.00036662747152149677, Function loss: 0.000733164488337934\n",
      "Step: 3896, Loss: 0.0010997919598594308\n",
      "Data loss: 0.0003522028273437172, Function loss: 0.0008926840964704752\n",
      "Step: 3897, Loss: 0.001244886894710362\n",
      "Data loss: 0.00039414034108631313, Function loss: 0.0010302113369107246\n",
      "Step: 3898, Loss: 0.0014243517071008682\n",
      "Data loss: 0.00037586918915621936, Function loss: 0.001265192055143416\n",
      "Step: 3899, Loss: 0.0016410612734034657\n",
      "Data loss: 0.0004338615108281374, Function loss: 0.0014628585195168853\n",
      "Step: 3900, Loss: 0.0018967200303450227\n",
      "Data loss: 0.00041219493141397834, Function loss: 0.0017996369861066341\n",
      "Step: 3901, Loss: 0.0022118319757282734\n",
      "Data loss: 0.0004885211819782853, Function loss: 0.0020927393343299627\n",
      "Step: 3902, Loss: 0.00258126063272357\n",
      "Data loss: 0.00046368935727514327, Function loss: 0.002522312570363283\n",
      "Step: 3903, Loss: 0.002986002014949918\n",
      "Data loss: 0.0005512359202839434, Function loss: 0.002833497477695346\n",
      "Step: 3904, Loss: 0.0033847333397716284\n",
      "Data loss: 0.0005144542665220797, Function loss: 0.0032054658513516188\n",
      "Step: 3905, Loss: 0.0037199200596660376\n",
      "Data loss: 0.0005902561242692173, Function loss: 0.0033108920324593782\n",
      "Step: 3906, Loss: 0.0039011482149362564\n",
      "Data loss: 0.0005283156060613692, Function loss: 0.0033384973648935556\n",
      "Step: 3907, Loss: 0.003866812912747264\n",
      "Data loss: 0.0005588579224422574, Function loss: 0.0029830557759851217\n",
      "Step: 3908, Loss: 0.003541913814842701\n",
      "Data loss: 0.0004705317260231823, Function loss: 0.002481125295162201\n",
      "Step: 3909, Loss: 0.002951656933873892\n",
      "Data loss: 0.00044687979971058667, Function loss: 0.0017756803426891565\n",
      "Step: 3910, Loss: 0.0022225601132959127\n",
      "Data loss: 0.00038012381992302835, Function loss: 0.0012085862690582871\n",
      "Step: 3911, Loss: 0.001588710118085146\n",
      "Data loss: 0.00034560231142677367, Function loss: 0.0008507522288709879\n",
      "Step: 3912, Loss: 0.001196354511193931\n",
      "Data loss: 0.0003505150380078703, Function loss: 0.0007787042995914817\n",
      "Step: 3913, Loss: 0.0011292193084955215\n",
      "Data loss: 0.00033100013388320804, Function loss: 0.0009940069867298007\n",
      "Step: 3914, Loss: 0.0013250070624053478\n",
      "Data loss: 0.00039023710996843874, Function loss: 0.0012466724729165435\n",
      "Step: 3915, Loss: 0.0016369095537811518\n",
      "Data loss: 0.0003618626797106117, Function loss: 0.0014920856337994337\n",
      "Step: 3916, Loss: 0.0018539483426138759\n",
      "Data loss: 0.000414817564887926, Function loss: 0.0014562919968739152\n",
      "Step: 3917, Loss: 0.0018711095908656716\n",
      "Data loss: 0.0003600314084906131, Function loss: 0.001283023739233613\n",
      "Step: 3918, Loss: 0.0016430551186203957\n",
      "Data loss: 0.0003793103387579322, Function loss: 0.0009012222290039062\n",
      "Step: 3919, Loss: 0.0012805325677618384\n",
      "Data loss: 0.00033340707886964083, Function loss: 0.0006050870288163424\n",
      "Step: 3920, Loss: 0.0009384941076859832\n",
      "Data loss: 0.0003398548869881779, Function loss: 0.00040025656926445663\n",
      "Step: 3921, Loss: 0.0007401114562526345\n",
      "Data loss: 0.00033663652720861137, Function loss: 0.000381579709937796\n",
      "Step: 3922, Loss: 0.0007182162371464074\n",
      "Data loss: 0.00033706429530866444, Function loss: 0.0004957378259859979\n",
      "Step: 3923, Loss: 0.0008328021503984928\n",
      "Data loss: 0.0003644756507128477, Function loss: 0.0006250852020457387\n",
      "Step: 3924, Loss: 0.0009895608527585864\n",
      "Data loss: 0.00034601264633238316, Function loss: 0.0007467881077900529\n",
      "Step: 3925, Loss: 0.001092800754122436\n",
      "Data loss: 0.0003721074026543647, Function loss: 0.0007192874909378588\n",
      "Step: 3926, Loss: 0.001091394922696054\n",
      "Data loss: 0.00033825988066382706, Function loss: 0.0006564480718225241\n",
      "Step: 3927, Loss: 0.0009947079233825207\n",
      "Data loss: 0.00035220259451307356, Function loss: 0.0005038816598244011\n",
      "Step: 3928, Loss: 0.0008560842834413052\n",
      "Data loss: 0.00032909843139350414, Function loss: 0.00041121046524494886\n",
      "Step: 3929, Loss: 0.000740308896638453\n",
      "Data loss: 0.00033381590037606657, Function loss: 0.00035892214509658515\n",
      "Step: 3930, Loss: 0.0006927380454726517\n",
      "Data loss: 0.0003375308006070554, Function loss: 0.0003814824449364096\n",
      "Step: 3931, Loss: 0.0007190132746472955\n",
      "Data loss: 0.00033191865077242255, Function loss: 0.0004560514644253999\n",
      "Step: 3932, Loss: 0.0007879701443016529\n",
      "Data loss: 0.00035166862653568387, Function loss: 0.0004974532639607787\n",
      "Step: 3933, Loss: 0.0008491218904964626\n",
      "Data loss: 0.00033220325713045895, Function loss: 0.0005327080143615603\n",
      "Step: 3934, Loss: 0.0008649112423881888\n",
      "Data loss: 0.00034932169364765286, Function loss: 0.00048089888878166676\n",
      "Step: 3935, Loss: 0.0008302205824293196\n",
      "Data loss: 0.0003274143091402948, Function loss: 0.000444158649770543\n",
      "Step: 3936, Loss: 0.0007715729298070073\n",
      "Data loss: 0.00033709881245158613, Function loss: 0.0003840083081740886\n",
      "Step: 3937, Loss: 0.0007211071206256747\n",
      "Data loss: 0.0003274734481237829, Function loss: 0.0003713024780154228\n",
      "Step: 3938, Loss: 0.0006987759261392057\n",
      "Data loss: 0.00033096695551648736, Function loss: 0.0003741526452358812\n",
      "Step: 3939, Loss: 0.0007051195716485381\n",
      "Data loss: 0.0003345056320540607, Function loss: 0.00039669391117058694\n",
      "Step: 3940, Loss: 0.0007311995141208172\n",
      "Data loss: 0.00033130173687823117, Function loss: 0.0004321585001889616\n",
      "Step: 3941, Loss: 0.0007634602370671928\n",
      "Data loss: 0.0003410634817555547, Function loss: 0.00044948112918064\n",
      "Step: 3942, Loss: 0.0007905446109361947\n",
      "Data loss: 0.0003324143181089312, Function loss: 0.0004676647949963808\n",
      "Step: 3943, Loss: 0.0008000790840014815\n",
      "Data loss: 0.0003401657158974558, Function loss: 0.00045080919517204165\n",
      "Step: 3944, Loss: 0.0007909749401733279\n",
      "Data loss: 0.0003309449239168316, Function loss: 0.00043691962491720915\n",
      "Step: 3945, Loss: 0.0007678645197302103\n",
      "Data loss: 0.0003330097533762455, Function loss: 0.00040790840284898877\n",
      "Step: 3946, Loss: 0.0007409181562252343\n",
      "Data loss: 0.000329922535456717, Function loss: 0.00039080483838915825\n",
      "Step: 3947, Loss: 0.0007207273738458753\n",
      "Data loss: 0.00032588402973487973, Function loss: 0.00038507833960466087\n",
      "Step: 3948, Loss: 0.0007109623402357101\n",
      "Data loss: 0.00033175304997712374, Function loss: 0.0003810125926975161\n",
      "Step: 3949, Loss: 0.0007127656135708094\n",
      "Data loss: 0.00032216685940511525, Function loss: 0.00040039222221821547\n",
      "Step: 3950, Loss: 0.0007225590525195003\n",
      "Data loss: 0.0003352041239850223, Function loss: 0.00040244837873615324\n",
      "Step: 3951, Loss: 0.000737652531825006\n",
      "Data loss: 0.00032086402643471956, Function loss: 0.00043559487676247954\n",
      "Step: 3952, Loss: 0.0007564589031971991\n",
      "Data loss: 0.00033823840203695, Function loss: 0.0004368566151242703\n",
      "Step: 3953, Loss: 0.0007750950171612203\n",
      "Data loss: 0.0003197730693500489, Function loss: 0.00047528231516480446\n",
      "Step: 3954, Loss: 0.0007950554136186838\n",
      "Data loss: 0.00034012997639365494, Function loss: 0.00048142115701921284\n",
      "Step: 3955, Loss: 0.0008215511334128678\n",
      "Data loss: 0.0003184714005328715, Function loss: 0.000543024973012507\n",
      "Step: 3956, Loss: 0.0008614963735453784\n",
      "Data loss: 0.00034309609327465296, Function loss: 0.0005793731543235481\n",
      "Step: 3957, Loss: 0.000922469247598201\n",
      "Data loss: 0.00031813926761969924, Function loss: 0.0006788352620787919\n",
      "Step: 3958, Loss: 0.000996974529698491\n",
      "Data loss: 0.0003474255499895662, Function loss: 0.0007511908770538867\n",
      "Step: 3959, Loss: 0.0010986163979396224\n",
      "Data loss: 0.000320685125188902, Function loss: 0.0009133569546975195\n",
      "Step: 3960, Loss: 0.001234042108990252\n",
      "Data loss: 0.0003567300154827535, Function loss: 0.0010919712949544191\n",
      "Step: 3961, Loss: 0.0014487013686448336\n",
      "Data loss: 0.00033151934621855617, Function loss: 0.0014011271996423602\n",
      "Step: 3962, Loss: 0.0017326464876532555\n",
      "Data loss: 0.00037879543378949165, Function loss: 0.0017929058521986008\n",
      "Step: 3963, Loss: 0.0021717012859880924\n",
      "Data loss: 0.0003585924278013408, Function loss: 0.002373512601479888\n",
      "Step: 3964, Loss: 0.0027321050874888897\n",
      "Data loss: 0.0004269440833013505, Function loss: 0.003213532269001007\n",
      "Step: 3965, Loss: 0.003640476381406188\n",
      "Data loss: 0.00041453607263974845, Function loss: 0.004384241532534361\n",
      "Step: 3966, Loss: 0.004798777401447296\n",
      "Data loss: 0.0005232221446931362, Function loss: 0.006095877382904291\n",
      "Step: 3967, Loss: 0.006619099527597427\n",
      "Data loss: 0.0005097315879538655, Function loss: 0.007898825220763683\n",
      "Step: 3968, Loss: 0.008408556692302227\n",
      "Data loss: 0.0006562077323906124, Function loss: 0.010214255191385746\n",
      "Step: 3969, Loss: 0.010870463214814663\n",
      "Data loss: 0.0005995639367029071, Function loss: 0.011423164047300816\n",
      "Step: 3970, Loss: 0.012022728100419044\n",
      "Data loss: 0.0007259640260599554, Function loss: 0.012284922413527966\n",
      "Step: 3971, Loss: 0.013010886497795582\n",
      "Data loss: 0.0005648979567922652, Function loss: 0.010381526313722134\n",
      "Step: 3972, Loss: 0.010946424677968025\n",
      "Data loss: 0.0005904218996874988, Function loss: 0.007454041391611099\n",
      "Step: 3973, Loss: 0.008044463582336903\n",
      "Data loss: 0.00040457560680806637, Function loss: 0.003530331887304783\n",
      "Step: 3974, Loss: 0.003934907726943493\n",
      "Data loss: 0.00038961926475167274, Function loss: 0.0008414273615926504\n",
      "Step: 3975, Loss: 0.0012310466263443232\n",
      "Data loss: 0.0003864537284243852, Function loss: 0.0004993475158698857\n",
      "Step: 3976, Loss: 0.0008858012733981013\n",
      "Data loss: 0.00039706085226498544, Function loss: 0.0020992790814489126\n",
      "Step: 3977, Loss: 0.0024963398464024067\n",
      "Data loss: 0.0005288712563924491, Function loss: 0.00398042518645525\n",
      "Step: 3978, Loss: 0.004509296268224716\n",
      "Data loss: 0.0004519247158896178, Function loss: 0.004489543382078409\n",
      "Step: 3979, Loss: 0.0049414681270718575\n",
      "Data loss: 0.0005070050247013569, Function loss: 0.0032482582610100508\n",
      "Step: 3980, Loss: 0.0037552632857114077\n",
      "Data loss: 0.0003961149777751416, Function loss: 0.0013249554904177785\n",
      "Step: 3981, Loss: 0.0017210704972967505\n",
      "Data loss: 0.00039722511428408325, Function loss: 0.00034880696330219507\n",
      "Step: 3982, Loss: 0.0007460321066901088\n",
      "Data loss: 0.00042843518895097077, Function loss: 0.0008603818714618683\n",
      "Step: 3983, Loss: 0.0012888170313090086\n",
      "Data loss: 0.0004198577953502536, Function loss: 0.0020095454528927803\n",
      "Step: 3984, Loss: 0.0024294033646583557\n",
      "Data loss: 0.0004889110568910837, Function loss: 0.002492670202627778\n",
      "Step: 3985, Loss: 0.0029815812595188618\n",
      "Data loss: 0.00042870838660746813, Function loss: 0.001912325737066567\n",
      "Step: 3986, Loss: 0.002341034123674035\n",
      "Data loss: 0.0004300552827771753, Function loss: 0.0008931417833082378\n",
      "Step: 3987, Loss: 0.0013231970369815826\n",
      "Data loss: 0.00040448407526127994, Function loss: 0.00035921449307352304\n",
      "Step: 3988, Loss: 0.0007636985974386334\n",
      "Data loss: 0.0003974490682594478, Function loss: 0.000581819680519402\n",
      "Step: 3989, Loss: 0.000979268690571189\n",
      "Data loss: 0.0004306102928239852, Function loss: 0.0011223630281165242\n",
      "Step: 3990, Loss: 0.001552973291836679\n",
      "Data loss: 0.00041368106030859053, Function loss: 0.0014125194866210222\n",
      "Step: 3991, Loss: 0.0018262005178257823\n",
      "Data loss: 0.0004304174508433789, Function loss: 0.0011827423004433513\n",
      "Step: 3992, Loss: 0.0016131597803905606\n",
      "Data loss: 0.0004064584500156343, Function loss: 0.0007200957625173032\n",
      "Step: 3993, Loss: 0.0011265542125329375\n",
      "Data loss: 0.00039057584945112467, Function loss: 0.0004196408553980291\n",
      "Step: 3994, Loss: 0.0008102167048491538\n",
      "Data loss: 0.0004010302072856575, Function loss: 0.00044496977352537215\n",
      "Step: 3995, Loss: 0.0008459999808110297\n",
      "Data loss: 0.0003764837747439742, Function loss: 0.0007299003773368895\n",
      "Step: 3996, Loss: 0.0011063842102885246\n",
      "Data loss: 0.00040918163722380996, Function loss: 0.0009068314684554935\n",
      "Step: 3997, Loss: 0.0013160130474716425\n",
      "Data loss: 0.0003813289513345808, Function loss: 0.0008936814847402275\n",
      "Step: 3998, Loss: 0.0012750104069709778\n",
      "Data loss: 0.0003932691761292517, Function loss: 0.0006627994007430971\n",
      "Step: 3999, Loss: 0.0010560685768723488\n",
      "Data loss: 0.0003801357524935156, Function loss: 0.0004395137366373092\n",
      "Step: 4000, Loss: 0.0008196494891308248\n",
      "Data loss: 0.0003658848872873932, Function loss: 0.00038310536183416843\n",
      "Step: 4001, Loss: 0.0007489902200177312\n",
      "Data loss: 0.0003862845478579402, Function loss: 0.00047117049689404666\n",
      "Step: 4002, Loss: 0.0008574550738558173\n",
      "Data loss: 0.0003561608318705112, Function loss: 0.000654116680379957\n",
      "Step: 4003, Loss: 0.0010102775413542986\n",
      "Data loss: 0.0003897928982041776, Function loss: 0.0007005984080024064\n",
      "Step: 4004, Loss: 0.001090391306206584\n",
      "Data loss: 0.00035485762055031955, Function loss: 0.0006763619603589177\n",
      "Step: 4005, Loss: 0.0010312196100130677\n",
      "Data loss: 0.0003756945370696485, Function loss: 0.0005166174960322678\n",
      "Step: 4006, Loss: 0.0008923120331019163\n",
      "Data loss: 0.0003557750897016376, Function loss: 0.00039638756425119936\n",
      "Step: 4007, Loss: 0.000752162653952837\n",
      "Data loss: 0.00035571298212744296, Function loss: 0.0003358605317771435\n",
      "Step: 4008, Loss: 0.000691573484800756\n",
      "Data loss: 0.0003639552742242813, Function loss: 0.0003658072091639042\n",
      "Step: 4009, Loss: 0.0007297624833881855\n",
      "Data loss: 0.0003451158117968589, Function loss: 0.0004643778083845973\n",
      "Step: 4010, Loss: 0.0008094935910776258\n",
      "Data loss: 0.0003690218727570027, Function loss: 0.0005036885268054903\n",
      "Step: 4011, Loss: 0.0008727103704586625\n",
      "Data loss: 0.00034214361221529543, Function loss: 0.0005310747073963284\n",
      "Step: 4012, Loss: 0.0008732182905077934\n",
      "Data loss: 0.00036219359026290476, Function loss: 0.0004627850721590221\n",
      "Step: 4013, Loss: 0.0008249786915257573\n",
      "Data loss: 0.0003432014200370759, Function loss: 0.00041013944428414106\n",
      "Step: 4014, Loss: 0.0007533408934250474\n",
      "Data loss: 0.0003496398567222059, Function loss: 0.0003484071930870414\n",
      "Step: 4015, Loss: 0.0006980470498092473\n",
      "Data loss: 0.00034686780418269336, Function loss: 0.00032954919151961803\n",
      "Step: 4016, Loss: 0.0006764170248061419\n",
      "Data loss: 0.00034023201442323625, Function loss: 0.0003457161656115204\n",
      "Step: 4017, Loss: 0.0006859481800347567\n",
      "Data loss: 0.0003496594144962728, Function loss: 0.0003627288097050041\n",
      "Step: 4018, Loss: 0.0007123881950974464\n",
      "Data loss: 0.0003360056725796312, Function loss: 0.0004010888806078583\n",
      "Step: 4019, Loss: 0.0007370945531874895\n",
      "Data loss: 0.00034950897679664195, Function loss: 0.0004014741280116141\n",
      "Step: 4020, Loss: 0.0007509831339120865\n",
      "Data loss: 0.000334142183419317, Function loss: 0.0004147622094023973\n",
      "Step: 4021, Loss: 0.0007489044219255447\n",
      "Data loss: 0.00034656416391953826, Function loss: 0.000386067054932937\n",
      "Step: 4022, Loss: 0.0007326311897486448\n",
      "Data loss: 0.0003327595768496394, Function loss: 0.00037408401840366423\n",
      "Step: 4023, Loss: 0.0007068435661494732\n",
      "Data loss: 0.00034138697083108127, Function loss: 0.00034213208709843457\n",
      "Step: 4024, Loss: 0.0006835190579295158\n",
      "Data loss: 0.00033318871282972395, Function loss: 0.00033678836189210415\n",
      "Step: 4025, Loss: 0.0006699770456179976\n",
      "Data loss: 0.0003367304161656648, Function loss: 0.00033150266972370446\n",
      "Step: 4026, Loss: 0.0006682330858893692\n",
      "Data loss: 0.0003350837796460837, Function loss: 0.00033838360104709864\n",
      "Step: 4027, Loss: 0.0006734674097970128\n",
      "Data loss: 0.00033217138843610883, Function loss: 0.00034833725658245385\n",
      "Step: 4028, Loss: 0.0006805086741223931\n",
      "Data loss: 0.00033686813549138606, Function loss: 0.0003498970763757825\n",
      "Step: 4029, Loss: 0.0006867651827633381\n",
      "Data loss: 0.00032806117087602615, Function loss: 0.00036229408578947186\n",
      "Step: 4030, Loss: 0.000690355256665498\n",
      "Data loss: 0.00033777349744923413, Function loss: 0.00035450441646389663\n",
      "Step: 4031, Loss: 0.0006922779139131308\n",
      "Data loss: 0.00032583135180175304, Function loss: 0.00036304688546806574\n",
      "Step: 4032, Loss: 0.0006888782372698188\n",
      "Data loss: 0.00033755734330043197, Function loss: 0.00034860282903537154\n",
      "Step: 4033, Loss: 0.0006861601723358035\n",
      "Data loss: 0.00032551566255278885, Function loss: 0.0003594301815610379\n",
      "Step: 4034, Loss: 0.0006849458441138268\n",
      "Data loss: 0.00033610357786528766, Function loss: 0.0003475380945019424\n",
      "Step: 4035, Loss: 0.0006836416432633996\n",
      "Data loss: 0.0003261408128309995, Function loss: 0.00035572709748521447\n",
      "Step: 4036, Loss: 0.0006818679394200444\n",
      "Data loss: 0.0003333937784191221, Function loss: 0.00034806178882718086\n",
      "Step: 4037, Loss: 0.0006814555963501334\n",
      "Data loss: 0.0003267042338848114, Function loss: 0.00035432574804872274\n",
      "Step: 4038, Loss: 0.0006810299819335341\n",
      "Data loss: 0.000330726063111797, Function loss: 0.000350970309227705\n",
      "Step: 4039, Loss: 0.0006816963432356715\n",
      "Data loss: 0.00032711209496483207, Function loss: 0.00035490209120325744\n",
      "Step: 4040, Loss: 0.000682014157064259\n",
      "Data loss: 0.0003285175480414182, Function loss: 0.0003550438559614122\n",
      "Step: 4041, Loss: 0.0006835614331066608\n",
      "Data loss: 0.00032722254400141537, Function loss: 0.00035799952456727624\n",
      "Step: 4042, Loss: 0.0006852220976725221\n",
      "Data loss: 0.0003264344413764775, Function loss: 0.00036109902430325747\n",
      "Step: 4043, Loss: 0.0006875334656797349\n",
      "Data loss: 0.0003278629737906158, Function loss: 0.00036666711093857884\n",
      "Step: 4044, Loss: 0.0006945300847291946\n",
      "Data loss: 0.0003243141691200435, Function loss: 0.00037842124584130943\n",
      "Step: 4045, Loss: 0.0007027353858575225\n",
      "Data loss: 0.0003287469735369086, Function loss: 0.00037981095374561846\n",
      "Step: 4046, Loss: 0.0007085578981786966\n",
      "Data loss: 0.00032220507273450494, Function loss: 0.0003883002500515431\n",
      "Step: 4047, Loss: 0.0007105052936822176\n",
      "Data loss: 0.00032909607398323715, Function loss: 0.00038415644667111337\n",
      "Step: 4048, Loss: 0.0007132525206543505\n",
      "Data loss: 0.0003204596578143537, Function loss: 0.00039366085547953844\n",
      "Step: 4049, Loss: 0.0007141205132938921\n",
      "Data loss: 0.0003293903428129852, Function loss: 0.00038668769411742687\n",
      "Step: 4050, Loss: 0.000716078036930412\n",
      "Data loss: 0.0003190134302712977, Function loss: 0.00039527888293378055\n",
      "Step: 4051, Loss: 0.0007142922841012478\n",
      "Data loss: 0.0003294788475614041, Function loss: 0.0003837805998045951\n",
      "Step: 4052, Loss: 0.0007132594473659992\n",
      "Data loss: 0.0003178660408593714, Function loss: 0.0003918496076948941\n",
      "Step: 4053, Loss: 0.0007097156485542655\n",
      "Data loss: 0.00032951816683635116, Function loss: 0.00037740753032267094\n",
      "Step: 4054, Loss: 0.0007069256971590221\n",
      "Data loss: 0.00031720122206024826, Function loss: 0.0003850406501442194\n",
      "Step: 4055, Loss: 0.0007022419013082981\n",
      "Data loss: 0.00033004966098815203, Function loss: 0.0003725068236235529\n",
      "Step: 4056, Loss: 0.0007025564555078745\n",
      "Data loss: 0.0003174729645252228, Function loss: 0.00038608917384408414\n",
      "Step: 4057, Loss: 0.0007035621674731374\n",
      "Data loss: 0.000331912626279518, Function loss: 0.0003804626758210361\n",
      "Step: 4058, Loss: 0.0007123752729967237\n",
      "Data loss: 0.000319161219522357, Function loss: 0.00041314837289974093\n",
      "Step: 4059, Loss: 0.0007323095924220979\n",
      "Data loss: 0.0003376727399881929, Function loss: 0.00043092010309919715\n",
      "Step: 4060, Loss: 0.0007685928139835596\n",
      "Data loss: 0.0003240124206058681, Function loss: 0.000506209849845618\n",
      "Step: 4061, Loss: 0.0008302222704514861\n",
      "Data loss: 0.0003512488619890064, Function loss: 0.0005650056409649551\n",
      "Step: 4062, Loss: 0.000916254473850131\n",
      "Data loss: 0.00033477123361080885, Function loss: 0.0006937921862117946\n",
      "Step: 4063, Loss: 0.0010285633616149426\n",
      "Data loss: 0.000373014307115227, Function loss: 0.0007990172016434371\n",
      "Step: 4064, Loss: 0.0011720315087586641\n",
      "Data loss: 0.00035357591696083546, Function loss: 0.0010033465223386884\n",
      "Step: 4065, Loss: 0.0013569224392995238\n",
      "Data loss: 0.00040721948607824743, Function loss: 0.00118204765021801\n",
      "Step: 4066, Loss: 0.0015892671654000878\n",
      "Data loss: 0.0003840317076537758, Function loss: 0.0014920824905857444\n",
      "Step: 4067, Loss: 0.0018761141691356897\n",
      "Data loss: 0.00045720572234131396, Function loss: 0.0017689180094748735\n",
      "Step: 4068, Loss: 0.002226123819127679\n",
      "Data loss: 0.000427750259405002, Function loss: 0.0022161221131682396\n",
      "Step: 4069, Loss: 0.0026438722852617502\n",
      "Data loss: 0.0005203944747336209, Function loss: 0.0025807099882513285\n",
      "Step: 4070, Loss: 0.0031011044047772884\n",
      "Data loss: 0.0004719608696177602, Function loss: 0.0030576284043490887\n",
      "Step: 4071, Loss: 0.003529589157551527\n",
      "Data loss: 0.0005615847185254097, Function loss: 0.0032739860471338034\n",
      "Step: 4072, Loss: 0.003835570765659213\n",
      "Data loss: 0.00047446726239286363, Function loss: 0.003424888476729393\n",
      "Step: 4073, Loss: 0.0038993556518107653\n",
      "Data loss: 0.0005291436100378633, Function loss: 0.0031675687059760094\n",
      "Step: 4074, Loss: 0.003696712199598551\n",
      "Data loss: 0.00041190418414771557, Function loss: 0.002840345725417137\n",
      "Step: 4075, Loss: 0.0032522499095648527\n",
      "Data loss: 0.0004370186070445925, Function loss: 0.002320765983313322\n",
      "Step: 4076, Loss: 0.002757784677669406\n",
      "Data loss: 0.00034334114752709866, Function loss: 0.0019543820526450872\n",
      "Step: 4077, Loss: 0.002297723200172186\n",
      "Data loss: 0.0003735213540494442, Function loss: 0.0017159554408863187\n",
      "Step: 4078, Loss: 0.002089476678520441\n",
      "Data loss: 0.0003418670385144651, Function loss: 0.0017366769025102258\n",
      "Step: 4079, Loss: 0.00207854388281703\n",
      "Data loss: 0.00038668772322125733, Function loss: 0.0019073185976594687\n",
      "Step: 4080, Loss: 0.0022940062917768955\n",
      "Data loss: 0.00038899111677892506, Function loss: 0.002086534397676587\n",
      "Step: 4081, Loss: 0.0024755254853516817\n",
      "Data loss: 0.00041776266880333424, Function loss: 0.002158218063414097\n",
      "Step: 4082, Loss: 0.002575980732217431\n",
      "Data loss: 0.00040195020847022533, Function loss: 0.0020238568540662527\n",
      "Step: 4083, Loss: 0.002425807062536478\n",
      "Data loss: 0.000401176541345194, Function loss: 0.0016988154966384172\n",
      "Step: 4084, Loss: 0.0020999920088797808\n",
      "Data loss: 0.00036204911884851754, Function loss: 0.001251401612535119\n",
      "Step: 4085, Loss: 0.0016134507022798061\n",
      "Data loss: 0.00035702163586393, Function loss: 0.0008172744419425726\n",
      "Step: 4086, Loss: 0.0011742960195988417\n",
      "Data loss: 0.0003300933458376676, Function loss: 0.0005406179116107523\n",
      "Step: 4087, Loss: 0.0008707112865522504\n",
      "Data loss: 0.0003427666670177132, Function loss: 0.0004210570186842233\n",
      "Step: 4088, Loss: 0.0007638236857019365\n",
      "Data loss: 0.0003402281436137855, Function loss: 0.0004755584232043475\n",
      "Step: 4089, Loss: 0.0008157865377143025\n",
      "Data loss: 0.00035698164720088243, Function loss: 0.0005761286593042314\n",
      "Step: 4090, Loss: 0.0009331103065051138\n",
      "Data loss: 0.00035882036900147796, Function loss: 0.0006725898711010814\n",
      "Step: 4091, Loss: 0.0010314101818948984\n",
      "Data loss: 0.0003579977492336184, Function loss: 0.0007045860402286053\n",
      "Step: 4092, Loss: 0.0010625837603583932\n",
      "Data loss: 0.000356390984961763, Function loss: 0.0006785505684092641\n",
      "Step: 4093, Loss: 0.0010349415242671967\n",
      "Data loss: 0.00034017153666354716, Function loss: 0.0006456972914747894\n",
      "Step: 4094, Loss: 0.000985868857242167\n",
      "Data loss: 0.00034855466219596565, Function loss: 0.0006136205047369003\n",
      "Step: 4095, Loss: 0.0009621751960366964\n",
      "Data loss: 0.000328240217640996, Function loss: 0.0006444961181841791\n",
      "Step: 4096, Loss: 0.000972736335825175\n",
      "Data loss: 0.0003521128965076059, Function loss: 0.0006679405923932791\n",
      "Step: 4097, Loss: 0.0010200535180047154\n",
      "Data loss: 0.0003272055182605982, Function loss: 0.0007480417843908072\n",
      "Step: 4098, Loss: 0.0010752473026514053\n",
      "Data loss: 0.0003612001019064337, Function loss: 0.0007702526054345071\n",
      "Step: 4099, Loss: 0.0011314527364447713\n",
      "Data loss: 0.00032894915784709156, Function loss: 0.0008277184679172933\n",
      "Step: 4100, Loss: 0.0011566675966605544\n",
      "Data loss: 0.0003646424156613648, Function loss: 0.0008001552778296173\n",
      "Step: 4101, Loss: 0.001164797693490982\n",
      "Data loss: 0.0003277399518992752, Function loss: 0.0008151016081683338\n",
      "Step: 4102, Loss: 0.0011428415309637785\n",
      "Data loss: 0.00036049532354809344, Function loss: 0.0007434053695760667\n",
      "Step: 4103, Loss: 0.0011039007222279906\n",
      "Data loss: 0.00032423800439573824, Function loss: 0.0007227291935123503\n",
      "Step: 4104, Loss: 0.001046967227011919\n",
      "Data loss: 0.00035184406442567706, Function loss: 0.0006438277196139097\n",
      "Step: 4105, Loss: 0.0009956718422472477\n",
      "Data loss: 0.00032239308347925544, Function loss: 0.0006183948717080057\n",
      "Step: 4106, Loss: 0.0009407879551872611\n",
      "Data loss: 0.0003440772998146713, Function loss: 0.0005525687010958791\n",
      "Step: 4107, Loss: 0.0008966460009105504\n",
      "Data loss: 0.00032423401717096567, Function loss: 0.00053346564527601\n",
      "Step: 4108, Loss: 0.0008576996624469757\n",
      "Data loss: 0.00033908564364537597, Function loss: 0.00048650478129275143\n",
      "Step: 4109, Loss: 0.0008255904540419579\n",
      "Data loss: 0.0003265860432293266, Function loss: 0.0004610009491443634\n",
      "Step: 4110, Loss: 0.0007875870214775205\n",
      "Data loss: 0.0003345971927046776, Function loss: 0.0004133746260777116\n",
      "Step: 4111, Loss: 0.0007479718187823892\n",
      "Data loss: 0.0003265603445470333, Function loss: 0.000385186547646299\n",
      "Step: 4112, Loss: 0.0007117468630895019\n",
      "Data loss: 0.0003307214064989239, Function loss: 0.0003543414641171694\n",
      "Step: 4113, Loss: 0.0006850628415122628\n",
      "Data loss: 0.0003254576586186886, Function loss: 0.0003423759189900011\n",
      "Step: 4114, Loss: 0.0006678336067125201\n",
      "Data loss: 0.0003287683066446334, Function loss: 0.00032733980333432555\n",
      "Step: 4115, Loss: 0.0006561081390827894\n",
      "Data loss: 0.0003241622180212289, Function loss: 0.00032455899054184556\n",
      "Step: 4116, Loss: 0.0006487212376669049\n",
      "Data loss: 0.0003280884411651641, Function loss: 0.00031678754021413624\n",
      "Step: 4117, Loss: 0.0006448759813793004\n",
      "Data loss: 0.0003230780712328851, Function loss: 0.0003201533108949661\n",
      "Step: 4118, Loss: 0.0006432313821278512\n",
      "Data loss: 0.0003276851784903556, Function loss: 0.000314795586746186\n",
      "Step: 4119, Loss: 0.0006424807943403721\n",
      "Data loss: 0.00032240155269391835, Function loss: 0.0003198152408003807\n",
      "Step: 4120, Loss: 0.0006422167643904686\n",
      "Data loss: 0.0003273259208071977, Function loss: 0.00031538112671114504\n",
      "Step: 4121, Loss: 0.0006427070475183427\n",
      "Data loss: 0.0003216452314518392, Function loss: 0.00032108338200487196\n",
      "Step: 4122, Loss: 0.0006427286425605416\n",
      "Data loss: 0.0003263989638071507, Function loss: 0.00031573354499414563\n",
      "Step: 4123, Loss: 0.0006421324796974659\n",
      "Data loss: 0.000321437168167904, Function loss: 0.00032168577308766544\n",
      "Step: 4124, Loss: 0.0006431229412555695\n",
      "Data loss: 0.0003251562302466482, Function loss: 0.00031902940827421844\n",
      "Step: 4125, Loss: 0.0006441856385208666\n",
      "Data loss: 0.00032174313673749566, Function loss: 0.0003252717724535614\n",
      "Step: 4126, Loss: 0.0006470149382948875\n",
      "Data loss: 0.0003246234555263072, Function loss: 0.0003265773120801896\n",
      "Step: 4127, Loss: 0.0006512007676064968\n",
      "Data loss: 0.0003215771575924009, Function loss: 0.00033592432737350464\n",
      "Step: 4128, Loss: 0.000657501514069736\n",
      "Data loss: 0.00032511132303625345, Function loss: 0.00034251983743160963\n",
      "Step: 4129, Loss: 0.0006676311604678631\n",
      "Data loss: 0.00032051908783614635, Function loss: 0.0003641872899606824\n",
      "Step: 4130, Loss: 0.0006847063777968287\n",
      "Data loss: 0.00032786931842565536, Function loss: 0.00038998271338641644\n",
      "Step: 4131, Loss: 0.0007178520318120718\n",
      "Data loss: 0.0003188291157130152, Function loss: 0.0004574787453748286\n",
      "Step: 4132, Loss: 0.0007763078901916742\n",
      "Data loss: 0.0003347186138853431, Function loss: 0.0005483669810928404\n",
      "Step: 4133, Loss: 0.0008830855949781835\n",
      "Data loss: 0.0003185478853993118, Function loss: 0.0007472644210793078\n",
      "Step: 4134, Loss: 0.0010658123064786196\n",
      "Data loss: 0.0003515436837915331, Function loss: 0.0010427270317450166\n",
      "Step: 4135, Loss: 0.0013942706864327192\n",
      "Data loss: 0.0003247722052037716, Function loss: 0.0016179726226255298\n",
      "Step: 4136, Loss: 0.0019427448278293014\n",
      "Data loss: 0.00039413548074662685, Function loss: 0.002565814182162285\n",
      "Step: 4137, Loss: 0.0029599496629089117\n",
      "Data loss: 0.0003590597480069846, Function loss: 0.0042531187646090984\n",
      "Step: 4138, Loss: 0.0046121785417199135\n",
      "Data loss: 0.0005074667278677225, Function loss: 0.0072538298554718494\n",
      "Step: 4139, Loss: 0.007761296816170216\n",
      "Data loss: 0.0004788801888935268, Function loss: 0.011678829789161682\n",
      "Step: 4140, Loss: 0.012157710269093513\n",
      "Data loss: 0.0007760163280181587, Function loss: 0.019213607534766197\n",
      "Step: 4141, Loss: 0.019989624619483948\n",
      "Data loss: 0.0007368379738181829, Function loss: 0.025721034035086632\n",
      "Step: 4142, Loss: 0.02645787224173546\n",
      "Data loss: 0.0010938873747363687, Function loss: 0.03344734385609627\n",
      "Step: 4143, Loss: 0.03454123064875603\n",
      "Data loss: 0.0008235848508775234, Function loss: 0.027517303824424744\n",
      "Step: 4144, Loss: 0.028340889140963554\n",
      "Data loss: 0.0007727357442490757, Function loss: 0.01667245849967003\n",
      "Step: 4145, Loss: 0.017445193603634834\n",
      "Data loss: 0.00045018852688372135, Function loss: 0.003276916453614831\n",
      "Step: 4146, Loss: 0.0037271049804985523\n",
      "Data loss: 0.0004751172673422843, Function loss: 0.0015590646071359515\n",
      "Step: 4147, Loss: 0.002034181961789727\n",
      "Data loss: 0.0006572782294824719, Function loss: 0.009380413219332695\n",
      "Step: 4148, Loss: 0.010037691332399845\n",
      "Data loss: 0.0006939080194570124, Function loss: 0.012526708655059338\n",
      "Step: 4149, Loss: 0.013220616616308689\n",
      "Data loss: 0.000699185358826071, Function loss: 0.0073609184473752975\n",
      "Step: 4150, Loss: 0.008060104213654995\n",
      "Data loss: 0.0005308754043653607, Function loss: 0.0009237540070898831\n",
      "Step: 4151, Loss: 0.0014546294696629047\n",
      "Data loss: 0.0006167820538394153, Function loss: 0.002777643734589219\n",
      "Step: 4152, Loss: 0.0033944258466362953\n",
      "Data loss: 0.0007454860606230795, Function loss: 0.0072546834126114845\n",
      "Step: 4153, Loss: 0.00800016988068819\n",
      "Data loss: 0.0006469869986176491, Function loss: 0.004946217406541109\n",
      "Step: 4154, Loss: 0.005593204405158758\n",
      "Data loss: 0.0006367175956256688, Function loss: 0.0008049734169617295\n",
      "Step: 4155, Loss: 0.0014416910707950592\n",
      "Data loss: 0.0006370758637785912, Function loss: 0.0016003383789211512\n",
      "Step: 4156, Loss: 0.0022374142426997423\n",
      "Data loss: 0.0006894694524817169, Function loss: 0.004272878635674715\n",
      "Step: 4157, Loss: 0.004962347913533449\n",
      "Data loss: 0.000707677099853754, Function loss: 0.0032760060857981443\n",
      "Step: 4158, Loss: 0.003983682952821255\n",
      "Data loss: 0.0006290434394031763, Function loss: 0.0005540912388823926\n",
      "Step: 4159, Loss: 0.0011831347364932299\n",
      "Data loss: 0.0006546615622937679, Function loss: 0.001350843464024365\n",
      "Step: 4160, Loss: 0.002005504909902811\n",
      "Data loss: 0.0007203260902315378, Function loss: 0.0031716194935142994\n",
      "Step: 4161, Loss: 0.003891945583745837\n",
      "Data loss: 0.0006397387478500605, Function loss: 0.001907136058434844\n",
      "Step: 4162, Loss: 0.0025468748062849045\n",
      "Data loss: 0.00061475281836465, Function loss: 0.0003123705100733787\n",
      "Step: 4163, Loss: 0.0009271233575418591\n",
      "Data loss: 0.000641526305116713, Function loss: 0.0011886818101629615\n",
      "Step: 4164, Loss: 0.0018302081152796745\n",
      "Data loss: 0.0006067698122933507, Function loss: 0.0021701159421354532\n",
      "Step: 4165, Loss: 0.0027768858708441257\n",
      "Data loss: 0.0006316512590274215, Function loss: 0.0011763391084969044\n",
      "Step: 4166, Loss: 0.0018079903675243258\n",
      "Data loss: 0.0005719300243072212, Function loss: 0.0002999566204380244\n",
      "Step: 4167, Loss: 0.0008718866156414151\n",
      "Data loss: 0.0005491572665050626, Function loss: 0.001074617961421609\n",
      "Step: 4168, Loss: 0.0016237752279266715\n",
      "Data loss: 0.0006152893183752894, Function loss: 0.0016615549102425575\n",
      "Step: 4169, Loss: 0.002276844345033169\n",
      "Data loss: 0.0005151839577592909, Function loss: 0.0009773194324225187\n",
      "Step: 4170, Loss: 0.0014925033319741488\n",
      "Data loss: 0.0005214518751017749, Function loss: 0.0003091446415055543\n",
      "Step: 4171, Loss: 0.0008305965457111597\n",
      "Data loss: 0.0005389287252910435, Function loss: 0.0007095293840393424\n",
      "Step: 4172, Loss: 0.0012484581675380468\n",
      "Data loss: 0.0004783577751368284, Function loss: 0.0012098103761672974\n",
      "Step: 4173, Loss: 0.0016881681513041258\n",
      "Data loss: 0.0005311216809786856, Function loss: 0.0007898604962974787\n",
      "Step: 4174, Loss: 0.0013209821190685034\n",
      "Data loss: 0.0004738075949717313, Function loss: 0.0003323027049191296\n",
      "Step: 4175, Loss: 0.0008061103289946914\n",
      "Data loss: 0.00045826565474271774, Function loss: 0.0005388071294873953\n",
      "Step: 4176, Loss: 0.000997072784230113\n",
      "Data loss: 0.0005086025921627879, Function loss: 0.0009069504449144006\n",
      "Step: 4177, Loss: 0.0014155530370771885\n",
      "Data loss: 0.00043357806862331927, Function loss: 0.000817496154922992\n",
      "Step: 4178, Loss: 0.0012510742526501417\n",
      "Data loss: 0.00046015947009436786, Function loss: 0.0003592796274460852\n",
      "Step: 4179, Loss: 0.0008194390684366226\n",
      "Data loss: 0.00045016652438789606, Function loss: 0.00035068689612671733\n",
      "Step: 4180, Loss: 0.0008008534205146134\n",
      "Data loss: 0.0004141709941904992, Function loss: 0.0006682691746391356\n",
      "Step: 4181, Loss: 0.0010824401397258043\n",
      "Data loss: 0.000460987095721066, Function loss: 0.0006306449067778885\n",
      "Step: 4182, Loss: 0.0010916320607066154\n",
      "Data loss: 0.00040771346539258957, Function loss: 0.00040056509897112846\n",
      "Step: 4183, Loss: 0.000808278564363718\n",
      "Data loss: 0.00041086951387114823, Function loss: 0.000315044482704252\n",
      "Step: 4184, Loss: 0.0007259140256792307\n",
      "Data loss: 0.0004319992149248719, Function loss: 0.00047436635941267014\n",
      "Step: 4185, Loss: 0.0009063655743375421\n",
      "Data loss: 0.00038551472243852913, Function loss: 0.0005991822690702975\n",
      "Step: 4186, Loss: 0.000984697020612657\n",
      "Data loss: 0.0004190242907498032, Function loss: 0.00041740507003851235\n",
      "Step: 4187, Loss: 0.0008364293607883155\n",
      "Data loss: 0.0003904361801687628, Function loss: 0.00030114431865513325\n",
      "Step: 4188, Loss: 0.0006915804697200656\n",
      "Data loss: 0.0003828550106845796, Function loss: 0.0003589018015190959\n",
      "Step: 4189, Loss: 0.0007417568122036755\n",
      "Data loss: 0.0004070809518452734, Function loss: 0.00045189011143520474\n",
      "Step: 4190, Loss: 0.0008589710341766477\n",
      "Data loss: 0.00036962589365430176, Function loss: 0.00046272313920781016\n",
      "Step: 4191, Loss: 0.0008323490619659424\n",
      "Data loss: 0.00038814821164123714, Function loss: 0.0003203547385055572\n",
      "Step: 4192, Loss: 0.0007085029501467943\n",
      "Data loss: 0.0003774730139411986, Function loss: 0.0002923782740253955\n",
      "Step: 4193, Loss: 0.0006698513170704246\n",
      "Data loss: 0.0003641123475972563, Function loss: 0.00037771087954752147\n",
      "Step: 4194, Loss: 0.0007418232271447778\n",
      "Data loss: 0.00038428843254223466, Function loss: 0.0004093426396138966\n",
      "Step: 4195, Loss: 0.0007936310721561313\n",
      "Data loss: 0.0003584600635804236, Function loss: 0.0003893939428962767\n",
      "Step: 4196, Loss: 0.0007478540064767003\n",
      "Data loss: 0.0003683174727484584, Function loss: 0.0003058495349250734\n",
      "Step: 4197, Loss: 0.0006741670076735318\n",
      "Data loss: 0.0003627384139690548, Function loss: 0.0002910694747697562\n",
      "Step: 4198, Loss: 0.000653807888738811\n",
      "Data loss: 0.000352653645677492, Function loss: 0.0003348951577208936\n",
      "Step: 4199, Loss: 0.0006875487742945552\n",
      "Data loss: 0.0003660176007542759, Function loss: 0.0003536925360094756\n",
      "Step: 4200, Loss: 0.0007197101367637515\n",
      "Data loss: 0.00034769903868436813, Function loss: 0.0003575050795916468\n",
      "Step: 4201, Loss: 0.0007052040891721845\n",
      "Data loss: 0.00035586554440669715, Function loss: 0.00030843241256661713\n",
      "Step: 4202, Loss: 0.0006642979569733143\n",
      "Data loss: 0.00035074487095698714, Function loss: 0.00029137186356820166\n",
      "Step: 4203, Loss: 0.0006421167636290193\n",
      "Data loss: 0.00034313189098611474, Function loss: 0.00031227132421918213\n",
      "Step: 4204, Loss: 0.0006554032443091273\n",
      "Data loss: 0.00035401908098720014, Function loss: 0.0003229082503821701\n",
      "Step: 4205, Loss: 0.0006769273313693702\n",
      "Data loss: 0.00033815624192357063, Function loss: 0.0003397829714231193\n",
      "Step: 4206, Loss: 0.0006779392133466899\n",
      "Data loss: 0.00034806845360435545, Function loss: 0.0003112081903964281\n",
      "Step: 4207, Loss: 0.0006592766148969531\n",
      "Data loss: 0.0003393551451154053, Function loss: 0.00029903388349339366\n",
      "Step: 4208, Loss: 0.000638389028608799\n",
      "Data loss: 0.0003381397691555321, Function loss: 0.00029524348792620003\n",
      "Step: 4209, Loss: 0.0006333832861855626\n",
      "Data loss: 0.00034268523450009525, Function loss: 0.00030244342633523047\n",
      "Step: 4210, Loss: 0.0006451286608353257\n",
      "Data loss: 0.0003323425189591944, Function loss: 0.000322314677760005\n",
      "Step: 4211, Loss: 0.0006546571967191994\n",
      "Data loss: 0.0003405044262763113, Function loss: 0.0003097937733400613\n",
      "Step: 4212, Loss: 0.0006502981996163726\n",
      "Data loss: 0.0003326644073240459, Function loss: 0.0003037162823602557\n",
      "Step: 4213, Loss: 0.0006363806896843016\n",
      "Data loss: 0.0003330176987219602, Function loss: 0.0002954683732241392\n",
      "Step: 4214, Loss: 0.0006284860428422689\n",
      "Data loss: 0.00033545063342899084, Function loss: 0.00029579512192867696\n",
      "Step: 4215, Loss: 0.0006312457844614983\n",
      "Data loss: 0.0003277529904153198, Function loss: 0.0003091028775088489\n",
      "Step: 4216, Loss: 0.0006368558388203382\n",
      "Data loss: 0.0003346909652464092, Function loss: 0.0003024130128324032\n",
      "Step: 4217, Loss: 0.0006371039780788124\n",
      "Data loss: 0.00032679433934390545, Function loss: 0.00030498162959702313\n",
      "Step: 4218, Loss: 0.0006317759398370981\n",
      "Data loss: 0.0003299424424767494, Function loss: 0.0002954024530481547\n",
      "Step: 4219, Loss: 0.0006253449246287346\n",
      "Data loss: 0.000328625290421769, Function loss: 0.00029474272741936147\n",
      "Step: 4220, Loss: 0.0006233680178411305\n",
      "Data loss: 0.000324444001307711, Function loss: 0.000303026958135888\n",
      "Step: 4221, Loss: 0.000627470959443599\n",
      "Data loss: 0.0003303725679870695, Function loss: 0.000301864929497242\n",
      "Step: 4222, Loss: 0.0006322375265881419\n",
      "Data loss: 0.00032205501338467, Function loss: 0.00031103906803764403\n",
      "Step: 4223, Loss: 0.0006330941105261445\n",
      "Data loss: 0.00032850002753548324, Function loss: 0.0002998234995175153\n",
      "Step: 4224, Loss: 0.0006283235270529985\n",
      "Data loss: 0.00032247157650999725, Function loss: 0.0003002299927175045\n",
      "Step: 4225, Loss: 0.0006227015983313322\n",
      "Data loss: 0.00032437959453091025, Function loss: 0.00029552486375905573\n",
      "Step: 4226, Loss: 0.0006199044873937964\n",
      "Data loss: 0.00032361451303586364, Function loss: 0.0002962744329124689\n",
      "Step: 4227, Loss: 0.0006198889459483325\n",
      "Data loss: 0.000321317435009405, Function loss: 0.0002996211696881801\n",
      "Step: 4228, Loss: 0.0006209386046975851\n",
      "Data loss: 0.00032393186120316386, Function loss: 0.0002985730825457722\n",
      "Step: 4229, Loss: 0.0006225049728527665\n",
      "Data loss: 0.00031999300699681044, Function loss: 0.0003023821336682886\n",
      "Step: 4230, Loss: 0.0006223751697689295\n",
      "Data loss: 0.000322615698678419, Function loss: 0.0002989184868056327\n",
      "Step: 4231, Loss: 0.0006215341854840517\n",
      "Data loss: 0.0003198328777216375, Function loss: 0.00029952044133096933\n",
      "Step: 4232, Loss: 0.0006193533190526068\n",
      "Data loss: 0.0003203093947377056, Function loss: 0.000297588761895895\n",
      "Step: 4233, Loss: 0.000617898185737431\n",
      "Data loss: 0.0003210202557966113, Function loss: 0.00029756908770650625\n",
      "Step: 4234, Loss: 0.0006185893435031176\n",
      "Data loss: 0.00031804278842173517, Function loss: 0.00030237180180847645\n",
      "Step: 4235, Loss: 0.0006204146193340421\n",
      "Data loss: 0.0003223432577215135, Function loss: 0.00030238673207350075\n",
      "Step: 4236, Loss: 0.0006247300188988447\n",
      "Data loss: 0.00031664117705076933, Function loss: 0.0003142438945360482\n",
      "Step: 4237, Loss: 0.0006308850715868175\n",
      "Data loss: 0.000323912245221436, Function loss: 0.00031397075508721173\n",
      "Step: 4238, Loss: 0.0006378829712048173\n",
      "Data loss: 0.0003161847998853773, Function loss: 0.00032874118187464774\n",
      "Step: 4239, Loss: 0.000644925981760025\n",
      "Data loss: 0.0003247377753723413, Function loss: 0.00032606706372462213\n",
      "Step: 4240, Loss: 0.0006508048390969634\n",
      "Data loss: 0.0003162177454214543, Function loss: 0.0003374673833604902\n",
      "Step: 4241, Loss: 0.0006536851287819445\n",
      "Data loss: 0.0003240754595026374, Function loss: 0.00033127181814052165\n",
      "Step: 4242, Loss: 0.0006553472485393286\n",
      "Data loss: 0.00031687645241618156, Function loss: 0.0003389658813830465\n",
      "Step: 4243, Loss: 0.0006558423629030585\n",
      "Data loss: 0.00032230454962700605, Function loss: 0.0003330708423163742\n",
      "Step: 4244, Loss: 0.0006553754210472107\n",
      "Data loss: 0.0003173289296682924, Function loss: 0.00033805836574174464\n",
      "Step: 4245, Loss: 0.000655387295410037\n",
      "Data loss: 0.00032066195853985846, Function loss: 0.00033248704858124256\n",
      "Step: 4246, Loss: 0.0006531489780172706\n",
      "Data loss: 0.0003167673130519688, Function loss: 0.0003330522740725428\n",
      "Step: 4247, Loss: 0.0006498196162283421\n",
      "Data loss: 0.0003204659733455628, Function loss: 0.0003254580369684845\n",
      "Step: 4248, Loss: 0.0006459240103140473\n",
      "Data loss: 0.0003157242899760604, Function loss: 0.0003286323044449091\n",
      "Step: 4249, Loss: 0.0006443565944209695\n",
      "Data loss: 0.0003209948190487921, Function loss: 0.00032275752164423466\n",
      "Step: 4250, Loss: 0.0006437523406930268\n",
      "Data loss: 0.0003146874369122088, Function loss: 0.000330069538904354\n",
      "Step: 4251, Loss: 0.0006447569467127323\n",
      "Data loss: 0.00032182340510189533, Function loss: 0.0003256079799029976\n",
      "Step: 4252, Loss: 0.0006474313559010625\n",
      "Data loss: 0.0003140629269182682, Function loss: 0.0003353857609909028\n",
      "Step: 4253, Loss: 0.0006494487170130014\n",
      "Data loss: 0.0003220493090339005, Function loss: 0.00032739213202148676\n",
      "Step: 4254, Loss: 0.0006494414410553873\n",
      "Data loss: 0.00031378614949062467, Function loss: 0.0003334968350827694\n",
      "Step: 4255, Loss: 0.0006472829845733941\n",
      "Data loss: 0.00032121752155944705, Function loss: 0.00032304931664839387\n",
      "Step: 4256, Loss: 0.0006442668382078409\n",
      "Data loss: 0.00031378577114082873, Function loss: 0.00033052999060600996\n",
      "Step: 4257, Loss: 0.0006443157326430082\n",
      "Data loss: 0.0003209617279935628, Function loss: 0.00032710269442759454\n",
      "Step: 4258, Loss: 0.0006480644224211574\n",
      "Data loss: 0.0003145488153677434, Function loss: 0.00034476653672754765\n",
      "Step: 4259, Loss: 0.0006593153811991215\n",
      "Data loss: 0.0003234736213926226, Function loss: 0.0003556182491593063\n",
      "Step: 4260, Loss: 0.0006790918996557593\n",
      "Data loss: 0.0003168945841025561, Function loss: 0.0003890892257913947\n",
      "Step: 4261, Loss: 0.0007059838389977813\n",
      "Data loss: 0.00032877366174943745, Function loss: 0.0004140513192396611\n",
      "Step: 4262, Loss: 0.0007428249809890985\n",
      "Data loss: 0.0003213612944819033, Function loss: 0.0004662324790842831\n",
      "Step: 4263, Loss: 0.0007875937735661864\n",
      "Data loss: 0.00033690716372802854, Function loss: 0.000508213066495955\n",
      "Step: 4264, Loss: 0.0008451202302239835\n",
      "Data loss: 0.0003289029991719872, Function loss: 0.0005894458736293018\n",
      "Step: 4265, Loss: 0.0009183489019051194\n",
      "Data loss: 0.00034951293491758406, Function loss: 0.0006548612145707011\n",
      "Step: 4266, Loss: 0.0010043741203844547\n",
      "Data loss: 0.00033992022508755326, Function loss: 0.0007620907854288816\n",
      "Step: 4267, Loss: 0.001102010952308774\n",
      "Data loss: 0.0003649549908004701, Function loss: 0.0008404108812101185\n",
      "Step: 4268, Loss: 0.0012053658720105886\n",
      "Data loss: 0.00035289739025756717, Function loss: 0.0009543955675326288\n",
      "Step: 4269, Loss: 0.001307292957790196\n",
      "Data loss: 0.0003784684813581407, Function loss: 0.001021494739688933\n",
      "Step: 4270, Loss: 0.0013999631628394127\n",
      "Data loss: 0.00036448397440835834, Function loss: 0.0011216760613024235\n",
      "Step: 4271, Loss: 0.0014861600939184427\n",
      "Data loss: 0.0003887282218784094, Function loss: 0.001163093838840723\n",
      "Step: 4272, Loss: 0.0015518220607191324\n",
      "Data loss: 0.0003717209037858993, Function loss: 0.0012252654414623976\n",
      "Step: 4273, Loss: 0.0015969863161444664\n",
      "Data loss: 0.0003934263077098876, Function loss: 0.0012212193105369806\n",
      "Step: 4274, Loss: 0.0016146455891430378\n",
      "Data loss: 0.0003738771192729473, Function loss: 0.0012402159627526999\n",
      "Step: 4275, Loss: 0.0016140930820256472\n",
      "Data loss: 0.00039302566437982023, Function loss: 0.0011982452124357224\n",
      "Step: 4276, Loss: 0.001591270905919373\n",
      "Data loss: 0.00037073183921165764, Function loss: 0.0011771987192332745\n",
      "Step: 4277, Loss: 0.0015479305293411016\n",
      "Data loss: 0.0003854355600196868, Function loss: 0.0010842427145689726\n",
      "Step: 4278, Loss: 0.001469678245484829\n",
      "Data loss: 0.0003592139692045748, Function loss: 0.001003856654278934\n",
      "Step: 4279, Loss: 0.0013630706816911697\n",
      "Data loss: 0.0003686954441946, Function loss: 0.0008593297679908574\n",
      "Step: 4280, Loss: 0.001228025183081627\n",
      "Data loss: 0.00034098411560989916, Function loss: 0.0007377914153039455\n",
      "Step: 4281, Loss: 0.0010787755018100142\n",
      "Data loss: 0.000347310007782653, Function loss: 0.000591773190535605\n",
      "Step: 4282, Loss: 0.0009390831692144275\n",
      "Data loss: 0.00032397746690548956, Function loss: 0.0004952803137712181\n",
      "Step: 4283, Loss: 0.0008192578097805381\n",
      "Data loss: 0.0003295607748441398, Function loss: 0.0003928037767764181\n",
      "Step: 4284, Loss: 0.0007223645225167274\n",
      "Data loss: 0.0003146155213471502, Function loss: 0.00033715294557623565\n",
      "Step: 4285, Loss: 0.0006517684669233859\n",
      "Data loss: 0.0003180984640493989, Function loss: 0.0002930418704636395\n",
      "Step: 4286, Loss: 0.0006111403345130384\n",
      "Data loss: 0.00031568281701765954, Function loss: 0.00029327830998227\n",
      "Step: 4287, Loss: 0.0006089610978960991\n",
      "Data loss: 0.000316419085720554, Function loss: 0.00031981730717234313\n",
      "Step: 4288, Loss: 0.0006362363928928971\n",
      "Data loss: 0.00032407630351372063, Function loss: 0.0003542110789567232\n",
      "Step: 4289, Loss: 0.0006782873533666134\n",
      "Data loss: 0.000320444320095703, Function loss: 0.0004077735939063132\n",
      "Step: 4290, Loss: 0.0007282178848981857\n",
      "Data loss: 0.0003325618745293468, Function loss: 0.00043498838203959167\n",
      "Step: 4291, Loss: 0.0007675502565689385\n",
      "Data loss: 0.0003234396281186491, Function loss: 0.0004625541914720088\n",
      "Step: 4292, Loss: 0.000785993819590658\n",
      "Data loss: 0.0003334056818857789, Function loss: 0.00044484256068244576\n",
      "Step: 4293, Loss: 0.0007782482425682247\n",
      "Data loss: 0.00032142834970727563, Function loss: 0.0004294278332963586\n",
      "Step: 4294, Loss: 0.0007508561830036342\n",
      "Data loss: 0.0003270242887083441, Function loss: 0.0003850269131362438\n",
      "Step: 4295, Loss: 0.0007120511727407575\n",
      "Data loss: 0.0003177743055857718, Function loss: 0.0003554950817488134\n",
      "Step: 4296, Loss: 0.0006732693873345852\n",
      "Data loss: 0.00031859471346251667, Function loss: 0.00032286220812238753\n",
      "Step: 4297, Loss: 0.0006414569215849042\n",
      "Data loss: 0.0003165200469084084, Function loss: 0.0003076176217291504\n",
      "Step: 4298, Loss: 0.0006241376977413893\n",
      "Data loss: 0.00031353728263638914, Function loss: 0.0003053900436498225\n",
      "Step: 4299, Loss: 0.0006189272971823812\n",
      "Data loss: 0.00031829081126488745, Function loss: 0.00030478578992187977\n",
      "Step: 4300, Loss: 0.0006230765720829368\n",
      "Data loss: 0.0003117431770078838, Function loss: 0.00031801575096324086\n",
      "Step: 4301, Loss: 0.0006297589279711246\n",
      "Data loss: 0.0003208674897905439, Function loss: 0.00031796348048374057\n",
      "Step: 4302, Loss: 0.000638830941170454\n",
      "Data loss: 0.0003116833686362952, Function loss: 0.0003357304085511714\n",
      "Step: 4303, Loss: 0.0006474137771874666\n",
      "Data loss: 0.0003226693079341203, Function loss: 0.000330777169438079\n",
      "Step: 4304, Loss: 0.0006534464773721993\n",
      "Data loss: 0.00031221602694131434, Function loss: 0.0003429638163652271\n",
      "Step: 4305, Loss: 0.0006551798433065414\n",
      "Data loss: 0.0003226379049010575, Function loss: 0.0003287945582997054\n",
      "Step: 4306, Loss: 0.0006514324340969324\n",
      "Data loss: 0.0003123448695987463, Function loss: 0.00033355134655721486\n",
      "Step: 4307, Loss: 0.0006458961870521307\n",
      "Data loss: 0.00032117177033796906, Function loss: 0.0003179411287419498\n",
      "Step: 4308, Loss: 0.0006391128990799189\n",
      "Data loss: 0.00031248375307768583, Function loss: 0.0003207222034689039\n",
      "Step: 4309, Loss: 0.0006332059856504202\n",
      "Data loss: 0.0003195037425030023, Function loss: 0.00030736156622879207\n",
      "Step: 4310, Loss: 0.0006268653087317944\n",
      "Data loss: 0.00031258611124940217, Function loss: 0.00030897915712557733\n",
      "Step: 4311, Loss: 0.0006215652683749795\n",
      "Data loss: 0.0003179825725965202, Function loss: 0.0002987071929965168\n",
      "Step: 4312, Loss: 0.0006166897946968675\n",
      "Data loss: 0.0003126823285128921, Function loss: 0.0003017812268808484\n",
      "Step: 4313, Loss: 0.000614463584497571\n",
      "Data loss: 0.00031728032627142966, Function loss: 0.00029816912137903273\n",
      "Step: 4314, Loss: 0.0006154494476504624\n",
      "Data loss: 0.00031300284899771214, Function loss: 0.0003077151777688414\n",
      "Step: 4315, Loss: 0.0006207179976627231\n",
      "Data loss: 0.0003185863315593451, Function loss: 0.000313024123897776\n",
      "Step: 4316, Loss: 0.0006316104554571211\n",
      "Data loss: 0.00031439753365702927, Function loss: 0.00033591920509934425\n",
      "Step: 4317, Loss: 0.0006503167096525431\n",
      "Data loss: 0.00032245091279037297, Function loss: 0.0003536208823788911\n",
      "Step: 4318, Loss: 0.0006760717951692641\n",
      "Data loss: 0.00031732473871670663, Function loss: 0.000390373490517959\n",
      "Step: 4319, Loss: 0.0007076982292346656\n",
      "Data loss: 0.00032852182630449533, Function loss: 0.0004227466124575585\n",
      "Step: 4320, Loss: 0.0007512684678658843\n",
      "Data loss: 0.0003233005409128964, Function loss: 0.0004906361573375762\n",
      "Step: 4321, Loss: 0.0008139366982504725\n",
      "Data loss: 0.0003398234839551151, Function loss: 0.0005625367630273104\n",
      "Step: 4322, Loss: 0.0009023602469824255\n",
      "Data loss: 0.0003363205469213426, Function loss: 0.0006946861394681036\n",
      "Step: 4323, Loss: 0.0010310066863894463\n",
      "Data loss: 0.0003615582245402038, Function loss: 0.0008528140024282038\n",
      "Step: 4324, Loss: 0.0012143722269684076\n",
      "Data loss: 0.00036308547714725137, Function loss: 0.001109888544306159\n",
      "Step: 4325, Loss: 0.0014729739632457495\n",
      "Data loss: 0.00040059632738120854, Function loss: 0.0014285978395491838\n",
      "Step: 4326, Loss: 0.001829194137826562\n",
      "Data loss: 0.0004141848476137966, Function loss: 0.001908675185404718\n",
      "Step: 4327, Loss: 0.002322860062122345\n",
      "Data loss: 0.00047018786426633596, Function loss: 0.0024903006851673126\n",
      "Step: 4328, Loss: 0.0029604886658489704\n",
      "Data loss: 0.0005025060963816941, Function loss: 0.003292409935966134\n",
      "Step: 4329, Loss: 0.0037949159741401672\n",
      "Data loss: 0.0005777347832918167, Function loss: 0.0041614705696702\n",
      "Step: 4330, Loss: 0.004739205352962017\n",
      "Data loss: 0.0006203449447639287, Function loss: 0.005132692400366068\n",
      "Step: 4331, Loss: 0.005753037519752979\n",
      "Data loss: 0.0006741316174156964, Function loss: 0.005785027984529734\n",
      "Step: 4332, Loss: 0.006459159776568413\n",
      "Data loss: 0.0006769240135326982, Function loss: 0.0060447026044130325\n",
      "Step: 4333, Loss: 0.006721626501530409\n",
      "Data loss: 0.0006294838967733085, Function loss: 0.005449316930025816\n",
      "Step: 4334, Loss: 0.006078800652176142\n",
      "Data loss: 0.0005565520841628313, Function loss: 0.004260338842868805\n",
      "Step: 4335, Loss: 0.004816890694200993\n",
      "Data loss: 0.00043342733988538384, Function loss: 0.0027007884345948696\n",
      "Step: 4336, Loss: 0.0031342157162725925\n",
      "Data loss: 0.00037552646244876087, Function loss: 0.001400425797328353\n",
      "Step: 4337, Loss: 0.0017759522888809443\n",
      "Data loss: 0.00032370485132560134, Function loss: 0.000768480182159692\n",
      "Step: 4338, Loss: 0.0010921850334852934\n",
      "Data loss: 0.00035952040343545377, Function loss: 0.0007764167967252433\n",
      "Step: 4339, Loss: 0.0011359371710568666\n",
      "Data loss: 0.0003913029213435948, Function loss: 0.001179385813884437\n",
      "Step: 4340, Loss: 0.0015706887934356928\n",
      "Data loss: 0.00042644981294870377, Function loss: 0.0015212858561426401\n",
      "Step: 4341, Loss: 0.0019477356690913439\n",
      "Data loss: 0.00042589259101077914, Function loss: 0.0015171323902904987\n",
      "Step: 4342, Loss: 0.0019430250395089388\n",
      "Data loss: 0.0003896696725860238, Function loss: 0.0011948583414778113\n",
      "Step: 4343, Loss: 0.0015845280140638351\n",
      "Data loss: 0.0003633694723248482, Function loss: 0.0008040967513807118\n",
      "Step: 4344, Loss: 0.001167466165497899\n",
      "Data loss: 0.000330466398736462, Function loss: 0.0006119954050518572\n",
      "Step: 4345, Loss: 0.0009424617746844888\n",
      "Data loss: 0.0003518130397424102, Function loss: 0.0006311074830591679\n",
      "Step: 4346, Loss: 0.000982920522801578\n",
      "Data loss: 0.00034627708373591304, Function loss: 0.0007952384767122567\n",
      "Step: 4347, Loss: 0.0011415155604481697\n",
      "Data loss: 0.00038159507676027715, Function loss: 0.0008484168210998178\n",
      "Step: 4348, Loss: 0.0012300119269639254\n",
      "Data loss: 0.0003558715106919408, Function loss: 0.000785272044595331\n",
      "Step: 4349, Loss: 0.0011411434970796108\n",
      "Data loss: 0.00036243218346498907, Function loss: 0.0005470410105772316\n",
      "Step: 4350, Loss: 0.0009094731649383903\n",
      "Data loss: 0.00033398770028725266, Function loss: 0.0003555013972800225\n",
      "Step: 4351, Loss: 0.0006894890684634447\n",
      "Data loss: 0.0003294850466772914, Function loss: 0.00028756188112311065\n",
      "Step: 4352, Loss: 0.0006170469569042325\n",
      "Data loss: 0.00034303433494642377, Function loss: 0.00036455021472647786\n",
      "Step: 4353, Loss: 0.0007075845496729016\n",
      "Data loss: 0.00033216216252185404, Function loss: 0.0005275430157780647\n",
      "Step: 4354, Loss: 0.0008597051491960883\n",
      "Data loss: 0.0003621037758421153, Function loss: 0.0005749815027229488\n",
      "Step: 4355, Loss: 0.0009370852494612336\n",
      "Data loss: 0.0003312311018817127, Function loss: 0.0005528308101929724\n",
      "Step: 4356, Loss: 0.0008840619120746851\n",
      "Data loss: 0.0003450998046901077, Function loss: 0.000402046280214563\n",
      "Step: 4357, Loss: 0.0007471460849046707\n",
      "Data loss: 0.00032332914997823536, Function loss: 0.0003084790369030088\n",
      "Step: 4358, Loss: 0.0006318081868812442\n",
      "Data loss: 0.00032675580587238073, Function loss: 0.0002784777316264808\n",
      "Step: 4359, Loss: 0.0006052335374988616\n",
      "Data loss: 0.00033552770037204027, Function loss: 0.00031968895928002894\n",
      "Step: 4360, Loss: 0.0006552166305482388\n",
      "Data loss: 0.00032594086951576173, Function loss: 0.00039163260953500867\n",
      "Step: 4361, Loss: 0.0007175734499469399\n",
      "Data loss: 0.0003436289553064853, Function loss: 0.0003984585637226701\n",
      "Step: 4362, Loss: 0.0007420874899253249\n",
      "Data loss: 0.0003226478584110737, Function loss: 0.00039689289405941963\n",
      "Step: 4363, Loss: 0.0007195407524704933\n",
      "Data loss: 0.00033512531081214547, Function loss: 0.0003383351140655577\n",
      "Step: 4364, Loss: 0.0006734604248777032\n",
      "Data loss: 0.00031884072814136744, Function loss: 0.0003137418534606695\n",
      "Step: 4365, Loss: 0.000632582581602037\n",
      "Data loss: 0.00032601470593363047, Function loss: 0.0002967396576423198\n",
      "Step: 4366, Loss: 0.0006227543344721198\n",
      "Data loss: 0.0003238199860788882, Function loss: 0.00031338364351540804\n",
      "Step: 4367, Loss: 0.0006372036295942962\n",
      "Data loss: 0.00032423975062556565, Function loss: 0.0003306583676021546\n",
      "Step: 4368, Loss: 0.0006548981182277203\n",
      "Data loss: 0.00032865392859093845, Function loss: 0.0003302846453152597\n",
      "Step: 4369, Loss: 0.0006589385448023677\n",
      "Data loss: 0.00032146924058906734, Function loss: 0.0003196903853677213\n",
      "Step: 4370, Loss: 0.0006411595968529582\n",
      "Data loss: 0.0003250233130529523, Function loss: 0.0002876825747080147\n",
      "Step: 4371, Loss: 0.000612705887760967\n",
      "Data loss: 0.00031829875661060214, Function loss: 0.00027160655008628964\n",
      "Step: 4372, Loss: 0.0005899053066968918\n",
      "Data loss: 0.0003198332560714334, Function loss: 0.0002660505997482687\n",
      "Step: 4373, Loss: 0.0005858838558197021\n",
      "Data loss: 0.0003203525557182729, Function loss: 0.0002786321274470538\n",
      "Step: 4374, Loss: 0.0005989846540614963\n",
      "Data loss: 0.00031917516025714576, Function loss: 0.00029597143293358386\n",
      "Step: 4375, Loss: 0.0006151465931907296\n",
      "Data loss: 0.0003223822277504951, Function loss: 0.00029859988717362285\n",
      "Step: 4376, Loss: 0.0006209821440279484\n",
      "Data loss: 0.0003182350774295628, Function loss: 0.00029540210380218923\n",
      "Step: 4377, Loss: 0.0006136371521279216\n",
      "Data loss: 0.0003199957136530429, Function loss: 0.00027902176952920854\n",
      "Step: 4378, Loss: 0.0005990174831822515\n",
      "Data loss: 0.0003162765933666378, Function loss: 0.0002694414579309523\n",
      "Step: 4379, Loss: 0.0005857180804014206\n",
      "Data loss: 0.0003171573334839195, Function loss: 0.0002630211238283664\n",
      "Step: 4380, Loss: 0.0005801784573122859\n",
      "Data loss: 0.0003166305541526526, Function loss: 0.00026642915327101946\n",
      "Step: 4381, Loss: 0.0005830597365275025\n",
      "Data loss: 0.00031607499113306403, Function loss: 0.0002737905306275934\n",
      "Step: 4382, Loss: 0.000589865492656827\n",
      "Data loss: 0.00031806621700525284, Function loss: 0.00027782758115790784\n",
      "Step: 4383, Loss: 0.0005958938272669911\n",
      "Data loss: 0.00031518845935352147, Function loss: 0.000283068569842726\n",
      "Step: 4384, Loss: 0.0005982570583000779\n",
      "Data loss: 0.00031844389741308987, Function loss: 0.00027762341778725386\n",
      "Step: 4385, Loss: 0.0005960672860965133\n",
      "Data loss: 0.0003139530890621245, Function loss: 0.00027665402740240097\n",
      "Step: 4386, Loss: 0.0005906071164645255\n",
      "Data loss: 0.00031696123187430203, Function loss: 0.00026717805303633213\n",
      "Step: 4387, Loss: 0.0005841392558068037\n",
      "Data loss: 0.0003135711594950408, Function loss: 0.00026568243629299104\n",
      "Step: 4388, Loss: 0.0005792535957880318\n",
      "Data loss: 0.0003150904958602041, Function loss: 0.00026272321701981127\n",
      "Step: 4389, Loss: 0.0005778137128800154\n",
      "Data loss: 0.000313743541482836, Function loss: 0.0002653419505804777\n",
      "Step: 4390, Loss: 0.0005790854920633137\n",
      "Data loss: 0.000314363423967734, Function loss: 0.00026819182676263154\n",
      "Step: 4391, Loss: 0.0005825552507303655\n",
      "Data loss: 0.0003139731416013092, Function loss: 0.0002739054325502366\n",
      "Step: 4392, Loss: 0.0005878785741515458\n",
      "Data loss: 0.0003147140669170767, Function loss: 0.00027989401132799685\n",
      "Step: 4393, Loss: 0.0005946080782450736\n",
      "Data loss: 0.000313706899760291, Function loss: 0.0002899192622862756\n",
      "Step: 4394, Loss: 0.0006036261329427361\n",
      "Data loss: 0.0003158221661578864, Function loss: 0.0002990565844811499\n",
      "Step: 4395, Loss: 0.0006148787215352058\n",
      "Data loss: 0.0003122827038168907, Function loss: 0.0003220656653866172\n",
      "Step: 4396, Loss: 0.0006343483692035079\n",
      "Data loss: 0.0003186514077242464, Function loss: 0.00035020499490201473\n",
      "Step: 4397, Loss: 0.0006688564317300916\n",
      "Data loss: 0.00031036758446134627, Function loss: 0.0004112925671506673\n",
      "Step: 4398, Loss: 0.0007216601516120136\n",
      "Data loss: 0.0003241926897317171, Function loss: 0.0004803451301995665\n",
      "Step: 4399, Loss: 0.0008045377908274531\n",
      "Data loss: 0.00030963655444793403, Function loss: 0.0006132334819994867\n",
      "Step: 4400, Loss: 0.0009228700073435903\n",
      "Data loss: 0.00033442393760196865, Function loss: 0.0007715299725532532\n",
      "Step: 4401, Loss: 0.0011059539392590523\n",
      "Data loss: 0.00031234868220053613, Function loss: 0.0010542577365413308\n",
      "Step: 4402, Loss: 0.0013666064478456974\n",
      "Data loss: 0.00035305472556501627, Function loss: 0.0014249903615564108\n",
      "Step: 4403, Loss: 0.001778045087121427\n",
      "Data loss: 0.0003240465885028243, Function loss: 0.0020131911151111126\n",
      "Step: 4404, Loss: 0.0023372378200292587\n",
      "Data loss: 0.00038743260665796697, Function loss: 0.0028085322119295597\n",
      "Step: 4405, Loss: 0.0031959647312760353\n",
      "Data loss: 0.00035219418350607157, Function loss: 0.0038374175783246756\n",
      "Step: 4406, Loss: 0.004189611878246069\n",
      "Data loss: 0.0004430740373209119, Function loss: 0.0052131409756839275\n",
      "Step: 4407, Loss: 0.005656214896589518\n",
      "Data loss: 0.0004021349304821342, Function loss: 0.006648337934166193\n",
      "Step: 4408, Loss: 0.007050472777336836\n",
      "Data loss: 0.0005196462152525783, Function loss: 0.008544321171939373\n",
      "Step: 4409, Loss: 0.009063967503607273\n",
      "Data loss: 0.0004609135212376714, Function loss: 0.009626833721995354\n",
      "Step: 4410, Loss: 0.010087747126817703\n",
      "Data loss: 0.000570060801692307, Function loss: 0.010607532225549221\n",
      "Step: 4411, Loss: 0.011177592910826206\n",
      "Data loss: 0.0004689391062129289, Function loss: 0.009375845082104206\n",
      "Step: 4412, Loss: 0.009844784624874592\n",
      "Data loss: 0.0005086024757474661, Function loss: 0.007352342829108238\n",
      "Step: 4413, Loss: 0.007860945537686348\n",
      "Data loss: 0.00039086572360247374, Function loss: 0.004096742253750563\n",
      "Step: 4414, Loss: 0.004487608093768358\n",
      "Data loss: 0.0003842474252451211, Function loss: 0.001459759077988565\n",
      "Step: 4415, Loss: 0.0018440064741298556\n",
      "Data loss: 0.0003468350332695991, Function loss: 0.00030672058346681297\n",
      "Step: 4416, Loss: 0.000653555616736412\n",
      "Data loss: 0.00035613059299066663, Function loss: 0.0008163784514181316\n",
      "Step: 4417, Loss: 0.0011725090444087982\n",
      "Data loss: 0.00041624053847044706, Function loss: 0.002257159212604165\n",
      "Step: 4418, Loss: 0.0026733996346592903\n",
      "Data loss: 0.00039800896774977446, Function loss: 0.0034003625623881817\n",
      "Step: 4419, Loss: 0.003798371646553278\n",
      "Data loss: 0.00045879281242378056, Function loss: 0.0034165994729846716\n",
      "Step: 4420, Loss: 0.0038753922563046217\n",
      "Data loss: 0.00038810280966572464, Function loss: 0.002253970829769969\n",
      "Step: 4421, Loss: 0.002642073668539524\n",
      "Data loss: 0.0004005621012765914, Function loss: 0.0008899527601897717\n",
      "Step: 4422, Loss: 0.0012905148323625326\n",
      "Data loss: 0.00037268977030180395, Function loss: 0.0002854546473827213\n",
      "Step: 4423, Loss: 0.0006581444176845253\n",
      "Data loss: 0.00037477543810382485, Function loss: 0.0006992600974626839\n",
      "Step: 4424, Loss: 0.0010740355355665088\n",
      "Data loss: 0.00041675512329675257, Function loss: 0.0015731039457023144\n",
      "Step: 4425, Loss: 0.0019898591563105583\n",
      "Data loss: 0.00039290159475058317, Function loss: 0.00206641829572618\n",
      "Step: 4426, Loss: 0.0024593197740614414\n",
      "Data loss: 0.00042470841435715556, Function loss: 0.0017692700494080782\n",
      "Step: 4427, Loss: 0.0021939785219728947\n",
      "Data loss: 0.00038083497202023864, Function loss: 0.0009884187020361423\n",
      "Step: 4428, Loss: 0.001369253732264042\n",
      "Data loss: 0.0003823808510787785, Function loss: 0.00035246534389443696\n",
      "Step: 4429, Loss: 0.0007348462240770459\n",
      "Data loss: 0.00038081096136011183, Function loss: 0.0003160121850669384\n",
      "Step: 4430, Loss: 0.0006968231173232198\n",
      "Data loss: 0.0003692072059493512, Function loss: 0.0007528032292611897\n",
      "Step: 4431, Loss: 0.0011220104061067104\n",
      "Data loss: 0.0004034154990222305, Function loss: 0.0011364949168637395\n",
      "Step: 4432, Loss: 0.0015399104449898005\n",
      "Data loss: 0.0003675433399621397, Function loss: 0.001205827109515667\n",
      "Step: 4433, Loss: 0.0015733704203739762\n",
      "Data loss: 0.0003936774446628988, Function loss: 0.0008688673842698336\n",
      "Step: 4434, Loss: 0.0012625448871403933\n",
      "Data loss: 0.00035837083123624325, Function loss: 0.0004668209294322878\n",
      "Step: 4435, Loss: 0.0008251917315647006\n",
      "Data loss: 0.00036290616844780743, Function loss: 0.00025027606170624495\n",
      "Step: 4436, Loss: 0.0006131822010502219\n",
      "Data loss: 0.00036708457628265023, Function loss: 0.0003470901574473828\n",
      "Step: 4437, Loss: 0.0007141747046262026\n",
      "Data loss: 0.00035031407605856657, Function loss: 0.0006215556641109288\n",
      "Step: 4438, Loss: 0.0009718697401694953\n",
      "Data loss: 0.00038079245132394135, Function loss: 0.0007766830967739224\n",
      "Step: 4439, Loss: 0.0011574755189940333\n",
      "Data loss: 0.00034582705120556056, Function loss: 0.0007693881634622812\n",
      "Step: 4440, Loss: 0.0011152152437716722\n",
      "Data loss: 0.00036976265255361795, Function loss: 0.000552059558685869\n",
      "Step: 4441, Loss: 0.0009218222112394869\n",
      "Data loss: 0.00034355424577370286, Function loss: 0.00036164859193377197\n",
      "Step: 4442, Loss: 0.0007052028086036444\n",
      "Data loss: 0.0003479293663986027, Function loss: 0.00025140104116871953\n",
      "Step: 4443, Loss: 0.0005993304075673223\n",
      "Data loss: 0.00035157715319655836, Function loss: 0.0002860613458324224\n",
      "Step: 4444, Loss: 0.0006376384990289807\n",
      "Data loss: 0.0003360300906933844, Function loss: 0.0004222424468025565\n",
      "Step: 4445, Loss: 0.0007582725374959409\n",
      "Data loss: 0.0003602293145377189, Function loss: 0.0005124909221194685\n",
      "Step: 4446, Loss: 0.0008727202657610178\n",
      "Data loss: 0.0003322345728520304, Function loss: 0.0005744227091781795\n",
      "Step: 4447, Loss: 0.0009066573111340404\n",
      "Data loss: 0.00035624639713205397, Function loss: 0.000507539079990238\n",
      "Step: 4448, Loss: 0.0008637855062261224\n",
      "Data loss: 0.00033301979419775307, Function loss: 0.00043698688386939466\n",
      "Step: 4449, Loss: 0.0007700066780671477\n",
      "Data loss: 0.00034288366441614926, Function loss: 0.00033943430753424764\n",
      "Step: 4450, Loss: 0.0006823180010542274\n",
      "Data loss: 0.00033900843118317425, Function loss: 0.0002971346548292786\n",
      "Step: 4451, Loss: 0.0006361430860124528\n",
      "Data loss: 0.00033180444734171033, Function loss: 0.00030236697057262063\n",
      "Step: 4452, Loss: 0.000634171417914331\n",
      "Data loss: 0.0003445269539952278, Function loss: 0.0003097551816608757\n",
      "Step: 4453, Loss: 0.000654282164759934\n",
      "Data loss: 0.0003249212459195405, Function loss: 0.0003476753772702068\n",
      "Step: 4454, Loss: 0.0006725966231897473\n",
      "Data loss: 0.00034291588235646486, Function loss: 0.00032941909739747643\n",
      "Step: 4455, Loss: 0.0006723349797539413\n",
      "Data loss: 0.0003222196246497333, Function loss: 0.00032940247911028564\n",
      "Step: 4456, Loss: 0.0006516220746561885\n",
      "Data loss: 0.0003351743798702955, Function loss: 0.0002874912752304226\n",
      "Step: 4457, Loss: 0.0006226656259968877\n",
      "Data loss: 0.0003238745266571641, Function loss: 0.00027478334959596395\n",
      "Step: 4458, Loss: 0.000598657876253128\n",
      "Data loss: 0.00032693136017769575, Function loss: 0.00026017261552624404\n",
      "Step: 4459, Loss: 0.0005871040048077703\n",
      "Data loss: 0.00032832386204972863, Function loss: 0.0002603016037028283\n",
      "Step: 4460, Loss: 0.0005886254366487265\n",
      "Data loss: 0.0003212960436940193, Function loss: 0.00027474190574139357\n",
      "Step: 4461, Loss: 0.0005960379494354129\n",
      "Data loss: 0.00033090158831328154, Function loss: 0.0002727613318711519\n",
      "Step: 4462, Loss: 0.0006036629201844335\n",
      "Data loss: 0.00031786621548235416, Function loss: 0.00028763897716999054\n",
      "Step: 4463, Loss: 0.0006055051926523447\n",
      "Data loss: 0.00032829539850354195, Function loss: 0.00027170064277015626\n",
      "Step: 4464, Loss: 0.0005999960703775287\n",
      "Data loss: 0.00031764587038196623, Function loss: 0.0002714247675612569\n",
      "Step: 4465, Loss: 0.0005890706088393927\n",
      "Data loss: 0.00032268394716084003, Function loss: 0.00025810141232796013\n",
      "Step: 4466, Loss: 0.0005807853303849697\n",
      "Data loss: 0.00032038369681686163, Function loss: 0.00025788586935959756\n",
      "Step: 4467, Loss: 0.0005782695952802896\n",
      "Data loss: 0.00031829255749471486, Function loss: 0.0002620571758598089\n",
      "Step: 4468, Loss: 0.0005803497042506933\n",
      "Data loss: 0.000323186133755371, Function loss: 0.0002613303659018129\n",
      "Step: 4469, Loss: 0.0005845164996571839\n",
      "Data loss: 0.000315679149935022, Function loss: 0.0002706683590076864\n",
      "Step: 4470, Loss: 0.0005863475380465388\n",
      "Data loss: 0.00032279742299579084, Function loss: 0.0002609251532703638\n",
      "Step: 4471, Loss: 0.0005837226053699851\n",
      "Data loss: 0.0003144632501062006, Function loss: 0.0002631831739563495\n",
      "Step: 4472, Loss: 0.0005776464240625501\n",
      "Data loss: 0.0003193457960151136, Function loss: 0.0002536820247769356\n",
      "Step: 4473, Loss: 0.0005730278207920492\n",
      "Data loss: 0.00031496852170675993, Function loss: 0.00025808889768086374\n",
      "Step: 4474, Loss: 0.0005730574484914541\n",
      "Data loss: 0.00031710986513644457, Function loss: 0.0002628163783811033\n",
      "Step: 4475, Loss: 0.0005799262435175478\n",
      "Data loss: 0.00031687782029621303, Function loss: 0.0002753809094429016\n",
      "Step: 4476, Loss: 0.0005922587588429451\n",
      "Data loss: 0.0003166834358125925, Function loss: 0.00028862516046501696\n",
      "Step: 4477, Loss: 0.000605308567173779\n",
      "Data loss: 0.000318018690450117, Function loss: 0.0002973823284264654\n",
      "Step: 4478, Loss: 0.0006154010188765824\n",
      "Data loss: 0.00031671018223278224, Function loss: 0.0003063415642827749\n",
      "Step: 4479, Loss: 0.0006230517756193876\n",
      "Data loss: 0.0003175221791025251, Function loss: 0.0003087545919697732\n",
      "Step: 4480, Loss: 0.0006262767710722983\n",
      "Data loss: 0.0003166723472531885, Function loss: 0.00031051316182129085\n",
      "Step: 4481, Loss: 0.0006271855090744793\n",
      "Data loss: 0.00031583342934027314, Function loss: 0.0003092240367550403\n",
      "Step: 4482, Loss: 0.0006250574951991439\n",
      "Data loss: 0.0003162279899697751, Function loss: 0.0003045187331736088\n",
      "Step: 4483, Loss: 0.0006207467522472143\n",
      "Data loss: 0.00031302496790885925, Function loss: 0.00029898888897150755\n",
      "Step: 4484, Loss: 0.0006120138568803668\n",
      "Data loss: 0.0003157656465191394, Function loss: 0.0002890879986807704\n",
      "Step: 4485, Loss: 0.0006048536160960793\n",
      "Data loss: 0.0003105012874584645, Function loss: 0.00028959204792045057\n",
      "Step: 4486, Loss: 0.0006000933353789151\n",
      "Data loss: 0.00031622027745470405, Function loss: 0.00028468575328588486\n",
      "Step: 4487, Loss: 0.0006009060307405889\n",
      "Data loss: 0.00030878058169037104, Function loss: 0.0002950261696241796\n",
      "Step: 4488, Loss: 0.0006038067513145506\n",
      "Data loss: 0.0003174173180013895, Function loss: 0.00029279710724949837\n",
      "Step: 4489, Loss: 0.0006102144252508879\n",
      "Data loss: 0.0003075598506256938, Function loss: 0.0003111669793725014\n",
      "Step: 4490, Loss: 0.0006187268299981952\n",
      "Data loss: 0.00031953633879311383, Function loss: 0.0003100436006207019\n",
      "Step: 4491, Loss: 0.0006295799394138157\n",
      "Data loss: 0.0003075329295825213, Function loss: 0.00033462027204222977\n",
      "Step: 4492, Loss: 0.0006421532016247511\n",
      "Data loss: 0.00032327466760762036, Function loss: 0.0003370043996255845\n",
      "Step: 4493, Loss: 0.0006602790672332048\n",
      "Data loss: 0.00031001417664811015, Function loss: 0.0003766111040022224\n",
      "Step: 4494, Loss: 0.0006866252515465021\n",
      "Data loss: 0.00032950122840702534, Function loss: 0.0003941452887374908\n",
      "Step: 4495, Loss: 0.0007236464880406857\n",
      "Data loss: 0.00031589955324307084, Function loss: 0.000458330730907619\n",
      "Step: 4496, Loss: 0.0007742302841506898\n",
      "Data loss: 0.0003393428632989526, Function loss: 0.0004975387710146606\n",
      "Step: 4497, Loss: 0.0008368816343136132\n",
      "Data loss: 0.0003243597166147083, Function loss: 0.0005853468319401145\n",
      "Step: 4498, Loss: 0.0009097065776586533\n",
      "Data loss: 0.0003520103346090764, Function loss: 0.0006392868235707283\n",
      "Step: 4499, Loss: 0.0009912971872836351\n",
      "Data loss: 0.00033548192004673183, Function loss: 0.000750742678064853\n",
      "Step: 4500, Loss: 0.0010862245690077543\n",
      "Data loss: 0.00036775603075511754, Function loss: 0.0008253479609265924\n",
      "Step: 4501, Loss: 0.0011931039625778794\n",
      "Data loss: 0.00035002228105440736, Function loss: 0.0009670636500231922\n",
      "Step: 4502, Loss: 0.0013170859310775995\n",
      "Data loss: 0.0003879104333464056, Function loss: 0.0010654921643435955\n",
      "Step: 4503, Loss: 0.0014534025685861707\n",
      "Data loss: 0.0003682080132421106, Function loss: 0.0012301541864871979\n",
      "Step: 4504, Loss: 0.001598362228833139\n",
      "Data loss: 0.0004107923014089465, Function loss: 0.0013423861237242818\n",
      "Step: 4505, Loss: 0.0017531784251332283\n",
      "Data loss: 0.0003891515370924026, Function loss: 0.0015210371930152178\n",
      "Step: 4506, Loss: 0.00191018870100379\n",
      "Data loss: 0.00043294255738146603, Function loss: 0.0016080819768831134\n",
      "Step: 4507, Loss: 0.002041024621576071\n",
      "Data loss: 0.00040400514262728393, Function loss: 0.001711663557216525\n",
      "Step: 4508, Loss: 0.0021156687289476395\n",
      "Data loss: 0.00043730836478061974, Function loss: 0.0016688886098563671\n",
      "Step: 4509, Loss: 0.0021061969455331564\n",
      "Data loss: 0.00039864383870735765, Function loss: 0.0016052244463935494\n",
      "Step: 4510, Loss: 0.002003868343308568\n",
      "Data loss: 0.0004133238107897341, Function loss: 0.0013907194370403886\n",
      "Step: 4511, Loss: 0.0018040433060377836\n",
      "Data loss: 0.0003693734761327505, Function loss: 0.001162674161605537\n",
      "Step: 4512, Loss: 0.0015320476377382874\n",
      "Data loss: 0.0003682441310957074, Function loss: 0.0008629362564533949\n",
      "Step: 4513, Loss: 0.0012311803875491023\n",
      "Data loss: 0.00033333615283481777, Function loss: 0.0006111721741035581\n",
      "Step: 4514, Loss: 0.0009445083560422063\n",
      "Data loss: 0.00032505180570296943, Function loss: 0.00039791877497918904\n",
      "Step: 4515, Loss: 0.0007229705806821585\n",
      "Data loss: 0.0003144879883620888, Function loss: 0.0002914909564424306\n",
      "Step: 4516, Loss: 0.0006059789448045194\n",
      "Data loss: 0.0003093598352279514, Function loss: 0.00029194814851507545\n",
      "Step: 4517, Loss: 0.0006013079546391964\n",
      "Data loss: 0.0003239369543734938, Function loss: 0.00035592011408880353\n",
      "Step: 4518, Loss: 0.0006798570975661278\n",
      "Data loss: 0.00031809863867238164, Function loss: 0.00047335823182947934\n",
      "Step: 4519, Loss: 0.0007914568996056914\n",
      "Data loss: 0.000342097831889987, Function loss: 0.0005518808611668646\n",
      "Step: 4520, Loss: 0.0008939786930568516\n",
      "Data loss: 0.0003283546247985214, Function loss: 0.0006328386371023953\n",
      "Step: 4521, Loss: 0.0009611932327970862\n",
      "Data loss: 0.00034963316284120083, Function loss: 0.0006314414204098284\n",
      "Step: 4522, Loss: 0.0009810745250433683\n",
      "Data loss: 0.00032899610232561827, Function loss: 0.0006243145908229053\n",
      "Step: 4523, Loss: 0.0009533106931485236\n",
      "Data loss: 0.0003422800509724766, Function loss: 0.0005396136548370123\n",
      "Step: 4524, Loss: 0.0008818936767056584\n",
      "Data loss: 0.0003210650756955147, Function loss: 0.000466337485704571\n",
      "Step: 4525, Loss: 0.0007874025614000857\n",
      "Data loss: 0.0003264853439759463, Function loss: 0.00036442195414565504\n",
      "Step: 4526, Loss: 0.0006909072981216013\n",
      "Data loss: 0.0003131239500362426, Function loss: 0.00029702228493988514\n",
      "Step: 4527, Loss: 0.0006101462058722973\n",
      "Data loss: 0.0003127603849861771, Function loss: 0.00025163055397570133\n",
      "Step: 4528, Loss: 0.000564390909858048\n",
      "Data loss: 0.00031395407859236, Function loss: 0.00025231382460333407\n",
      "Step: 4529, Loss: 0.0005662678740918636\n",
      "Data loss: 0.0003093190607614815, Function loss: 0.000293482473352924\n",
      "Step: 4530, Loss: 0.000602801563218236\n",
      "Data loss: 0.0003217400226276368, Function loss: 0.0003294202615506947\n",
      "Step: 4531, Loss: 0.000651160255074501\n",
      "Data loss: 0.00031025358475744724, Function loss: 0.00038001936627551913\n",
      "Step: 4532, Loss: 0.0006902729510329664\n",
      "Data loss: 0.00032555253710597754, Function loss: 0.00038783694617450237\n",
      "Step: 4533, Loss: 0.0007133894832804799\n",
      "Data loss: 0.00030947831692174077, Function loss: 0.0004049750277772546\n",
      "Step: 4534, Loss: 0.0007144533446989954\n",
      "Data loss: 0.00032321829348802567, Function loss: 0.0003762717533390969\n",
      "Step: 4535, Loss: 0.000699490075930953\n",
      "Data loss: 0.0003078207664657384, Function loss: 0.0003644379903562367\n",
      "Step: 4536, Loss: 0.0006722587859258056\n",
      "Data loss: 0.00031838781433179975, Function loss: 0.00032394472509622574\n",
      "Step: 4537, Loss: 0.0006423325394280255\n",
      "Data loss: 0.0003075067070312798, Function loss: 0.00030408165184780955\n",
      "Step: 4538, Loss: 0.0006115883588790894\n",
      "Data loss: 0.0003139805921819061, Function loss: 0.00027256266912445426\n",
      "Step: 4539, Loss: 0.0005865432322025299\n",
      "Data loss: 0.00030940165743231773, Function loss: 0.00026117588276974857\n",
      "Step: 4540, Loss: 0.0005705775693058968\n",
      "Data loss: 0.0003111771366093308, Function loss: 0.0002531275968067348\n",
      "Step: 4541, Loss: 0.000564304762519896\n",
      "Data loss: 0.0003123268252238631, Function loss: 0.00025349322822876275\n",
      "Step: 4542, Loss: 0.0005658200243487954\n",
      "Data loss: 0.0003096797445323318, Function loss: 0.0002609618823044002\n",
      "Step: 4543, Loss: 0.0005706415977329016\n",
      "Data loss: 0.0003142368223052472, Function loss: 0.00026052433531731367\n",
      "Step: 4544, Loss: 0.0005747611867263913\n",
      "Data loss: 0.00030868215253576636, Function loss: 0.00026944404817186296\n",
      "Step: 4545, Loss: 0.0005781261716037989\n",
      "Data loss: 0.0003149645053781569, Function loss: 0.0002661663747858256\n",
      "Step: 4546, Loss: 0.000581130851060152\n",
      "Data loss: 0.00030762498499825597, Function loss: 0.0002768215781543404\n",
      "Step: 4547, Loss: 0.0005844465922564268\n",
      "Data loss: 0.00031504593789577484, Function loss: 0.00027195236179977655\n",
      "Step: 4548, Loss: 0.0005869982996955514\n",
      "Data loss: 0.0003066806821152568, Function loss: 0.0002830722078215331\n",
      "Step: 4549, Loss: 0.0005897529190406203\n",
      "Data loss: 0.00031482146005146205, Function loss: 0.00028148750425316393\n",
      "Step: 4550, Loss: 0.000596308964304626\n",
      "Data loss: 0.0003059875452890992, Function loss: 0.0003054264234378934\n",
      "Step: 4551, Loss: 0.0006114139687269926\n",
      "Data loss: 0.00031589734135195613, Function loss: 0.00032282283063977957\n",
      "Step: 4552, Loss: 0.0006387201719917357\n",
      "Data loss: 0.0003060188901145011, Function loss: 0.00037808745400980115\n",
      "Step: 4553, Loss: 0.0006841063732281327\n",
      "Data loss: 0.00031922481139190495, Function loss: 0.0004410322289913893\n",
      "Step: 4554, Loss: 0.0007602570112794638\n",
      "Data loss: 0.0003087736840825528, Function loss: 0.0005723430658690631\n",
      "Step: 4555, Loss: 0.0008811167208477855\n",
      "Data loss: 0.0003286111168563366, Function loss: 0.0007508501294068992\n",
      "Step: 4556, Loss: 0.0010794613044708967\n",
      "Data loss: 0.0003186397079844028, Function loss: 0.0010437273886054754\n",
      "Step: 4557, Loss: 0.0013623670674860477\n",
      "Data loss: 0.00035093011683784425, Function loss: 0.0014757484896108508\n",
      "Step: 4558, Loss: 0.0018266786355525255\n",
      "Data loss: 0.0003443487803451717, Function loss: 0.0022059998009353876\n",
      "Step: 4559, Loss: 0.00255034863948822\n",
      "Data loss: 0.0004082933010067791, Function loss: 0.0034540672786533833\n",
      "Step: 4560, Loss: 0.003862360492348671\n",
      "Data loss: 0.0004146722494624555, Function loss: 0.0054388344287872314\n",
      "Step: 4561, Loss: 0.005853506736457348\n",
      "Data loss: 0.0005561455036513507, Function loss: 0.008971888571977615\n",
      "Step: 4562, Loss: 0.00952803436666727\n",
      "Data loss: 0.0005925179575569928, Function loss: 0.01353498175740242\n",
      "Step: 4563, Loss: 0.014127499423921108\n",
      "Data loss: 0.000864342087879777, Function loss: 0.020872723311185837\n",
      "Step: 4564, Loss: 0.02173706516623497\n",
      "Data loss: 0.00086241151439026, Function loss: 0.02549542859196663\n",
      "Step: 4565, Loss: 0.02635784074664116\n",
      "Data loss: 0.0010994153562933207, Function loss: 0.029343968257308006\n",
      "Step: 4566, Loss: 0.030443383380770683\n",
      "Data loss: 0.0007703119772486389, Function loss: 0.020564962178468704\n",
      "Step: 4567, Loss: 0.02133527398109436\n",
      "Data loss: 0.0006209768471308053, Function loss: 0.00885776523500681\n",
      "Step: 4568, Loss: 0.009478742256760597\n",
      "Data loss: 0.00040127409738488495, Function loss: 0.0006337398081086576\n",
      "Step: 4569, Loss: 0.001035013934597373\n",
      "Data loss: 0.00046235285117290914, Function loss: 0.0032427171245217323\n",
      "Step: 4570, Loss: 0.00370506988838315\n",
      "Data loss: 0.000774284009821713, Function loss: 0.010508284904062748\n",
      "Step: 4571, Loss: 0.011282568797469139\n",
      "Data loss: 0.0006445157923735678, Function loss: 0.010711261071264744\n",
      "Step: 4572, Loss: 0.011355777271091938\n",
      "Data loss: 0.0006469307700172067, Function loss: 0.004528869409114122\n",
      "Step: 4573, Loss: 0.005175800062716007\n",
      "Data loss: 0.0005114885279908776, Function loss: 0.0003119788016192615\n",
      "Step: 4574, Loss: 0.0008234673296101391\n",
      "Data loss: 0.000557035964448005, Function loss: 0.003454686375334859\n",
      "Step: 4575, Loss: 0.004011722281575203\n",
      "Data loss: 0.0007791910320520401, Function loss: 0.006936598103493452\n",
      "Step: 4576, Loss: 0.007715789135545492\n",
      "Data loss: 0.0005870907916687429, Function loss: 0.004010326694697142\n",
      "Step: 4577, Loss: 0.0045974175445735455\n",
      "Data loss: 0.0005707007949240506, Function loss: 0.00037975655868649483\n",
      "Step: 4578, Loss: 0.0009504573536105454\n",
      "Data loss: 0.0006249205325730145, Function loss: 0.001787942019291222\n",
      "Step: 4579, Loss: 0.0024128626100718975\n",
      "Data loss: 0.0006275814375840127, Function loss: 0.004185332451015711\n",
      "Step: 4580, Loss: 0.0048129139468073845\n",
      "Data loss: 0.0006702248356305063, Function loss: 0.002727748593315482\n",
      "Step: 4581, Loss: 0.0033979734871536493\n",
      "Data loss: 0.0005797867197543383, Function loss: 0.00034210545709356666\n",
      "Step: 4582, Loss: 0.0009218921768479049\n",
      "Data loss: 0.0005883791600354016, Function loss: 0.0013717581750825047\n",
      "Step: 4583, Loss: 0.0019601373933255672\n",
      "Data loss: 0.0006564329960383475, Function loss: 0.003015955677255988\n",
      "Step: 4584, Loss: 0.0036723886150866747\n",
      "Data loss: 0.0005870222230441868, Function loss: 0.0017726575024425983\n",
      "Step: 4585, Loss: 0.0023596796672791243\n",
      "Data loss: 0.0005588412168435752, Function loss: 0.0002661178295966238\n",
      "Step: 4586, Loss: 0.0008249590173363686\n",
      "Data loss: 0.0005804282263852656, Function loss: 0.0010082375956699252\n",
      "Step: 4587, Loss: 0.0015886658802628517\n",
      "Data loss: 0.0005574867245741189, Function loss: 0.002011403674259782\n",
      "Step: 4588, Loss: 0.0025688903406262398\n",
      "Data loss: 0.0005727546522393823, Function loss: 0.0012314369669184089\n",
      "Step: 4589, Loss: 0.0018041916191577911\n",
      "Data loss: 0.0005203697946853936, Function loss: 0.00025576501502655447\n",
      "Step: 4590, Loss: 0.0007761347806081176\n",
      "Data loss: 0.0005005370476283133, Function loss: 0.0007135109626688063\n",
      "Step: 4591, Loss: 0.0012140480102971196\n",
      "Data loss: 0.0005459512467496097, Function loss: 0.0013244515284895897\n",
      "Step: 4592, Loss: 0.0018704028334468603\n",
      "Data loss: 0.0004787603102158755, Function loss: 0.0009148732060566545\n",
      "Step: 4593, Loss: 0.0013936335453763604\n",
      "Data loss: 0.000481385737657547, Function loss: 0.000259617023402825\n",
      "Step: 4594, Loss: 0.0007410027319565415\n",
      "Data loss: 0.00048774483730085194, Function loss: 0.0004433782596606761\n",
      "Step: 4595, Loss: 0.0009311230969615281\n",
      "Data loss: 0.0004515652253758162, Function loss: 0.0009362587588839233\n",
      "Step: 4596, Loss: 0.001387823955155909\n",
      "Data loss: 0.0004890597774647176, Function loss: 0.0007564057013951242\n",
      "Step: 4597, Loss: 0.0012454654788598418\n",
      "Data loss: 0.00043610663851723075, Function loss: 0.0003327661834191531\n",
      "Step: 4598, Loss: 0.0007688727928325534\n",
      "Data loss: 0.0004359336453489959, Function loss: 0.00030630675610154867\n",
      "Step: 4599, Loss: 0.0007422404014505446\n",
      "Data loss: 0.0004567538562696427, Function loss: 0.0006197219481691718\n",
      "Step: 4600, Loss: 0.001076475833542645\n",
      "Data loss: 0.00041294415132142603, Function loss: 0.0007078326307237148\n",
      "Step: 4601, Loss: 0.0011207767529413104\n",
      "Data loss: 0.0004336268175393343, Function loss: 0.00038194115040823817\n",
      "Step: 4602, Loss: 0.0008155679679475725\n",
      "Data loss: 0.0004091624286957085, Function loss: 0.0002313088480150327\n",
      "Step: 4603, Loss: 0.0006404712912626565\n",
      "Data loss: 0.0004000730987172574, Function loss: 0.00041199385304935277\n",
      "Step: 4604, Loss: 0.0008120669517666101\n",
      "Data loss: 0.00042482835124246776, Function loss: 0.0005740244523622096\n",
      "Step: 4605, Loss: 0.0009988527745008469\n",
      "Data loss: 0.0003867018094751984, Function loss: 0.0005046776495873928\n",
      "Step: 4606, Loss: 0.0008913794299587607\n",
      "Data loss: 0.0003953658160753548, Function loss: 0.0002647191868163645\n",
      "Step: 4607, Loss: 0.0006600850028917193\n",
      "Data loss: 0.0003879303694702685, Function loss: 0.00024433000362478197\n",
      "Step: 4608, Loss: 0.00063226034399122\n",
      "Data loss: 0.0003736271464731544, Function loss: 0.0004171354230493307\n",
      "Step: 4609, Loss: 0.0007907625986263156\n",
      "Data loss: 0.0003935224376618862, Function loss: 0.0004595390928443521\n",
      "Step: 4610, Loss: 0.0008530615596100688\n",
      "Data loss: 0.000366662978194654, Function loss: 0.0003624357341323048\n",
      "Step: 4611, Loss: 0.0007290986832231283\n",
      "Data loss: 0.00037019766750745475, Function loss: 0.0002304488734807819\n",
      "Step: 4612, Loss: 0.0006006465409882367\n",
      "Data loss: 0.0003674920881167054, Function loss: 0.00024439467233605683\n",
      "Step: 4613, Loss: 0.0006118867313489318\n",
      "Data loss: 0.0003566294617485255, Function loss: 0.00034747723839245737\n",
      "Step: 4614, Loss: 0.0007041067001409829\n",
      "Data loss: 0.00036907734465785325, Function loss: 0.0003658686182461679\n",
      "Step: 4615, Loss: 0.0007349459920078516\n",
      "Data loss: 0.00034863108885474503, Function loss: 0.0003123891947325319\n",
      "Step: 4616, Loss: 0.0006610202835872769\n",
      "Data loss: 0.00035593926440924406, Function loss: 0.00022865452046971768\n",
      "Step: 4617, Loss: 0.000584593799430877\n",
      "Data loss: 0.0003484884509816766, Function loss: 0.00024413199571426958\n",
      "Step: 4618, Loss: 0.0005926204612478614\n",
      "Data loss: 0.0003483790787868202, Function loss: 0.00030419317772611976\n",
      "Step: 4619, Loss: 0.0006525722565129399\n",
      "Data loss: 0.0003495175333227962, Function loss: 0.00032799149630591273\n",
      "Step: 4620, Loss: 0.0006775090005248785\n",
      "Data loss: 0.0003428928612265736, Function loss: 0.0002919610124081373\n",
      "Step: 4621, Loss: 0.0006348538445308805\n",
      "Data loss: 0.00033966777846217155, Function loss: 0.00023794348817318678\n",
      "Step: 4622, Loss: 0.0005776112666353583\n",
      "Data loss: 0.0003394545055925846, Function loss: 0.00022405124036595225\n",
      "Step: 4623, Loss: 0.0005635057459585369\n",
      "Data loss: 0.0003334179928060621, Function loss: 0.0002581537119112909\n",
      "Step: 4624, Loss: 0.0005915716756135225\n",
      "Data loss: 0.0003377765533514321, Function loss: 0.0002756476460490376\n",
      "Step: 4625, Loss: 0.0006134242285043001\n",
      "Data loss: 0.0003334530920255929, Function loss: 0.0002659268502611667\n",
      "Step: 4626, Loss: 0.0005993799422867596\n",
      "Data loss: 0.00032969520543701947, Function loss: 0.00024122337345033884\n",
      "Step: 4627, Loss: 0.0005709185497835279\n",
      "Data loss: 0.00033394378260709345, Function loss: 0.00022875626746099442\n",
      "Step: 4628, Loss: 0.0005627000355161726\n",
      "Data loss: 0.00032258001738227904, Function loss: 0.00025668463786132634\n",
      "Step: 4629, Loss: 0.0005792646552436054\n",
      "Data loss: 0.0003333236090838909, Function loss: 0.0002611064410302788\n",
      "Step: 4630, Loss: 0.0005944300210103393\n",
      "Data loss: 0.00032074202317744493, Function loss: 0.000265998300164938\n",
      "Step: 4631, Loss: 0.0005867403233423829\n",
      "Data loss: 0.0003279032534919679, Function loss: 0.00023694233095739037\n",
      "Step: 4632, Loss: 0.0005648455698974431\n",
      "Data loss: 0.00032147442107088864, Function loss: 0.0002275863371323794\n",
      "Step: 4633, Loss: 0.000549060758203268\n",
      "Data loss: 0.0003215018077753484, Function loss: 0.00022721049026586115\n",
      "Step: 4634, Loss: 0.00054871232714504\n",
      "Data loss: 0.0003228837449569255, Function loss: 0.00023543203133158386\n",
      "Step: 4635, Loss: 0.0005583157762885094\n",
      "Data loss: 0.0003176044556312263, Function loss: 0.0002484730212017894\n",
      "Step: 4636, Loss: 0.0005660774768330157\n",
      "Data loss: 0.0003220945072825998, Function loss: 0.00024227453104685992\n",
      "Step: 4637, Loss: 0.0005643690237775445\n",
      "Data loss: 0.0003161679778713733, Function loss: 0.00023905240232124925\n",
      "Step: 4638, Loss: 0.000555220409296453\n",
      "Data loss: 0.0003190168645232916, Function loss: 0.0002272956189699471\n",
      "Step: 4639, Loss: 0.0005463124834932387\n",
      "Data loss: 0.0003157977480441332, Function loss: 0.00022682128474116325\n",
      "Step: 4640, Loss: 0.0005426190327852964\n",
      "Data loss: 0.0003165202506352216, Function loss: 0.00022804555192124099\n",
      "Step: 4641, Loss: 0.0005445657880045474\n",
      "Data loss: 0.00031529838452115655, Function loss: 0.00023153767688199878\n",
      "Step: 4642, Loss: 0.0005468360614031553\n",
      "Data loss: 0.00031481633777730167, Function loss: 0.0002307421382283792\n",
      "Step: 4643, Loss: 0.0005455584614537656\n",
      "Data loss: 0.0003140777407679707, Function loss: 0.00022807784262113273\n",
      "Step: 4644, Loss: 0.0005421555833891034\n",
      "Data loss: 0.00031360378488898277, Function loss: 0.00022545066894963384\n",
      "Step: 4645, Loss: 0.0005390544538386166\n",
      "Data loss: 0.0003125379153061658, Function loss: 0.00022678793175145984\n",
      "Step: 4646, Loss: 0.0005393258761614561\n",
      "Data loss: 0.0003128149837721139, Function loss: 0.00022793826065026224\n",
      "Step: 4647, Loss: 0.0005407532444223762\n",
      "Data loss: 0.0003114528954029083, Function loss: 0.0002300762862432748\n",
      "Step: 4648, Loss: 0.0005415291525423527\n",
      "Data loss: 0.0003118883178103715, Function loss: 0.00022792277741245925\n",
      "Step: 4649, Loss: 0.0005398110952228308\n",
      "Data loss: 0.00031101470813155174, Function loss: 0.00022709219774696976\n",
      "Step: 4650, Loss: 0.0005381068913266063\n",
      "Data loss: 0.0003104833886027336, Function loss: 0.00022614905901718885\n",
      "Step: 4651, Loss: 0.0005366324330680072\n",
      "Data loss: 0.0003109893877990544, Function loss: 0.0002272192796226591\n",
      "Step: 4652, Loss: 0.000538208638317883\n",
      "Data loss: 0.00030904883169569075, Function loss: 0.00023148581385612488\n",
      "Step: 4653, Loss: 0.0005405346164479852\n",
      "Data loss: 0.00031072204001247883, Function loss: 0.00023119503748603165\n",
      "Step: 4654, Loss: 0.00054191704839468\n",
      "Data loss: 0.0003082519397139549, Function loss: 0.00023273540136869997\n",
      "Step: 4655, Loss: 0.0005409873556345701\n",
      "Data loss: 0.00031029406818561256, Function loss: 0.00022879266180098057\n",
      "Step: 4656, Loss: 0.0005390867590904236\n",
      "Data loss: 0.00030773613252677023, Function loss: 0.00022970345162320882\n",
      "Step: 4657, Loss: 0.0005374395987018943\n",
      "Data loss: 0.0003093863488174975, Function loss: 0.0002267205563839525\n",
      "Step: 4658, Loss: 0.0005361068760976195\n",
      "Data loss: 0.00030745117692276835, Function loss: 0.00022993417223915458\n",
      "Step: 4659, Loss: 0.0005373853491619229\n",
      "Data loss: 0.0003088432422373444, Function loss: 0.00022907597303856164\n",
      "Step: 4660, Loss: 0.0005379192298278213\n",
      "Data loss: 0.00030739797512069345, Function loss: 0.00023073425109032542\n",
      "Step: 4661, Loss: 0.0005381322116591036\n",
      "Data loss: 0.00030842539854347706, Function loss: 0.0002292467252118513\n",
      "Step: 4662, Loss: 0.0005376721383072436\n",
      "Data loss: 0.0003067787329200655, Function loss: 0.00022939822520129383\n",
      "Step: 4663, Loss: 0.0005361769581213593\n",
      "Data loss: 0.0003079493762925267, Function loss: 0.00022750310017727315\n",
      "Step: 4664, Loss: 0.0005354525055736303\n",
      "Data loss: 0.000306127272779122, Function loss: 0.00022843868646305054\n",
      "Step: 4665, Loss: 0.0005345659446902573\n",
      "Data loss: 0.0003081202448811382, Function loss: 0.0002268127427669242\n",
      "Step: 4666, Loss: 0.0005349330021999776\n",
      "Data loss: 0.00030545887420885265, Function loss: 0.00023132206115406007\n",
      "Step: 4667, Loss: 0.0005367809208109975\n",
      "Data loss: 0.00030864751897752285, Function loss: 0.000229936238611117\n",
      "Step: 4668, Loss: 0.0005385837284848094\n",
      "Data loss: 0.00030473576043732464, Function loss: 0.0002377271157456562\n",
      "Step: 4669, Loss: 0.0005424628616310656\n",
      "Data loss: 0.00031006321660242975, Function loss: 0.00023908031289465725\n",
      "Step: 4670, Loss: 0.000549143529497087\n",
      "Data loss: 0.00030459059053100646, Function loss: 0.00025433977134525776\n",
      "Step: 4671, Loss: 0.0005589303327724338\n",
      "Data loss: 0.00031302409479394555, Function loss: 0.00026111488114111125\n",
      "Step: 4672, Loss: 0.0005741389468312263\n",
      "Data loss: 0.0003054956323467195, Function loss: 0.0002896710648201406\n",
      "Step: 4673, Loss: 0.0005951666971668601\n",
      "Data loss: 0.0003177168546244502, Function loss: 0.0003044896002393216\n",
      "Step: 4674, Loss: 0.0006222064839676023\n",
      "Data loss: 0.00030776095809414983, Function loss: 0.0003523562918417156\n",
      "Step: 4675, Loss: 0.0006601172499358654\n",
      "Data loss: 0.0003258866199757904, Function loss: 0.0003883405006490648\n",
      "Step: 4676, Loss: 0.0007142270915210247\n",
      "Data loss: 0.00031310503254644573, Function loss: 0.0004819593741558492\n",
      "Step: 4677, Loss: 0.0007950643775984645\n",
      "Data loss: 0.000342007027938962, Function loss: 0.0005728615215048194\n",
      "Step: 4678, Loss: 0.0009148685494437814\n",
      "Data loss: 0.0003273014444857836, Function loss: 0.0007547586574219167\n",
      "Step: 4679, Loss: 0.0010820601601153612\n",
      "Data loss: 0.0003729398886207491, Function loss: 0.0009509198134765029\n",
      "Step: 4680, Loss: 0.0013238596729934216\n",
      "Data loss: 0.00035984659916721284, Function loss: 0.0013133868342265487\n",
      "Step: 4681, Loss: 0.001673233462497592\n",
      "Data loss: 0.0004333121469244361, Function loss: 0.0017125352751463652\n",
      "Step: 4682, Loss: 0.0021458473056554794\n",
      "Data loss: 0.00042323480010963976, Function loss: 0.002325495472177863\n",
      "Step: 4683, Loss: 0.0027487303595989943\n",
      "Data loss: 0.0005257785087451339, Function loss: 0.0029483851976692677\n",
      "Step: 4684, Loss: 0.0034741638228297234\n",
      "Data loss: 0.0005083392607048154, Function loss: 0.0036932462826371193\n",
      "Step: 4685, Loss: 0.004201585426926613\n",
      "Data loss: 0.0006089290836825967, Function loss: 0.004155967850238085\n",
      "Step: 4686, Loss: 0.00476489681750536\n",
      "Data loss: 0.0005494087818078697, Function loss: 0.004399619065225124\n",
      "Step: 4687, Loss: 0.004949027672410011\n",
      "Data loss: 0.0005906785372644663, Function loss: 0.004001041874289513\n",
      "Step: 4688, Loss: 0.004591720178723335\n",
      "Data loss: 0.00046742139966227114, Function loss: 0.0031746437307447195\n",
      "Step: 4689, Loss: 0.003642065217718482\n",
      "Data loss: 0.00043656231719069183, Function loss: 0.0019552831072360277\n",
      "Step: 4690, Loss: 0.00239184545353055\n",
      "Data loss: 0.0003340385155752301, Function loss: 0.0009175895247608423\n",
      "Step: 4691, Loss: 0.0012516280403360724\n",
      "Data loss: 0.0003179104533046484, Function loss: 0.0003164092777296901\n",
      "Step: 4692, Loss: 0.0006343197310343385\n",
      "Data loss: 0.00032384746009483933, Function loss: 0.00033756555058062077\n",
      "Step: 4693, Loss: 0.0006614130106754601\n",
      "Data loss: 0.00034300610423088074, Function loss: 0.0007662277203053236\n",
      "Step: 4694, Loss: 0.0011092338245362043\n",
      "Data loss: 0.0003920330782420933, Function loss: 0.0012082216562703252\n",
      "Step: 4695, Loss: 0.0016002547927200794\n",
      "Data loss: 0.00037570379208773375, Function loss: 0.0014448253205046058\n",
      "Step: 4696, Loss: 0.0018205291125923395\n",
      "Data loss: 0.00038770638639107347, Function loss: 0.001279854099266231\n",
      "Step: 4697, Loss: 0.0016675605438649654\n",
      "Data loss: 0.0003371576895006001, Function loss: 0.0009221317595802248\n",
      "Step: 4698, Loss: 0.0012592894490808249\n",
      "Data loss: 0.00033297998015768826, Function loss: 0.0005238336161710322\n",
      "Step: 4699, Loss: 0.0008568136254325509\n",
      "Data loss: 0.00031367302290163934, Function loss: 0.00032806184026412666\n",
      "Step: 4700, Loss: 0.000641734863165766\n",
      "Data loss: 0.0003222326631657779, Function loss: 0.0003296178765594959\n",
      "Step: 4701, Loss: 0.0006518505397252738\n",
      "Data loss: 0.00033858371898531914, Function loss: 0.0004636728554032743\n",
      "Step: 4702, Loss: 0.0008022565743885934\n",
      "Data loss: 0.00033646359224803746, Function loss: 0.0006219941424205899\n",
      "Step: 4703, Loss: 0.0009584577055647969\n",
      "Data loss: 0.0003513418778311461, Function loss: 0.0006686497363261878\n",
      "Step: 4704, Loss: 0.0010199915850535035\n",
      "Data loss: 0.0003271757159382105, Function loss: 0.0006475934642367065\n",
      "Step: 4705, Loss: 0.000974769180174917\n",
      "Data loss: 0.00033549973159097135, Function loss: 0.0005332884611561894\n",
      "Step: 4706, Loss: 0.0008687882218509912\n",
      "Data loss: 0.00031277339439839125, Function loss: 0.00045011393376626074\n",
      "Step: 4707, Loss: 0.0007628872990608215\n",
      "Data loss: 0.0003230426518712193, Function loss: 0.00038089905865490437\n",
      "Step: 4708, Loss: 0.0007039416814222932\n",
      "Data loss: 0.0003170681302435696, Function loss: 0.000372290174709633\n",
      "Step: 4709, Loss: 0.0006893583340570331\n",
      "Data loss: 0.00032348005333915353, Function loss: 0.0003703646652866155\n",
      "Step: 4710, Loss: 0.0006938447477295995\n",
      "Data loss: 0.0003251532034482807, Function loss: 0.00035712996032088995\n",
      "Step: 4711, Loss: 0.0006822831928730011\n",
      "Data loss: 0.0003185480600222945, Function loss: 0.00032779324101284146\n",
      "Step: 4712, Loss: 0.000646341301035136\n",
      "Data loss: 0.00032139199902303517, Function loss: 0.0002799659559968859\n",
      "Step: 4713, Loss: 0.0006013579550199211\n",
      "Data loss: 0.0003104954957962036, Function loss: 0.0002606516645755619\n",
      "Step: 4714, Loss: 0.000571147189475596\n",
      "Data loss: 0.00031611911254003644, Function loss: 0.000253402249654755\n",
      "Step: 4715, Loss: 0.000569521333090961\n",
      "Data loss: 0.00031076668528839946, Function loss: 0.0002805313852149993\n",
      "Step: 4716, Loss: 0.0005912980996072292\n",
      "Data loss: 0.0003169851843267679, Function loss: 0.00030746395350433886\n",
      "Step: 4717, Loss: 0.0006244491087272763\n",
      "Data loss: 0.00031572673469781876, Function loss: 0.0003268956206738949\n",
      "Step: 4718, Loss: 0.0006426223553717136\n",
      "Data loss: 0.0003167453978676349, Function loss: 0.00032034897594712675\n",
      "Step: 4719, Loss: 0.0006370943738147616\n",
      "Data loss: 0.00031580321956425905, Function loss: 0.00029106944566592574\n",
      "Step: 4720, Loss: 0.0006068726652301848\n",
      "Data loss: 0.0003121500776614994, Function loss: 0.00025860877940431237\n",
      "Step: 4721, Loss: 0.0005707588279619813\n",
      "Data loss: 0.0003121769113931805, Function loss: 0.0002283933717990294\n",
      "Step: 4722, Loss: 0.0005405702977441251\n",
      "Data loss: 0.00030879315454512835, Function loss: 0.00021787156583741307\n",
      "Step: 4723, Loss: 0.0005266647203825414\n",
      "Data loss: 0.00031043984927237034, Function loss: 0.00022054715373087674\n",
      "Step: 4724, Loss: 0.0005309869884513319\n",
      "Data loss: 0.0003094637650065124, Function loss: 0.00023866485571488738\n",
      "Step: 4725, Loss: 0.0005481286207213998\n",
      "Data loss: 0.00031155082979239523, Function loss: 0.0002576380502432585\n",
      "Step: 4726, Loss: 0.0005691888509318233\n",
      "Data loss: 0.0003105153446085751, Function loss: 0.0002737996692303568\n",
      "Step: 4727, Loss: 0.0005843150429427624\n",
      "Data loss: 0.00031242091790772974, Function loss: 0.0002790590806398541\n",
      "Step: 4728, Loss: 0.0005914799985475838\n",
      "Data loss: 0.0003081883769482374, Function loss: 0.0002849898883141577\n",
      "Step: 4729, Loss: 0.0005931782652623951\n",
      "Data loss: 0.000313027820084244, Function loss: 0.0002822590176947415\n",
      "Step: 4730, Loss: 0.0005952868377789855\n",
      "Data loss: 0.0003048584912903607, Function loss: 0.0002981226716656238\n",
      "Step: 4731, Loss: 0.0006029811920598149\n",
      "Data loss: 0.0003151743148919195, Function loss: 0.000304219574900344\n",
      "Step: 4732, Loss: 0.0006193938897922635\n",
      "Data loss: 0.0003036304551642388, Function loss: 0.00033072230871766806\n",
      "Step: 4733, Loss: 0.0006343527929857373\n",
      "Data loss: 0.000316700927214697, Function loss: 0.0003274213522672653\n",
      "Step: 4734, Loss: 0.0006441222503781319\n",
      "Data loss: 0.00030339392833411694, Function loss: 0.00033868607715703547\n",
      "Step: 4735, Loss: 0.000642079976387322\n",
      "Data loss: 0.00031538037001155317, Function loss: 0.00031715314253233373\n",
      "Step: 4736, Loss: 0.0006325335125438869\n",
      "Data loss: 0.00030354881891980767, Function loss: 0.00031135708559304476\n",
      "Step: 4737, Loss: 0.0006149059045128524\n",
      "Data loss: 0.00031244615092873573, Function loss: 0.0002863031695596874\n",
      "Step: 4738, Loss: 0.0005987493204884231\n",
      "Data loss: 0.00030450979829765856, Function loss: 0.00027941050939261913\n",
      "Step: 4739, Loss: 0.0005839202785864472\n",
      "Data loss: 0.00031031007529236376, Function loss: 0.00026266646455042064\n",
      "Step: 4740, Loss: 0.0005729765398427844\n",
      "Data loss: 0.00030519693973474205, Function loss: 0.00025963556254282594\n",
      "Step: 4741, Loss: 0.0005648324731737375\n",
      "Data loss: 0.0003092206025030464, Function loss: 0.00025155392359010875\n",
      "Step: 4742, Loss: 0.0005607745260931551\n",
      "Data loss: 0.00030531288939528167, Function loss: 0.0002528586483094841\n",
      "Step: 4743, Loss: 0.0005581715377047658\n",
      "Data loss: 0.0003088087250944227, Function loss: 0.00025005204952321947\n",
      "Step: 4744, Loss: 0.0005588607746176422\n",
      "Data loss: 0.00030483512091450393, Function loss: 0.0002610360970720649\n",
      "Step: 4745, Loss: 0.0005658712470903993\n",
      "Data loss: 0.00030977081041783094, Function loss: 0.0002707490639295429\n",
      "Step: 4746, Loss: 0.0005805199034512043\n",
      "Data loss: 0.00030403019627556205, Function loss: 0.00030017460812814534\n",
      "Step: 4747, Loss: 0.0006042048335075378\n",
      "Data loss: 0.00031221137032844126, Function loss: 0.00032728188671171665\n",
      "Step: 4748, Loss: 0.0006394932279363275\n",
      "Data loss: 0.00030346892890520394, Function loss: 0.0003848691703751683\n",
      "Step: 4749, Loss: 0.0006883380701765418\n",
      "Data loss: 0.0003161092463415116, Function loss: 0.0004374182899482548\n",
      "Step: 4750, Loss: 0.000753527507185936\n",
      "Data loss: 0.0003040123847313225, Function loss: 0.0005324083031155169\n",
      "Step: 4751, Loss: 0.0008364206878468394\n",
      "Data loss: 0.0003219782083760947, Function loss: 0.0006297391373664141\n",
      "Step: 4752, Loss: 0.0009517173748463392\n",
      "Data loss: 0.0003064225020352751, Function loss: 0.0007756821578368545\n",
      "Step: 4753, Loss: 0.001082104630768299\n",
      "Data loss: 0.00032964794081635773, Function loss: 0.0009063724428415298\n",
      "Step: 4754, Loss: 0.0012360203545540571\n",
      "Data loss: 0.00030943259480409324, Function loss: 0.0010637531522661448\n",
      "Step: 4755, Loss: 0.0013731857761740685\n",
      "Data loss: 0.0003371315833646804, Function loss: 0.0011813787277787924\n",
      "Step: 4756, Loss: 0.0015185102820396423\n",
      "Data loss: 0.00031166995177045465, Function loss: 0.0013019077014178038\n",
      "Step: 4757, Loss: 0.0016135775949805975\n",
      "Data loss: 0.00034282353590242565, Function loss: 0.0013682108838111162\n",
      "Step: 4758, Loss: 0.0017110344488173723\n",
      "Data loss: 0.00031309929909184575, Function loss: 0.0014177196426317096\n",
      "Step: 4759, Loss: 0.0017308189999312162\n",
      "Data loss: 0.00034547329414635897, Function loss: 0.0013989611761644483\n",
      "Step: 4760, Loss: 0.0017444344703108072\n",
      "Data loss: 0.00031392814707942307, Function loss: 0.0013595529599115252\n",
      "Step: 4761, Loss: 0.0016734810778871179\n",
      "Data loss: 0.0003439213032834232, Function loss: 0.001246314262971282\n",
      "Step: 4762, Loss: 0.001590235624462366\n",
      "Data loss: 0.00031311556813307106, Function loss: 0.0011267147492617369\n",
      "Step: 4763, Loss: 0.0014398302882909775\n",
      "Data loss: 0.00033751456066966057, Function loss: 0.0009462343878112733\n",
      "Step: 4764, Loss: 0.0012837490066885948\n",
      "Data loss: 0.00031076098093762994, Function loss: 0.0007884626975283027\n",
      "Step: 4765, Loss: 0.0010992237366735935\n",
      "Data loss: 0.0003274944901932031, Function loss: 0.0006032073288224638\n",
      "Step: 4766, Loss: 0.0009307018481194973\n",
      "Data loss: 0.0003078527806792408, Function loss: 0.0004763305187225342\n",
      "Step: 4767, Loss: 0.0007841832702979445\n",
      "Data loss: 0.00031802611192688346, Function loss: 0.00035099434899166226\n",
      "Step: 4768, Loss: 0.0006690204609185457\n",
      "Data loss: 0.00030650218832306564, Function loss: 0.0002786972327157855\n",
      "Step: 4769, Loss: 0.0005851994501426816\n",
      "Data loss: 0.00031046807998791337, Function loss: 0.0002244211354991421\n",
      "Step: 4770, Loss: 0.0005348892300389707\n",
      "Data loss: 0.0003078478330280632, Function loss: 0.00020994956139475107\n",
      "Step: 4771, Loss: 0.0005177974235266447\n",
      "Data loss: 0.00030622727354057133, Function loss: 0.00022706101299263537\n",
      "Step: 4772, Loss: 0.0005332882865332067\n",
      "Data loss: 0.0003124040085822344, Function loss: 0.00025699162506498396\n",
      "Step: 4773, Loss: 0.0005693956045433879\n",
      "Data loss: 0.00030583731131628156, Function loss: 0.0003082292096223682\n",
      "Step: 4774, Loss: 0.0006140664918348193\n",
      "Data loss: 0.000317821599310264, Function loss: 0.0003410302451811731\n",
      "Step: 4775, Loss: 0.0006588518153876066\n",
      "Data loss: 0.00030670405249111354, Function loss: 0.0003893349494319409\n",
      "Step: 4776, Loss: 0.0006960390019230545\n",
      "Data loss: 0.0003212330339010805, Function loss: 0.00039532524533569813\n",
      "Step: 4777, Loss: 0.0007165583083406091\n",
      "Data loss: 0.0003071068204008043, Function loss: 0.00040997486212290823\n",
      "Step: 4778, Loss: 0.000717081711627543\n",
      "Data loss: 0.00032109205494634807, Function loss: 0.0003803676227107644\n",
      "Step: 4779, Loss: 0.0007014597067609429\n",
      "Data loss: 0.0003067113284487277, Function loss: 0.0003692047903314233\n",
      "Step: 4780, Loss: 0.0006759160896763206\n",
      "Data loss: 0.0003182720101904124, Function loss: 0.00032907098648138344\n",
      "Step: 4781, Loss: 0.0006473429966717958\n",
      "Data loss: 0.00030564822372980416, Function loss: 0.0003152544377371669\n",
      "Step: 4782, Loss: 0.0006209026323631406\n",
      "Data loss: 0.00031497987220063806, Function loss: 0.00028417640714906156\n",
      "Step: 4783, Loss: 0.0005991562502458692\n",
      "Data loss: 0.0003047441423404962, Function loss: 0.0002766241377685219\n",
      "Step: 4784, Loss: 0.0005813682801090181\n",
      "Data loss: 0.00031231026514433324, Function loss: 0.0002540219866205007\n",
      "Step: 4785, Loss: 0.0005663322517648339\n",
      "Data loss: 0.0003041945747099817, Function loss: 0.0002489281469024718\n",
      "Step: 4786, Loss: 0.0005531227216124535\n",
      "Data loss: 0.00031007439247332513, Function loss: 0.00023190943466033787\n",
      "Step: 4787, Loss: 0.0005419838125817478\n",
      "Data loss: 0.00030423776479437947, Function loss: 0.00022884427744429559\n",
      "Step: 4788, Loss: 0.0005330820567905903\n",
      "Data loss: 0.00030844524735584855, Function loss: 0.00021655728050973266\n",
      "Step: 4789, Loss: 0.0005250025424174964\n",
      "Data loss: 0.0003045746707357466, Function loss: 0.00021508897771127522\n",
      "Step: 4790, Loss: 0.0005196636775508523\n",
      "Data loss: 0.00030731703736819327, Function loss: 0.00020935217617079616\n",
      "Step: 4791, Loss: 0.0005166692426428199\n",
      "Data loss: 0.00030488852644339204, Function loss: 0.00021212045976426452\n",
      "Step: 4792, Loss: 0.0005170090007595718\n",
      "Data loss: 0.0003072262625209987, Function loss: 0.0002114284288836643\n",
      "Step: 4793, Loss: 0.0005186547059565783\n",
      "Data loss: 0.00030472688376903534, Function loss: 0.0002169541548937559\n",
      "Step: 4794, Loss: 0.0005216810386627913\n",
      "Data loss: 0.000308183953166008, Function loss: 0.00021868069597985595\n",
      "Step: 4795, Loss: 0.0005268646636977792\n",
      "Data loss: 0.000304436165606603, Function loss: 0.00023062710533849895\n",
      "Step: 4796, Loss: 0.000535063270945102\n",
      "Data loss: 0.00031017110450193286, Function loss: 0.0002354598545935005\n",
      "Step: 4797, Loss: 0.0005456309299916029\n",
      "Data loss: 0.0003044097975362092, Function loss: 0.0002547338663134724\n",
      "Step: 4798, Loss: 0.0005591436638496816\n",
      "Data loss: 0.0003129286924377084, Function loss: 0.00026346166850999\n",
      "Step: 4799, Loss: 0.0005763903609476984\n",
      "Data loss: 0.00030438677640631795, Function loss: 0.00029630184872075915\n",
      "Step: 4800, Loss: 0.0006006886251270771\n",
      "Data loss: 0.0003171286662109196, Function loss: 0.00032579939579591155\n",
      "Step: 4801, Loss: 0.0006429280620068312\n",
      "Data loss: 0.0003048517683055252, Function loss: 0.0004018282634206116\n",
      "Step: 4802, Loss: 0.0007066800026223063\n",
      "Data loss: 0.0003247239510528743, Function loss: 0.0004739428113680333\n",
      "Step: 4803, Loss: 0.0007986667333170772\n",
      "Data loss: 0.00030802012770436704, Function loss: 0.0006200621137395501\n",
      "Step: 4804, Loss: 0.0009280822705477476\n",
      "Data loss: 0.0003385116287972778, Function loss: 0.0007764730835333467\n",
      "Step: 4805, Loss: 0.001114984741434455\n",
      "Data loss: 0.00031743175350129604, Function loss: 0.0010539200156927109\n",
      "Step: 4806, Loss: 0.001371351769194007\n",
      "Data loss: 0.0003635082975961268, Function loss: 0.0013587396824732423\n",
      "Step: 4807, Loss: 0.00172224803827703\n",
      "Data loss: 0.00033718248596414924, Function loss: 0.0018026214092969894\n",
      "Step: 4808, Loss: 0.0021398039534687996\n",
      "Data loss: 0.00040094947325997055, Function loss: 0.002285128692165017\n",
      "Step: 4809, Loss: 0.002686078194528818\n",
      "Data loss: 0.00036918531986884773, Function loss: 0.002927296096459031\n",
      "Step: 4810, Loss: 0.0032964814454317093\n",
      "Data loss: 0.000448880105977878, Function loss: 0.003646442899480462\n",
      "Step: 4811, Loss: 0.004095322918146849\n",
      "Data loss: 0.0004074669850524515, Function loss: 0.0044470373541116714\n",
      "Step: 4812, Loss: 0.004854504484683275\n",
      "Data loss: 0.0004903312074020505, Function loss: 0.005270514637231827\n",
      "Step: 4813, Loss: 0.0057608457282185555\n",
      "Data loss: 0.0004288808268029243, Function loss: 0.0058533018454909325\n",
      "Step: 4814, Loss: 0.00628218287602067\n",
      "Data loss: 0.00048699823673814535, Function loss: 0.006204208824783564\n",
      "Step: 4815, Loss: 0.006691207177937031\n",
      "Data loss: 0.0004059180209878832, Function loss: 0.0057892813347280025\n",
      "Step: 4816, Loss: 0.0061951992101967335\n",
      "Data loss: 0.0004319879808463156, Function loss: 0.005067014135420322\n",
      "Step: 4817, Loss: 0.005499002058058977\n",
      "Data loss: 0.00036972176167182624, Function loss: 0.003819283563643694\n",
      "Step: 4818, Loss: 0.004189005121588707\n",
      "Data loss: 0.0003997528401669115, Function loss: 0.002742077922448516\n",
      "Step: 4819, Loss: 0.003141830675303936\n",
      "Data loss: 0.00038636859972029924, Function loss: 0.001961089437827468\n",
      "Step: 4820, Loss: 0.002347458153963089\n",
      "Data loss: 0.00043759201071225107, Function loss: 0.0016881844494491816\n",
      "Step: 4821, Loss: 0.0021257763728499413\n",
      "Data loss: 0.00044156209332868457, Function loss: 0.0018037251429632306\n",
      "Step: 4822, Loss: 0.0022452871780842543\n",
      "Data loss: 0.0004645265871658921, Function loss: 0.0019375297706574202\n",
      "Step: 4823, Loss: 0.002402056474238634\n",
      "Data loss: 0.0004362368199508637, Function loss: 0.0018806597217917442\n",
      "Step: 4824, Loss: 0.0023168965708464384\n",
      "Data loss: 0.0003965294163208455, Function loss: 0.001501386403106153\n",
      "Step: 4825, Loss: 0.001897915848530829\n",
      "Data loss: 0.0003668279096018523, Function loss: 0.000986782950349152\n",
      "Step: 4826, Loss: 0.0013536108890548348\n",
      "Data loss: 0.00033722713124006987, Function loss: 0.0006259033689275384\n",
      "Step: 4827, Loss: 0.0009631305001676083\n",
      "Data loss: 0.0003635549219325185, Function loss: 0.000565783295314759\n",
      "Step: 4828, Loss: 0.0009293382172472775\n",
      "Data loss: 0.0003771196643356234, Function loss: 0.0008169567445293069\n",
      "Step: 4829, Loss: 0.0011940763797610998\n",
      "Data loss: 0.00041407422395423055, Function loss: 0.0011289381654933095\n",
      "Step: 4830, Loss: 0.001543012447655201\n",
      "Data loss: 0.0004213591164443642, Function loss: 0.0012868006015196443\n",
      "Step: 4831, Loss: 0.001708159688860178\n",
      "Data loss: 0.00040016029379330575, Function loss: 0.0011423465330153704\n",
      "Step: 4832, Loss: 0.0015425068559125066\n",
      "Data loss: 0.0003802803112193942, Function loss: 0.000758431269787252\n",
      "Step: 4833, Loss: 0.0011387115810066462\n",
      "Data loss: 0.0003418309788685292, Function loss: 0.0004066081310156733\n",
      "Step: 4834, Loss: 0.0007484391098842025\n",
      "Data loss: 0.0003385570307727903, Function loss: 0.00023009986034594476\n",
      "Step: 4835, Loss: 0.0005686568911187351\n",
      "Data loss: 0.0003422204463277012, Function loss: 0.00029107637237757444\n",
      "Step: 4836, Loss: 0.0006332967896014452\n",
      "Data loss: 0.0003504312480799854, Function loss: 0.0004957932978868484\n",
      "Step: 4837, Loss: 0.0008462245459668338\n",
      "Data loss: 0.00037677164073102176, Function loss: 0.0006783526041544974\n",
      "Step: 4838, Loss: 0.0010551242157816887\n",
      "Data loss: 0.0003609930572565645, Function loss: 0.0007707779295742512\n",
      "Step: 4839, Loss: 0.0011317710159346461\n",
      "Data loss: 0.00037383424933068454, Function loss: 0.0006666898843832314\n",
      "Step: 4840, Loss: 0.0010405241046100855\n",
      "Data loss: 0.00033986347261816263, Function loss: 0.0005066577577963471\n",
      "Step: 4841, Loss: 0.0008465212304145098\n",
      "Data loss: 0.0003407698532100767, Function loss: 0.0003102513146586716\n",
      "Step: 4842, Loss: 0.0006510211387649179\n",
      "Data loss: 0.00032388357794843614, Function loss: 0.0002104221930494532\n",
      "Step: 4843, Loss: 0.0005343057564459741\n",
      "Data loss: 0.0003226679691579193, Function loss: 0.00020564798614941537\n",
      "Step: 4844, Loss: 0.0005283159553073347\n",
      "Data loss: 0.00033350384910590947, Function loss: 0.00026627423358149827\n",
      "Step: 4845, Loss: 0.0005997780826874077\n",
      "Data loss: 0.00032550698961131275, Function loss: 0.00036884762812405825\n",
      "Step: 4846, Loss: 0.0006943546468392015\n",
      "Data loss: 0.00034573199809528887, Function loss: 0.0004125985724385828\n",
      "Step: 4847, Loss: 0.0007583305705338717\n",
      "Data loss: 0.00032577142701484263, Function loss: 0.00043631685548461974\n",
      "Step: 4848, Loss: 0.0007620882824994624\n",
      "Data loss: 0.00033945360337384045, Function loss: 0.00037053361302241683\n",
      "Step: 4849, Loss: 0.0007099872455000877\n",
      "Data loss: 0.0003174347511958331, Function loss: 0.00031048315577208996\n",
      "Step: 4850, Loss: 0.0006279179360717535\n",
      "Data loss: 0.00032412895234301686, Function loss: 0.0002332985313842073\n",
      "Step: 4851, Loss: 0.0005574274691753089\n",
      "Data loss: 0.00031414523255079985, Function loss: 0.00020941089314874262\n",
      "Step: 4852, Loss: 0.0005235561402514577\n",
      "Data loss: 0.00031689071329310536, Function loss: 0.00021399575052782893\n",
      "Step: 4853, Loss: 0.0005308864638209343\n",
      "Data loss: 0.0003208457783330232, Function loss: 0.00025007949443534017\n",
      "Step: 4854, Loss: 0.0005709253018721938\n",
      "Data loss: 0.00031918048625811934, Function loss: 0.0003045175108127296\n",
      "Step: 4855, Loss: 0.0006236979970708489\n",
      "Data loss: 0.0003284185950178653, Function loss: 0.0003392778744455427\n",
      "Step: 4856, Loss: 0.000667696469463408\n",
      "Data loss: 0.00032184820156544447, Function loss: 0.00036380812525749207\n",
      "Step: 4857, Loss: 0.0006856563268229365\n",
      "Data loss: 0.0003256170020904392, Function loss: 0.0003495153214316815\n",
      "Step: 4858, Loss: 0.0006751323235221207\n",
      "Data loss: 0.0003194782475475222, Function loss: 0.0003308165178168565\n",
      "Step: 4859, Loss: 0.0006502947653643787\n",
      "Data loss: 0.0003144603979308158, Function loss: 0.0003196379402652383\n",
      "Step: 4860, Loss: 0.0006340983090922236\n",
      "Data loss: 0.00031912288977764547, Function loss: 0.00032758963061496615\n",
      "Step: 4861, Loss: 0.0006467124912887812\n",
      "Data loss: 0.00030617276206612587, Function loss: 0.0003926697245333344\n",
      "Step: 4862, Loss: 0.0006988424574956298\n",
      "Data loss: 0.00032615428790450096, Function loss: 0.00046998739708215\n",
      "Step: 4863, Loss: 0.0007961416849866509\n",
      "Data loss: 0.00030472222715616226, Function loss: 0.0006148184766061604\n",
      "Step: 4864, Loss: 0.0009195407037623227\n",
      "Data loss: 0.0003356271772645414, Function loss: 0.000735920388251543\n",
      "Step: 4865, Loss: 0.0010715476237237453\n",
      "Data loss: 0.00030645623337477446, Function loss: 0.0009308407898060977\n",
      "Step: 4866, Loss: 0.0012372969649732113\n",
      "Data loss: 0.00034430227242410183, Function loss: 0.0011289900867268443\n",
      "Step: 4867, Loss: 0.0014732923591509461\n",
      "Data loss: 0.00031467643566429615, Function loss: 0.0014510739129036665\n",
      "Step: 4868, Loss: 0.0017657503485679626\n",
      "Data loss: 0.00036238483153283596, Function loss: 0.0018514844123274088\n",
      "Step: 4869, Loss: 0.0022138692438602448\n",
      "Data loss: 0.0003414125239942223, Function loss: 0.0024204389192163944\n",
      "Step: 4870, Loss: 0.002761851530522108\n",
      "Data loss: 0.00040615617763251066, Function loss: 0.003212032373994589\n",
      "Step: 4871, Loss: 0.0036181886680424213\n",
      "Data loss: 0.00039806129643693566, Function loss: 0.0041268072091042995\n",
      "Step: 4872, Loss: 0.004524868447333574\n",
      "Data loss: 0.0004822825430892408, Function loss: 0.005273475777357817\n",
      "Step: 4873, Loss: 0.005755758378654718\n",
      "Data loss: 0.0004690181231126189, Function loss: 0.006146038882434368\n",
      "Step: 4874, Loss: 0.006615057121962309\n",
      "Data loss: 0.0005512897623702884, Function loss: 0.006918423343449831\n",
      "Step: 4875, Loss: 0.007469713222235441\n",
      "Data loss: 0.0004917171318084002, Function loss: 0.0066847060807049274\n",
      "Step: 4876, Loss: 0.007176423445343971\n",
      "Data loss: 0.0005293022259138525, Function loss: 0.0059171742759644985\n",
      "Step: 4877, Loss: 0.006446476560086012\n",
      "Data loss: 0.00042014283826574683, Function loss: 0.0042564705945551395\n",
      "Step: 4878, Loss: 0.004676613491028547\n",
      "Data loss: 0.0004159426025580615, Function loss: 0.0024677528999745846\n",
      "Step: 4879, Loss: 0.0028836955316364765\n",
      "Data loss: 0.0003347279562149197, Function loss: 0.000982128782197833\n",
      "Step: 4880, Loss: 0.0013168567093089223\n",
      "Data loss: 0.000333427480654791, Function loss: 0.00023684735060669482\n",
      "Step: 4881, Loss: 0.0005702748312614858\n",
      "Data loss: 0.000347892171703279, Function loss: 0.00038515543565154076\n",
      "Step: 4882, Loss: 0.0007330476073548198\n",
      "Data loss: 0.00034891554969362915, Function loss: 0.0010936170583590865\n",
      "Step: 4883, Loss: 0.0014425326371565461\n",
      "Data loss: 0.00040849484503269196, Function loss: 0.001750724040903151\n",
      "Step: 4884, Loss: 0.002159218769520521\n",
      "Data loss: 0.000371165486285463, Function loss: 0.0020365272648632526\n",
      "Step: 4885, Loss: 0.002407692838460207\n",
      "Data loss: 0.0004045835230499506, Function loss: 0.0017791657010093331\n",
      "Step: 4886, Loss: 0.0021837493404746056\n",
      "Data loss: 0.0003506215289235115, Function loss: 0.001186254434287548\n",
      "Step: 4887, Loss: 0.0015368759632110596\n",
      "Data loss: 0.0003555521252565086, Function loss: 0.0005476123187690973\n",
      "Step: 4888, Loss: 0.0009031644440256059\n",
      "Data loss: 0.00033585092751309276, Function loss: 0.0002246044750791043\n",
      "Step: 4889, Loss: 0.0005604553734883666\n",
      "Data loss: 0.0003394241794012487, Function loss: 0.00025732192443683743\n",
      "Step: 4890, Loss: 0.0005967461038380861\n",
      "Data loss: 0.000354433199390769, Function loss: 0.0005177214625291526\n",
      "Step: 4891, Loss: 0.0008721546619199216\n",
      "Data loss: 0.0003528958186507225, Function loss: 0.0008065036381594837\n",
      "Step: 4892, Loss: 0.001159399515017867\n",
      "Data loss: 0.00036657953751273453, Function loss: 0.0009266689885407686\n",
      "Step: 4893, Loss: 0.0012932484969496727\n",
      "Data loss: 0.00035148148890584707, Function loss: 0.0008287339587695897\n",
      "Step: 4894, Loss: 0.0011802155058830976\n",
      "Data loss: 0.0003505777276586741, Function loss: 0.0005623200559057295\n",
      "Step: 4895, Loss: 0.0009128977544605732\n",
      "Data loss: 0.0003349758335389197, Function loss: 0.00030319319921545684\n",
      "Step: 4896, Loss: 0.0006381690036505461\n",
      "Data loss: 0.00033217607415281236, Function loss: 0.00018763016851153225\n",
      "Step: 4897, Loss: 0.0005198062281124294\n",
      "Data loss: 0.000334433134412393, Function loss: 0.00024441719870083034\n",
      "Step: 4898, Loss: 0.0005788503331132233\n",
      "Data loss: 0.00033294898457825184, Function loss: 0.0003944697673432529\n",
      "Step: 4899, Loss: 0.0007274187519215047\n",
      "Data loss: 0.00034355244133621454, Function loss: 0.0005211918032728136\n",
      "Step: 4900, Loss: 0.0008647442446090281\n",
      "Data loss: 0.0003364476724527776, Function loss: 0.0005805542459711432\n",
      "Step: 4901, Loss: 0.0009170019184239209\n",
      "Data loss: 0.00034211899037472904, Function loss: 0.0005347331753000617\n",
      "Step: 4902, Loss: 0.0008768521947786212\n",
      "Data loss: 0.000331040850142017, Function loss: 0.0004276109393686056\n",
      "Step: 4903, Loss: 0.0007586517604067922\n",
      "Data loss: 0.00033009954495355487, Function loss: 0.00030027489992789924\n",
      "Step: 4904, Loss: 0.0006303744157776237\n",
      "Data loss: 0.00032426498364657164, Function loss: 0.00021561258472502232\n",
      "Step: 4905, Loss: 0.000539877568371594\n",
      "Data loss: 0.0003208070993423462, Function loss: 0.0001909975107992068\n",
      "Step: 4906, Loss: 0.0005118045955896378\n",
      "Data loss: 0.0003241790982428938, Function loss: 0.00021626798843499273\n",
      "Step: 4907, Loss: 0.0005404470721259713\n",
      "Data loss: 0.0003200600331183523, Function loss: 0.00028479547472670674\n",
      "Step: 4908, Loss: 0.0006048554787412286\n",
      "Data loss: 0.00032808020478114486, Function loss: 0.0003462808090262115\n",
      "Step: 4909, Loss: 0.0006743610138073564\n",
      "Data loss: 0.00032271892996504903, Function loss: 0.0003926744102500379\n",
      "Step: 4910, Loss: 0.0007153933402150869\n",
      "Data loss: 0.0003282252873759717, Function loss: 0.0003852494992315769\n",
      "Step: 4911, Loss: 0.000713474815711379\n",
      "Data loss: 0.0003210523573216051, Function loss: 0.0003475923149380833\n",
      "Step: 4912, Loss: 0.0006686446722596884\n",
      "Data loss: 0.00032156825182028115, Function loss: 0.0002845526032615453\n",
      "Step: 4913, Loss: 0.0006061208550818264\n",
      "Data loss: 0.00031576945912092924, Function loss: 0.00023310152755584568\n",
      "Step: 4914, Loss: 0.0005488710012286901\n",
      "Data loss: 0.0003154004516545683, Function loss: 0.00019708473701030016\n",
      "Step: 4915, Loss: 0.000512485159561038\n",
      "Data loss: 0.00031299900729209185, Function loss: 0.0001879132178146392\n",
      "Step: 4916, Loss: 0.0005009121960029006\n",
      "Data loss: 0.0003139519540127367, Function loss: 0.0001991142489714548\n",
      "Step: 4917, Loss: 0.0005130661884322762\n",
      "Data loss: 0.0003137409221380949, Function loss: 0.00022389514197129756\n",
      "Step: 4918, Loss: 0.0005376360495574772\n",
      "Data loss: 0.00031542559736408293, Function loss: 0.00024580853641964495\n",
      "Step: 4919, Loss: 0.0005612341337837279\n",
      "Data loss: 0.0003142828354611993, Function loss: 0.0002608058275654912\n",
      "Step: 4920, Loss: 0.0005750886630266905\n",
      "Data loss: 0.0003156584280077368, Function loss: 0.0002631195238791406\n",
      "Step: 4921, Loss: 0.0005787779809907079\n",
      "Data loss: 0.0003127027302980423, Function loss: 0.00025892435223795474\n",
      "Step: 4922, Loss: 0.0005716270534321666\n",
      "Data loss: 0.0003138188912998885, Function loss: 0.00024450424825772643\n",
      "Step: 4923, Loss: 0.0005583231104537845\n",
      "Data loss: 0.0003104519273620099, Function loss: 0.0002349072601646185\n",
      "Step: 4924, Loss: 0.0005453592166304588\n",
      "Data loss: 0.0003117834567092359, Function loss: 0.00022516313765663654\n",
      "Step: 4925, Loss: 0.0005369465798139572\n",
      "Data loss: 0.0003088493540417403, Function loss: 0.0002204417687607929\n",
      "Step: 4926, Loss: 0.000529291108250618\n",
      "Data loss: 0.00031005722121335566, Function loss: 0.00021368062880355865\n",
      "Step: 4927, Loss: 0.0005237378645688295\n",
      "Data loss: 0.0003076339780818671, Function loss: 0.00020802751532755792\n",
      "Step: 4928, Loss: 0.000515661493409425\n",
      "Data loss: 0.0003077410801779479, Function loss: 0.00019964830426033586\n",
      "Step: 4929, Loss: 0.0005073893698863685\n",
      "Data loss: 0.00030698537011630833, Function loss: 0.00019218458328396082\n",
      "Step: 4930, Loss: 0.0004991699242964387\n",
      "Data loss: 0.00030549612711183727, Function loss: 0.00019004533533006907\n",
      "Step: 4931, Loss: 0.0004955414915457368\n",
      "Data loss: 0.00030705792596563697, Function loss: 0.00018756995268631727\n",
      "Step: 4932, Loss: 0.000494627864100039\n",
      "Data loss: 0.00030461110873147845, Function loss: 0.0001922923547681421\n",
      "Step: 4933, Loss: 0.0004969034343957901\n",
      "Data loss: 0.00030778037034906447, Function loss: 0.00019536240142770112\n",
      "Step: 4934, Loss: 0.0005031427717767656\n",
      "Data loss: 0.00030457344837486744, Function loss: 0.0002077707031276077\n",
      "Step: 4935, Loss: 0.0005123441806063056\n",
      "Data loss: 0.000309219176415354, Function loss: 0.00021413525973912328\n",
      "Step: 4936, Loss: 0.0005233544507063925\n",
      "Data loss: 0.0003048055514227599, Function loss: 0.00022995122708380222\n",
      "Step: 4937, Loss: 0.0005347568076103926\n",
      "Data loss: 0.0003109379322268069, Function loss: 0.00023411132860928774\n",
      "Step: 4938, Loss: 0.0005450492608360946\n",
      "Data loss: 0.00030510671786032617, Function loss: 0.00024735910119488835\n",
      "Step: 4939, Loss: 0.0005524657899513841\n",
      "Data loss: 0.0003116551088169217, Function loss: 0.00024323821708094329\n",
      "Step: 4940, Loss: 0.0005548933404497802\n",
      "Data loss: 0.00030487935873679817, Function loss: 0.0002472412015777081\n",
      "Step: 4941, Loss: 0.0005521205603145063\n",
      "Data loss: 0.0003099108289461583, Function loss: 0.00023502878320869058\n",
      "Step: 4942, Loss: 0.0005449395976029336\n",
      "Data loss: 0.00030413837521336973, Function loss: 0.00023201326257549226\n",
      "Step: 4943, Loss: 0.000536151637788862\n",
      "Data loss: 0.00030742096714675426, Function loss: 0.00022015969443600625\n",
      "Step: 4944, Loss: 0.0005275806761346757\n",
      "Data loss: 0.00030342891113832593, Function loss: 0.0002161646116292104\n",
      "Step: 4945, Loss: 0.0005195935373194516\n",
      "Data loss: 0.00030498590786010027, Function loss: 0.00020628429774660617\n",
      "Step: 4946, Loss: 0.0005112701910547912\n",
      "Data loss: 0.0003030584193766117, Function loss: 0.00020086474251002073\n",
      "Step: 4947, Loss: 0.0005039231618866324\n",
      "Data loss: 0.00030259767663665116, Function loss: 0.00019677134696394205\n",
      "Step: 4948, Loss: 0.0004993689944967628\n",
      "Data loss: 0.00030322864768095315, Function loss: 0.0001942674134625122\n",
      "Step: 4949, Loss: 0.0004974960465915501\n",
      "Data loss: 0.00030107260681688786, Function loss: 0.00019733142107725143\n",
      "Step: 4950, Loss: 0.0004984040278941393\n",
      "Data loss: 0.0003039370058104396, Function loss: 0.00019711957429535687\n",
      "Step: 4951, Loss: 0.000501056551001966\n",
      "Data loss: 0.00030017734388820827, Function loss: 0.00020596156537067145\n",
      "Step: 4952, Loss: 0.0005061388947069645\n",
      "Data loss: 0.00030524301109835505, Function loss: 0.00020933346240781248\n",
      "Step: 4953, Loss: 0.0005145764444023371\n",
      "Data loss: 0.00029952492332085967, Function loss: 0.00022698093380313367\n",
      "Step: 4954, Loss: 0.0005265058716759086\n",
      "Data loss: 0.0003069639496970922, Function loss: 0.00023516290821135044\n",
      "Step: 4955, Loss: 0.0005421268288046122\n",
      "Data loss: 0.0002991683140862733, Function loss: 0.00026547457673586905\n",
      "Step: 4956, Loss: 0.0005646428908221424\n",
      "Data loss: 0.00030986222554929554, Function loss: 0.00029091903707012534\n",
      "Step: 4957, Loss: 0.0006007812917232513\n",
      "Data loss: 0.0002993122034240514, Function loss: 0.00036078738048672676\n",
      "Step: 4958, Loss: 0.0006600995548069477\n",
      "Data loss: 0.0003155074664391577, Function loss: 0.00044744726619683206\n",
      "Step: 4959, Loss: 0.0007629547035321593\n",
      "Data loss: 0.00030155671993270516, Function loss: 0.000630400434602052\n",
      "Step: 4960, Loss: 0.0009319571545347571\n",
      "Data loss: 0.00032814801670610905, Function loss: 0.0009039923897944391\n",
      "Step: 4961, Loss: 0.001232140464708209\n",
      "Data loss: 0.00031121933716349304, Function loss: 0.0014120977139100432\n",
      "Step: 4962, Loss: 0.0017233170801773667\n",
      "Data loss: 0.0003581331402529031, Function loss: 0.0021982367616146803\n",
      "Step: 4963, Loss: 0.002556369872763753\n",
      "Data loss: 0.0003413371159695089, Function loss: 0.0033653215505182743\n",
      "Step: 4964, Loss: 0.003706658724695444\n",
      "Data loss: 0.00041974522173404694, Function loss: 0.005048817023634911\n",
      "Step: 4965, Loss: 0.0054685622453689575\n",
      "Data loss: 0.00040494310087524354, Function loss: 0.006974445655941963\n",
      "Step: 4966, Loss: 0.007379388902336359\n",
      "Data loss: 0.0005198061699047685, Function loss: 0.009717892855405807\n",
      "Step: 4967, Loss: 0.010237699374556541\n",
      "Data loss: 0.0005006412975490093, Function loss: 0.011694096960127354\n",
      "Step: 4968, Loss: 0.012194737792015076\n",
      "Data loss: 0.000616136472672224, Function loss: 0.013706251978874207\n",
      "Step: 4969, Loss: 0.014322388917207718\n",
      "Data loss: 0.0005387359415180981, Function loss: 0.01233603898435831\n",
      "Step: 4970, Loss: 0.012874774634838104\n",
      "Data loss: 0.000563067093025893, Function loss: 0.009673663415014744\n",
      "Step: 4971, Loss: 0.010236730799078941\n",
      "Data loss: 0.00042945376480929554, Function loss: 0.0049541364423930645\n",
      "Step: 4972, Loss: 0.0053835902363061905\n",
      "Data loss: 0.00041416322346776724, Function loss: 0.0014796288451179862\n",
      "Step: 4973, Loss: 0.0018937920685857534\n",
      "Data loss: 0.0003635087050497532, Function loss: 0.0006144184735603631\n",
      "Step: 4974, Loss: 0.0009779271204024553\n",
      "Data loss: 0.0004006116942036897, Function loss: 0.0019948564004153013\n",
      "Step: 4975, Loss: 0.0023954680655151606\n",
      "Data loss: 0.0004498180642258376, Function loss: 0.003934765234589577\n",
      "Step: 4976, Loss: 0.004384583327919245\n",
      "Data loss: 0.0004279431886970997, Function loss: 0.004296379629522562\n",
      "Step: 4977, Loss: 0.004724322818219662\n",
      "Data loss: 0.0004531910235527903, Function loss: 0.002948532812297344\n",
      "Step: 4978, Loss: 0.003401723923161626\n",
      "Data loss: 0.00038383566425181925, Function loss: 0.0009781012777239084\n",
      "Step: 4979, Loss: 0.0013619369128718972\n",
      "Data loss: 0.0003985495714005083, Function loss: 0.00032458119676448405\n",
      "Step: 4980, Loss: 0.0007231307681649923\n",
      "Data loss: 0.0004156187642365694, Function loss: 0.0012997739249840379\n",
      "Step: 4981, Loss: 0.0017153926892206073\n",
      "Data loss: 0.00042588432552292943, Function loss: 0.002357088029384613\n",
      "Step: 4982, Loss: 0.0027829722966998816\n",
      "Data loss: 0.0004449299885891378, Function loss: 0.0022825130727142096\n",
      "Step: 4983, Loss: 0.0027274431195110083\n",
      "Data loss: 0.000405031576519832, Function loss: 0.0011325347004458308\n",
      "Step: 4984, Loss: 0.0015375663060694933\n",
      "Data loss: 0.00039807031862437725, Function loss: 0.0002482869604136795\n",
      "Step: 4985, Loss: 0.0006463573081418872\n",
      "Data loss: 0.0004026923270430416, Function loss: 0.0004249792837072164\n",
      "Step: 4986, Loss: 0.000827671610750258\n",
      "Data loss: 0.0003984121431130916, Function loss: 0.001202679704874754\n",
      "Step: 4987, Loss: 0.001601091818884015\n",
      "Data loss: 0.0004292789672035724, Function loss: 0.0015643546357750893\n",
      "Step: 4988, Loss: 0.001993633573874831\n",
      "Data loss: 0.00038975142524577677, Function loss: 0.0011499945539981127\n",
      "Step: 4989, Loss: 0.001539745950140059\n",
      "Data loss: 0.00039981570444069803, Function loss: 0.00047777401050552726\n",
      "Step: 4990, Loss: 0.0008775896858423948\n",
      "Data loss: 0.00038083610706962645, Function loss: 0.0002472801133990288\n",
      "Step: 4991, Loss: 0.0006281161913648248\n",
      "Data loss: 0.0003795928496401757, Function loss: 0.0005103565054014325\n",
      "Step: 4992, Loss: 0.0008899493841454387\n",
      "Data loss: 0.0003989861870650202, Function loss: 0.000868752074893564\n",
      "Step: 4993, Loss: 0.0012677382910624146\n",
      "Data loss: 0.00036823609843850136, Function loss: 0.0009451630176045001\n",
      "Step: 4994, Loss: 0.0013133990578353405\n",
      "Data loss: 0.00038547738222405314, Function loss: 0.0006406343309208751\n",
      "Step: 4995, Loss: 0.0010261116549372673\n",
      "Data loss: 0.00035463061067275703, Function loss: 0.0003173536679241806\n",
      "Step: 4996, Loss: 0.0006719842785969377\n",
      "Data loss: 0.0003583210054785013, Function loss: 0.0001810786925489083\n",
      "Step: 4997, Loss: 0.0005393996834754944\n",
      "Data loss: 0.000363780913176015, Function loss: 0.0002991524524986744\n",
      "Step: 4998, Loss: 0.000662933336570859\n",
      "Data loss: 0.00034943807986564934, Function loss: 0.0005238237790763378\n",
      "Step: 4999, Loss: 0.0008732618298381567\n"
     ]
    }
   ],
   "source": [
    "kdv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32c80456-e6dd-438c-9dc4-dca1c207823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = torch.linspace(-pi, 2*pi, 100)\n",
    "t_axis = torch.linspace(0, 0.5, 100)\n",
    "\n",
    "x_grid, t_grid = torch.meshgrid(x_axis, t_axis, indexing='xy')\n",
    "xt_pairs = torch.stack((x_grid, t_grid), dim=2)\n",
    "xt_pairs = xt_pairs.reshape(xt_pairs.shape[0] * xt_pairs.shape[1],2).to(device)\n",
    "\n",
    "phi = kdv.net(xt_pairs).reshape((100,100)).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "470c0bdb-45fb-417d-bcf8-c492dc9a3d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(x_grid, t_grid, phi)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845acf77-e73c-43fd-a252-940a20b82ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK IF THE TRAINING DATA IS ALIGNED\n",
    "\n",
    "xx = x_train.flatten()\n",
    "tt = t_train.flatten()\n",
    "z = phi_train.flatten()\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(90, -90, 0)\n",
    "ax.plot_trisurf(xx, tt, z, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b2085-800a-4b17-bf1e-05ff8a0e2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdv.closure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020006f1-5871-4e0c-9b8b-f3d0b4e9c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, t_train.shape, phi_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fecd079-8422-4b89-8060-a06d032d07e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10500, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hstack((kdv.x, kdv.t)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849881f4-97f3-4f18-9b47-eb07262a4211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a444917-5dc0-427f-a03c-405a448cbf06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf963241-4aff-433c-90b5-6d276cdddf32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71c89b-e370-46a0-8769-e5aef4f0612b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f4760-4465-473b-86d3-3e7494caf68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f05e6f-026d-461f-bcde-199a0a269cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c834d3-7fc8-4cb7-afb1-2502fc608eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af5c26-fa0a-453a-a675-dbf4e85c2760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dostNN",
   "language": "python",
   "name": "dostnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
